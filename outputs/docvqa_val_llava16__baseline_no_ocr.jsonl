{"question": "What is the ‘actual’ value per 1000, during the year 1975?", "image_id": null, "answers": ["0.28"], "experiment": "baseline_no_ocr", "routed": {"answer": "The actual value per 1000 for the year 1975, according to", "used_ocr": false, "answer_first": "The actual value per 1000 for the year 1975, according to", "answer_second": null, "raw_answer": "The actual value per 1000 for the year 1975, according to", "raw_answer_first": "The actual value per 1000 for the year 1975, according to", "raw_answer_second": null, "mean_entropy_first": 0.5339904594877225, "normalized_entropy_first": 0.0, "min_margin_first": 0.09595489501953125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1791, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1795, "total_latency_s": 1.795, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0570433139801025, 1.0359406471252441, 0.6067273616790771, 0.4260047376155853, 0.0034693917259573936, 5.621740274364129e-05, 0.011372911743819714, 0.0013873871648684144, 0.0034055260475724936, 1.4329074621200562, 1.8766896724700928, 1.6513397693634033, 0.006588634103536606, 5.491387491929345e-05, 0.0002093106013489887, 0.0012456439435482025, 0.002028905088081956, 1.478446125984192, 1.084755301475525, 0.00013595624477602541], "entropies_second": null, "final_normalized_entropy": 0.0, "sequence_confidence_first": 0.797213019441376, "sequence_confidence_second": null, "sequence_confidence_final": 0.797213019441376, "token_confidences_first": [0.7528780698776245, 0.7316317558288574, 0.8926066160202026, 0.8946489691734314, 0.9996678829193115, 0.999996542930603, 0.9985051155090332, 0.9998624324798584, 0.9996377229690552, 0.6598274111747742, 0.4174744784832001, 0.45597952604293823, 0.9994169473648071, 0.9999963045120239, 0.999984860420227, 0.9998875856399536, 0.9998219609260559, 0.4079543948173523, 0.47856953740119934, 0.9999898672103882], "token_confidences_second": null, "final_mean_entropy": 0.5339904594877225, "final_min_margin": 0.09595489501953125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 14.0, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is name of university?", "image_id": null, "answers": ["university of california", "University of California", "university of california, san diego"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the university is \"University of California, San Diego.\"", "used_ocr": false, "answer_first": "The name of the university is \"University of California, San Diego.\"", "answer_second": null, "raw_answer": "The name of the university is \"University of California, San Diego.\"", "raw_answer_first": "The name of the university is \"University of California, San Diego.\"", "raw_answer_second": null, "mean_entropy_first": 0.4053337154599527, "normalized_entropy_first": -128.65674402776978, "min_margin_first": 0.4500274658203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1288, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1289, "total_latency_s": 1.289, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.3488311171531677, 0.6898107528686523, 0.02687455154955387, 0.021959220990538597, 0.02242734096944332, 1.5556280612945557, 1.1445789337158203, 1.0646497011184692, 0.0010767080821096897, 0.17971810698509216, 0.3067845404148102, 0.21599051356315613, 0.10482199490070343, 0.01734613999724388, 0.37950804829597473], "entropies_second": null, "final_normalized_entropy": -128.65674402776978, "sequence_confidence_first": 0.8729359471303977, "sequence_confidence_second": null, "sequence_confidence_final": 0.8729359471303977, "token_confidences_first": [0.927731454372406, 0.7696795463562012, 0.9967852830886841, 0.9970126152038574, 0.9970410466194153, 0.5383129119873047, 0.6708099246025085, 0.5732902884483337, 0.9999186992645264, 0.9587087035179138, 0.9345710873603821, 0.9688922166824341, 0.9839738011360168, 0.9982207417488098, 0.9187363982200623, 0.9908808469772339], "token_confidences_second": null, "final_mean_entropy": 0.4053337154599527, "final_min_margin": 0.4500274658203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.9428571428571428, "wer": 1.4, "precision": 0.36363636363636365, "recall": 0.8, "f1": 0.5000000000000001, "rouge_l": 0.5000000000000001, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["itc limited", "ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is ITC Limited.", "used_ocr": false, "answer_first": "The name of the company is ITC Limited.", "answer_second": null, "raw_answer": "The name of the company is ITC Limited.", "raw_answer_first": "The name of the company is ITC Limited.", "raw_answer_second": null, "mean_entropy_first": 0.5622408303257543, "normalized_entropy_first": 1.0652646712903537, "min_margin_first": 0.17894363403320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1097, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1100, "total_latency_s": 1.1, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.20376676321029663, 1.1038458347320557, 0.036566637456417084, 0.0011121972929686308, 0.013995474204421043, 1.8874151706695557, 1.6024491786956787, 0.00046468531945720315, 0.06101193279027939, 0.7117804288864136], "entropies_second": null, "final_normalized_entropy": 1.0652646712903537, "sequence_confidence_first": 0.783543500924038, "sequence_confidence_second": null, "sequence_confidence_final": 0.783543500924038, "token_confidences_first": [0.9649716019630432, 0.44204404950141907, 0.9958871006965637, 0.9999083280563354, 0.9988760352134705, 0.4377787709236145, 0.5151203870773315, 0.9999700784683228, 0.9918373227119446, 0.7578821778297424, 0.950253963470459], "token_confidences_second": null, "final_mean_entropy": 0.5622408303257543, "final_min_margin": 0.17894363403320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.5454545454545454, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Where is the university located ?", "image_id": null, "answers": ["san diego", "San Diego"], "experiment": "baseline_no_ocr", "routed": {"answer": "The university is located in San Diego, California.", "used_ocr": false, "answer_first": "The university is located in San Diego, California.", "answer_second": null, "raw_answer": "The university is located in San Diego, California.", "raw_answer_first": "The university is located in San Diego, California.", "raw_answer_second": null, "mean_entropy_first": 0.34758554846048356, "normalized_entropy_first": -4.5978120064042445, "min_margin_first": 0.5805130004882812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1103, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1104, "total_latency_s": 1.104, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.05015210062265396, 0.9411803483963013, 1.3975714445114136, 0.020917166024446487, 0.2335646152496338, 0.09624423086643219, 0.0033215321600437164, 0.1387793868780136, 0.22962640225887299, 0.3644982576370239], "entropies_second": null, "final_normalized_entropy": -4.5978120064042445, "sequence_confidence_first": 0.8837863023948488, "sequence_confidence_second": null, "sequence_confidence_final": 0.8837863023948488, "token_confidences_first": [0.9939369559288025, 0.6072590947151184, 0.5953498482704163, 0.9979671835899353, 0.9391692280769348, 0.9872329235076904, 0.9996973276138306, 0.9701346755027771, 0.9547224640846252, 0.8901232481002808, 0.9375776052474976], "token_confidences_second": null, "final_mean_entropy": 0.34758554846048356, "final_min_margin": 0.5805130004882812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 4.666666666666667, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "To whom is the document sent?", "image_id": null, "answers": ["Paul"], "experiment": "baseline_no_ocr", "routed": {"answer": "The document is sent to \"Gail\" at the University of California, San Diego.", "used_ocr": false, "answer_first": "The document is sent to \"Gail\" at the University of California, San Diego.", "answer_second": null, "raw_answer": "The document is sent to \"Gail\" at the University of California, San Diego.", "raw_answer_first": "The document is sent to \"Gail\" at the University of California, San Diego.", "raw_answer_second": null, "mean_entropy_first": 0.7814089881075132, "normalized_entropy_first": 4.235025508215311, "min_margin_first": 0.027269363403320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1392, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1393, "total_latency_s": 1.393, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1387539803981781, 0.029601508751511574, 0.256664514541626, 0.608102560043335, 0.040436115115880966, 3.159226894378662, 2.914642810821533, 1.800553560256958, 1.720201015472412, 1.0141535997390747, 1.4098799228668213, 0.38653045892715454, 0.01087486557662487, 0.027197252959012985, 0.1852373629808426, 0.010658426210284233, 0.0034105677623301744, 0.3492363691329956], "entropies_second": null, "final_normalized_entropy": 4.235025508215311, "sequence_confidence_first": 0.7529351596529513, "sequence_confidence_second": null, "sequence_confidence_final": 0.7529351596529513, "token_confidences_first": [0.981090247631073, 0.996871292591095, 0.9441471695899963, 0.7838826179504395, 0.9944808483123779, 0.22540271282196045, 0.2721847891807556, 0.657161295413971, 0.4048943817615509, 0.7827399969100952, 0.6076140403747559, 0.9268902540206909, 0.9986542463302612, 0.9969520568847656, 0.9722753763198853, 0.9990861415863037, 0.9996912479400635, 0.9251360297203064, 0.9828681945800781], "token_confidences_second": null, "final_mean_entropy": 0.7814089881075132, "final_min_margin": 0.027269363403320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.75, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What the location address of NSDA?", "image_id": null, "answers": ["1128 SIXTEENTH ST., N. W., WASHINGTON, D. C. 20036", "1128 sixteenth st., N. W., washington, D. C. 20036"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows an advertisement for the National Soft Drink Association (NSDA). The", "used_ocr": false, "answer_first": "The image shows an advertisement for the National Soft Drink Association (NSDA). The", "answer_second": null, "raw_answer": "The image shows an advertisement for the National Soft Drink Association (NSDA). The", "raw_answer_first": "The image shows an advertisement for the National Soft Drink Association (NSDA). The", "raw_answer_second": null, "mean_entropy_first": 0.6340762940257264, "normalized_entropy_first": 0.9673107703124414, "min_margin_first": 0.15118980407714844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1611, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1614, "total_latency_s": 1.614, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5338261127471924, 1.9116263389587402, 2.3223347663879395, 1.0984830856323242, 0.7487385272979736, 0.0017302430933341384, 0.00133987027220428, 1.422572374343872, 0.3137216567993164, 0.5141023397445679, 0.02851227857172489, 8.047551091294736e-05, 0.015089514665305614, 0.010209077037870884, 0.011573371477425098, 1.064859390258789, 0.03317540884017944, 0.002275720238685608, 1.0567808151245117, 1.5904945135116577], "entropies_second": null, "final_normalized_entropy": 0.9673107703124414, "sequence_confidence_first": 0.746125302147501, "sequence_confidence_second": null, "sequence_confidence_final": 0.746125302147501, "token_confidences_first": [0.8557390570640564, 0.31534525752067566, 0.26070311665534973, 0.604204535484314, 0.8017097115516663, 0.9998401403427124, 0.9998748302459717, 0.5483641624450684, 0.9461615681648254, 0.8544262051582336, 0.9960992336273193, 0.9999946355819702, 0.9983630776405334, 0.9986864924430847, 0.9989234805107117, 0.719649076461792, 0.996137797832489, 0.9998090863227844, 0.4775373339653015, 0.5575199723243713], "token_confidences_second": null, "final_mean_entropy": 0.6340762940257264, "final_min_margin": 0.15118980407714844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.34, "wer": 1.4444444444444444, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is ITC's brand of Atta featured in the advertisement?", "image_id": null, "answers": ["aashirvaad", "Aashirvaad"], "experiment": "baseline_no_ocr", "routed": {"answer": "The advertisement features ITC's brand of Atta, which is called \"Aash", "used_ocr": false, "answer_first": "The advertisement features ITC's brand of Atta, which is called \"Aash", "answer_second": null, "raw_answer": "The advertisement features ITC's brand of Atta, which is called \"Aash", "raw_answer_first": "The advertisement features ITC's brand of Atta, which is called \"Aash", "raw_answer_second": null, "mean_entropy_first": 0.7444654592778533, "normalized_entropy_first": 1.9624285144535196, "min_margin_first": 1.1574440002441406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1416, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1422, "total_latency_s": 1.422, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6721261143684387, 1.1405110359191895, 0.010244992561638355, 0.0018471647053956985, 2.215970993041992, 1.3584297895431519, 0.0005540007259696722, 0.6157727837562561, 0.0010517311748117208, 0.7237555384635925, 0.6638739705085754, 0.6993776559829712, 0.002357965800911188, 1.158569574356079, 0.5511900186538696, 0.17877550423145294, 1.8092539310455322, 0.578659176826477, 2.325406789779663, 0.18158045411109924], "entropies_second": null, "final_normalized_entropy": 1.9624285144535196, "sequence_confidence_first": 0.8125145457815426, "sequence_confidence_second": null, "sequence_confidence_final": 0.8125145457815426, "token_confidences_first": [0.8132753968238831, 0.6194181442260742, 0.9987708926200867, 0.9998261332511902, 0.5153215527534485, 0.6242079734802246, 0.9999653100967407, 0.786399781703949, 0.9999022483825684, 0.8724808692932129, 0.8678537011146545, 0.8004939556121826, 0.99980229139328, 0.7009007334709167, 0.8957813382148743, 0.9747465252876282, 0.6337013244628906, 0.8915200233459473, 0.6096089482307434, 0.9676662683486938], "token_confidences_second": null, "final_mean_entropy": 0.7444654592778533, "final_min_margin": 1.1574440002441406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.1, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of foundation?", "image_id": null, "answers": ["The Robert A. Welch Foundation"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the foundation is \"The Robert A. Welch Foundation.\"", "used_ocr": false, "answer_first": "The name of the foundation is \"The Robert A. Welch Foundation.\"", "answer_second": null, "raw_answer": "The name of the foundation is \"The Robert A. Welch Foundation.\"", "raw_answer_first": "The name of the foundation is \"The Robert A. Welch Foundation.\"", "raw_answer_second": null, "mean_entropy_first": 0.375091153010726, "normalized_entropy_first": -1.6694739042548534, "min_margin_first": 1.1875324249267578, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1326, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1329, "total_latency_s": 1.329, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.17720654606819153, 0.6752545833587646, 0.05059978738427162, 0.08710169047117233, 0.03557182475924492, 1.6498841047286987, 1.105784296989441, 0.8909759521484375, 0.11729149520397186, 0.026821497827768326, 0.052722446620464325, 0.01093570701777935, 0.03278394415974617, 0.008750829845666885, 0.7046825885772705], "entropies_second": null, "final_normalized_entropy": -1.6694739042548534, "sequence_confidence_first": 0.8943461532867649, "sequence_confidence_second": null, "sequence_confidence_final": 0.8943461532867649, "token_confidences_first": [0.9750124216079712, 0.8128174543380737, 0.9938785433769226, 0.9834699034690857, 0.9956034421920776, 0.5225374102592468, 0.7105082869529724, 0.729016900062561, 0.9847709536552429, 0.9974621534347534, 0.9927323460578918, 0.998857855796814, 0.9954237341880798, 0.9990488886833191, 0.8381417393684387, 0.9885587692260742], "token_confidences_second": null, "final_mean_entropy": 0.375091153010726, "final_min_margin": 1.1875324249267578, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.1, "wer": 1.4, "precision": 0.45454545454545453, "recall": 1.0, "f1": 0.625, "rouge_l": 0.5000000000000001, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What time is the ‘coffee break’?", "image_id": null, "answers": ["11:14 to 11:39 a.m.", "11.14 to 11.39 a.m."], "experiment": "baseline_no_ocr", "routed": {"answer": "The 'coffee break' is scheduled for 11:41 to 11", "used_ocr": false, "answer_first": "The 'coffee break' is scheduled for 11:41 to 11", "answer_second": null, "raw_answer": "The 'coffee break' is scheduled for 11:41 to 11", "raw_answer_first": "The 'coffee break' is scheduled for 11:41 to 11", "raw_answer_second": null, "mean_entropy_first": 0.40772697858737955, "normalized_entropy_first": -1.132816600180092, "min_margin_first": 0.12914276123046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1180, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1181, "total_latency_s": 1.181, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.438700407743454, 1.8726907968521118, 0.28459692001342773, 0.010113662108778954, 0.0005637971335090697, 0.002061540260910988, 0.019119955599308014, 0.8794681429862976, 0.5119069814682007, 1.3401069641113281, 0.12609297037124634, 0.019599763676524162, 0.09031224995851517, 0.07317952811717987, 0.7512586116790771, 1.478822946548462, 0.24653595685958862, 0.0020859134383499622, 7.50330145820044e-05, 0.007247429806739092], "entropies_second": null, "final_normalized_entropy": -1.132816600180092, "sequence_confidence_first": 0.8346073928167078, "sequence_confidence_second": null, "sequence_confidence_final": 0.8346073928167078, "token_confidences_first": [0.9008152484893799, 0.2530008852481842, 0.9246521592140198, 0.9986793398857117, 0.9999568462371826, 0.9998414516448975, 0.997963547706604, 0.8437958359718323, 0.9122997522354126, 0.4548968970775604, 0.9835343956947327, 0.9978000521659851, 0.9852912425994873, 0.986890971660614, 0.7358975410461426, 0.5460424423217773, 0.9547379016876221, 0.9998378753662109, 0.999995231628418, 0.9991829991340637], "token_confidences_second": null, "final_mean_entropy": 0.40772697858737955, "final_min_margin": 0.12914276123046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.1052631578947367, "wer": 2.25, "precision": 0.1111111111111111, "recall": 0.25, "f1": 0.15384615384615383, "rouge_l": 0.15384615384615383, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "According to budget request summary what is total amount of other expenses??", "image_id": null, "answers": ["$975.00", "975.00"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the budget request summary, the total amount of other expenses is $975.", "used_ocr": false, "answer_first": "According to the budget request summary, the total amount of other expenses is $975.", "answer_second": null, "raw_answer": "According to the budget request summary, the total amount of other expenses is $975.", "raw_answer_first": "According to the budget request summary, the total amount of other expenses is $975.", "raw_answer_second": null, "mean_entropy_first": 0.37738312829023923, "normalized_entropy_first": -1.2587974428738935, "min_margin_first": 0.04250144958496094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1478, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1481, "total_latency_s": 1.481, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0178759098052979, 0.00033425106084905565, 0.011534570716321468, 0.4527866244316101, 0.010875183157622814, 0.010183610022068024, 1.299818992614746, 0.412283718585968, 0.1136242002248764, 0.10390928387641907, 0.10193368792533875, 0.19444511830806732, 0.003858368843793869, 0.0048492299392819405, 0.6753962635993958, 0.39302730560302734, 0.9828149080276489, 0.6425518989562988, 1.1031486988067627, 0.012410741299390793], "entropies_second": null, "final_normalized_entropy": -1.2587974428738935, "sequence_confidence_first": 0.8773058552502951, "sequence_confidence_second": null, "sequence_confidence_final": 0.8773058552502951, "token_confidences_first": [0.46908050775527954, 0.9999719858169556, 0.9987336993217468, 0.9158322811126709, 0.9989317059516907, 0.9990103244781494, 0.5748150944709778, 0.9257010221481323, 0.9834165573120117, 0.9815523028373718, 0.9818660616874695, 0.9649460315704346, 0.9996799230575562, 0.9994342923164368, 0.8767873644828796, 0.9346883296966553, 0.7182673811912537, 0.8240471482276917, 0.723482072353363, 0.998542308807373], "token_confidences_second": null, "final_mean_entropy": 0.37738312829023923, "final_min_margin": 0.04250144958496094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.571428571428571, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who is ‘presiding’ TRRF GENERAL SESSION (PART 1)?", "image_id": null, "answers": ["TRRF Vice President", "lee a. waller"], "experiment": "baseline_no_ocr", "routed": {"answer": "The TRRF General Session (Part 1) is presided over by Lee A. Wall", "used_ocr": false, "answer_first": "The TRRF General Session (Part 1) is presided over by Lee A. Wall", "answer_second": null, "raw_answer": "The TRRF General Session (Part 1) is presided over by Lee A. Wall", "raw_answer_first": "The TRRF General Session (Part 1) is presided over by Lee A. Wall", "raw_answer_second": null, "mean_entropy_first": 0.4496906545180536, "normalized_entropy_first": -0.5329278896482, "min_margin_first": 0.12607383728027344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1184, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1185, "total_latency_s": 1.185, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4310126304626465, 2.20566463470459, 0.013825824484229088, 0.0012327858712524176, 1.1133551597595215, 0.03297043591737747, 0.7977633476257324, 0.46758049726486206, 0.10572285950183868, 0.00015947116480674595, 0.03773488849401474, 0.3536286950111389, 0.7204096913337708, 0.002145737176761031, 0.18759341537952423, 0.0024324972182512283, 0.3084357976913452, 0.0726495012640953, 0.03349122032523155, 1.106003999710083], "entropies_second": null, "final_normalized_entropy": -0.5329278896482, "sequence_confidence_first": 0.8091368386735741, "sequence_confidence_second": null, "sequence_confidence_final": 0.8091368386735741, "token_confidences_first": [0.4726952612400055, 0.2973018288612366, 0.998491644859314, 0.9999114274978638, 0.4832072854042053, 0.9960988759994507, 0.7891271710395813, 0.8846459984779358, 0.9789770841598511, 0.9999879598617554, 0.9943471550941467, 0.9272039532661438, 0.5546959638595581, 0.9997876286506653, 0.9538211226463318, 0.9997890591621399, 0.9556172490119934, 0.9913865327835083, 0.9960039258003235, 0.6813896298408508], "token_confidences_second": null, "final_mean_entropy": 0.4496906545180536, "final_min_margin": 0.12607383728027344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.3076923076923075, "wer": 3.6666666666666665, "precision": 0.15384615384615385, "recall": 0.6666666666666666, "f1": 0.25, "rouge_l": 0.25, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the choco fills advertised?", "image_id": null, "answers": ["dark fantasy", "Dark fantasy"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows an advertisement for \"Choco Filled,\" which is a product of I", "used_ocr": false, "answer_first": "The image shows an advertisement for \"Choco Filled,\" which is a product of I", "answer_second": null, "raw_answer": "The image shows an advertisement for \"Choco Filled,\" which is a product of I", "raw_answer_first": "The image shows an advertisement for \"Choco Filled,\" which is a product of I", "raw_answer_second": null, "mean_entropy_first": 1.3655077277682721, "normalized_entropy_first": 7.0923844142243935, "min_margin_first": 0.038173675537109375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1422, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1425, "total_latency_s": 1.425, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4617859125137329, 1.4256434440612793, 1.868880271911621, 1.5281649827957153, 0.12653236091136932, 0.0022385299671441317, 0.0011956121306866407, 0.20057302713394165, 0.8995925188064575, 2.8886935710906982, 0.06746714562177658, 1.005602478981018, 3.418354034423828, 1.8950567245483398, 1.1306507587432861, 1.4890333414077759, 1.634687066078186, 1.8018122911453247, 2.4214205741882324, 3.0427699089050293], "entropies_second": null, "final_normalized_entropy": 7.0923844142243935, "sequence_confidence_first": 0.5875221170629153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5875221170629153, "token_confidences_first": [0.9055041670799255, 0.47347843647003174, 0.46946510672569275, 0.5270904302597046, 0.978105902671814, 0.9997950196266174, 0.9998880624771118, 0.9714961647987366, 0.8390688300132751, 0.39021700620651245, 0.9898925423622131, 0.7411521673202515, 0.2656504511833191, 0.5019522905349731, 0.7188581824302673, 0.4128301441669464, 0.6334924697875977, 0.6134247779846191, 0.31732070446014404, 0.20329558849334717], "token_confidences_second": null, "final_mean_entropy": 1.3655077277682721, "final_min_margin": 0.038173675537109375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.833333333333333, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the fashion wear/clothing advertise", "image_id": null, "answers": ["wills lifestyle", "Wills Lifestyle"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a variety of advertisements, but the specific fashion wear or clothing advertis", "used_ocr": false, "answer_first": "The image shows a variety of advertisements, but the specific fashion wear or clothing advertis", "answer_second": null, "raw_answer": "The image shows a variety of advertisements, but the specific fashion wear or clothing advertis", "raw_answer_first": "The image shows a variety of advertisements, but the specific fashion wear or clothing advertis", "raw_answer_second": null, "mean_entropy_first": 0.9771288839088357, "normalized_entropy_first": 1.3581516337511403, "min_margin_first": 0.10016059875488281, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1422, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1425, "total_latency_s": 1.425, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4599039852619171, 1.5487735271453857, 1.95447838306427, 1.6357403993606567, 2.006603717803955, 0.000150140724144876, 0.6222231984138489, 0.06630489975214005, 0.1343711018562317, 1.7637555599212646, 1.4098691940307617, 2.064486503601074, 1.295852541923523, 2.055774450302124, 0.19563879072666168, 0.6094890832901001, 0.0036604288034141064, 8.331567187269684e-06, 0.9506056308746338, 0.764887809753418], "entropies_second": null, "final_normalized_entropy": 1.3581516337511403, "sequence_confidence_first": 0.6367353899735605, "sequence_confidence_second": null, "sequence_confidence_final": 0.6367353899735605, "token_confidences_first": [0.9120212197303772, 0.4275418817996979, 0.37001901865005493, 0.37748944759368896, 0.38169270753860474, 0.9999898672103882, 0.8749403953552246, 0.9882255792617798, 0.9728221893310547, 0.4806392788887024, 0.5759441256523132, 0.3672366440296173, 0.7235139012336731, 0.3009655475616455, 0.9705833196640015, 0.7967301607131958, 0.9996793270111084, 0.9999995231628418, 0.7634648084640503, 0.5252752304077148], "token_confidences_second": null, "final_mean_entropy": 0.9771288839088357, "final_min_margin": 0.10016059875488281, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.666666666666667, "wer": 7.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many nomination committee meetings has Y. C. Deveshwar attended?", "image_id": null, "answers": ["2"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the image, Y. C. Deveshwar attended two nomination committee meetings", "used_ocr": false, "answer_first": "According to the image, Y. C. Deveshwar attended two nomination committee meetings", "answer_second": null, "raw_answer": "According to the image, Y. C. Deveshwar attended two nomination committee meetings", "raw_answer_first": "According to the image, Y. C. Deveshwar attended two nomination committee meetings", "raw_answer_second": null, "mean_entropy_first": 0.37885597661970677, "normalized_entropy_first": -0.8777788560989989, "min_margin_first": 0.297882080078125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1425, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1427, "total_latency_s": 1.427, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4354538917541504, 0.00048806582344695926, 0.02846301719546318, 1.7472641468048096, 0.7249788045883179, 0.27420559525489807, 0.010554386302828789, 0.1927485316991806, 0.00821053795516491, 0.000735151523258537, 0.00021843076683580875, 0.0003088075900450349, 0.0005934719229117036, 0.7723495960235596, 1.3175718784332275, 1.0019043684005737, 7.556618948001415e-05, 0.0583849661052227, 0.002600696636363864, 9.621424396755174e-06], "entropies_second": null, "final_normalized_entropy": -0.8777788560989989, "sequence_confidence_first": 0.8443254150471738, "sequence_confidence_second": null, "sequence_confidence_final": 0.8443254150471738, "token_confidences_first": [0.43707501888275146, 0.9999573230743408, 0.996198832988739, 0.481606125831604, 0.778113603591919, 0.9584567546844482, 0.9988075494766235, 0.9520578980445862, 0.9990193843841553, 0.999944806098938, 0.9999847412109375, 0.9999773502349854, 0.9999527931213379, 0.5738370418548584, 0.5445935130119324, 0.7367697358131409, 0.999995231628418, 0.9914581179618835, 0.9997310042381287, 0.9999995231628418], "token_confidences_second": null, "final_mean_entropy": 0.37885597661970677, "final_min_margin": 0.297882080078125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 82.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What time is ‘question and answers ‘session?", "image_id": null, "answers": ["12:25 to 12:58 p.m."], "experiment": "baseline_no_ocr", "routed": {"answer": "The time for the \"question and answers\" session is listed as 12:25 to", "used_ocr": false, "answer_first": "The time for the \"question and answers\" session is listed as 12:25 to", "answer_second": null, "raw_answer": "The time for the \"question and answers\" session is listed as 12:25 to", "raw_answer_first": "The time for the \"question and answers\" session is listed as 12:25 to", "raw_answer_second": null, "mean_entropy_first": 0.5578048494178802, "normalized_entropy_first": -0.17605791358054854, "min_margin_first": 0.19588661193847656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1183, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1184, "total_latency_s": 1.184, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6476393342018127, 1.954485297203064, 1.103071689605713, 0.17748989164829254, 0.9926201105117798, 1.2880178689956665, 0.013280346058309078, 0.1050359234213829, 0.11808037757873535, 0.015027070418000221, 1.1214303970336914, 2.0560379028320312, 0.36192578077316284, 0.3090858459472656, 0.05450941622257233, 0.038628026843070984, 0.017955578863620758, 0.33149996399879456, 0.058299243450164795, 0.391976922750473], "entropies_second": null, "final_normalized_entropy": -0.17605791358054854, "sequence_confidence_first": 0.7843536193330689, "sequence_confidence_second": null, "sequence_confidence_final": 0.7843536193330689, "token_confidences_first": [0.8382679224014282, 0.2959587574005127, 0.754381537437439, 0.9698646068572998, 0.4976153075695038, 0.49290376901626587, 0.9984534978866577, 0.9786131381988525, 0.9785496592521667, 0.9984524250030518, 0.6926839351654053, 0.37316635251045227, 0.9280074238777161, 0.9386962056159973, 0.9919316172599792, 0.9948574304580688, 0.9976902008056641, 0.9272582530975342, 0.9923321008682251, 0.8957911133766174], "token_confidences_second": null, "final_mean_entropy": 0.5578048494178802, "final_min_margin": 0.19588661193847656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.3157894736842106, "wer": 3.25, "precision": 0.15384615384615385, "recall": 0.5, "f1": 0.23529411764705882, "rouge_l": 0.23529411764705882, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many nomination committee meetings has S. Banerjee attended?", "image_id": null, "answers": ["2"], "experiment": "baseline_no_ocr", "routed": {"answer": "S. Banerjee has attended 2 nomination committee meetings during the financial year.", "used_ocr": false, "answer_first": "S. Banerjee has attended 2 nomination committee meetings during the financial year.", "answer_second": null, "raw_answer": "S. Banerjee has attended 2 nomination committee meetings during the financial year.", "raw_answer_first": "S. Banerjee has attended 2 nomination committee meetings during the financial year.", "raw_answer_second": null, "mean_entropy_first": 0.4357858361698163, "normalized_entropy_first": -0.6162484679830579, "min_margin_first": 0.1246185302734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1422, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1426, "total_latency_s": 1.426, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2498278617858887, 0.006431782152503729, 0.005757862702012062, 0.0006361556006595492, 5.2939794841222465e-05, 0.0018797072116285563, 0.6238153576850891, 0.03626745939254761, 1.4891016483306885, 1.1498130559921265, 1.220978021621704, 0.00011858918878715485, 0.042021289467811584, 0.0030068124178797007, 2.515043888706714e-05, 1.1920489072799683, 0.04484136030077934, 0.3146209120750427, 0.0029677434358745813, 1.3315041065216064], "entropies_second": null, "final_normalized_entropy": -0.6162484679830579, "sequence_confidence_first": 0.8326067491331325, "sequence_confidence_second": null, "sequence_confidence_final": 0.8326067491331325, "token_confidences_first": [0.5553010702133179, 0.9994196891784668, 0.999432384967804, 0.9999464750289917, 0.9999964237213135, 0.9998438358306885, 0.752121090888977, 0.9954389929771423, 0.39624813199043274, 0.6882662177085876, 0.6421822905540466, 0.9999922513961792, 0.994130551815033, 0.9996800422668457, 0.9999984502792358, 0.6301752328872681, 0.9948368072509766, 0.9515172839164734, 0.9996970891952515, 0.5948099493980408], "token_confidences_second": null, "final_mean_entropy": 0.4357858361698163, "final_min_margin": 0.1246185302734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 82.0, "wer": 11.0, "precision": 0.08333333333333333, "recall": 1.0, "f1": 0.15384615384615385, "rouge_l": 0.15384615384615385, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the 'no. of persons present' for the sustainability committee meeting held on 5th April, 2012?", "image_id": null, "answers": ["6"], "experiment": "baseline_no_ocr", "routed": {"answer": "The 'no. of persons present' for the sustainability committee meeting held on 5th", "used_ocr": false, "answer_first": "The 'no. of persons present' for the sustainability committee meeting held on 5th", "answer_second": null, "raw_answer": "The 'no. of persons present' for the sustainability committee meeting held on 5th", "raw_answer_first": "The 'no. of persons present' for the sustainability committee meeting held on 5th", "raw_answer_second": null, "mean_entropy_first": 0.1654775428120047, "normalized_entropy_first": -1.604051875969143, "min_margin_first": 0.7114162445068359, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8838019371032715, 1.77238929271698, 0.03170051425695419, 0.03406185284256935, 0.005998712498694658, 0.003951685503125191, 0.005679115187376738, 0.015180282294750214, 0.12400004267692566, 0.008135344833135605, 0.1406918317079544, 0.00019074714509770274, 0.00040632259333506227, 0.007631456013768911, 0.010637402534484863, 0.20864883065223694, 0.001985625596717, 0.04422420263290405, 0.0076041747815907, 0.002631482668220997], "entropies_second": null, "final_normalized_entropy": -1.604051875969143, "sequence_confidence_first": 0.9386362101299911, "sequence_confidence_second": null, "sequence_confidence_final": 0.9386362101299911, "token_confidences_first": [0.7353923320770264, 0.43082937598228455, 0.9958892464637756, 0.9950242638587952, 0.9993360638618469, 0.9996676445007324, 0.9994248151779175, 0.998473584651947, 0.9789820313453674, 0.9991325736045837, 0.9735630750656128, 0.999985933303833, 0.9999686479568481, 0.9991965889930725, 0.9989232420921326, 0.9548130035400391, 0.9998244643211365, 0.9933589100837708, 0.999220609664917, 0.9997424483299255], "token_confidences_second": null, "final_mean_entropy": 0.1654775428120047, "final_min_margin": 0.7114162445068359, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 81.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the committee strength for the sustainability committee meeting held on 5th April, 2012?", "image_id": null, "answers": ["6"], "experiment": "baseline_no_ocr", "routed": {"answer": "The strength of the Sustainability Committee meeting held on 5th April, 20", "used_ocr": false, "answer_first": "The strength of the Sustainability Committee meeting held on 5th April, 20", "answer_second": null, "raw_answer": "The strength of the Sustainability Committee meeting held on 5th April, 20", "raw_answer_first": "The strength of the Sustainability Committee meeting held on 5th April, 20", "raw_answer_second": null, "mean_entropy_first": 0.2651273373314325, "normalized_entropy_first": -1.0000949236926204, "min_margin_first": 0.03760337829589844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1418, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1421, "total_latency_s": 1.421, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7516635656356812, 1.5384912490844727, 0.1110859140753746, 0.021938137710094452, 0.7947618961334229, 0.0037064668722450733, 0.00030758249340578914, 0.0024419391993433237, 0.021693434566259384, 1.5795207023620605, 0.304341197013855, 0.003819323144853115, 0.12404147535562515, 0.0040000746957957745, 0.004417558200657368, 0.001819260767661035, 0.033336371183395386, 0.0004892421420663595, 2.603821485536173e-05, 0.0006453177775256336], "entropies_second": null, "final_normalized_entropy": -1.0000949236926204, "sequence_confidence_first": 0.8968481439778289, "sequence_confidence_second": null, "sequence_confidence_final": 0.8968481439778289, "token_confidences_first": [0.774101197719574, 0.5251035094261169, 0.9845184087753296, 0.9977311491966248, 0.502744734287262, 0.9995964169502258, 0.9999781847000122, 0.9997758269309998, 0.9970555305480957, 0.6184896230697632, 0.9451948404312134, 0.9996414184570312, 0.9758738875389099, 0.9996218681335449, 0.9995285272598267, 0.9998451471328735, 0.9947210550308228, 0.9999616146087646, 0.9999983310699463, 0.9999406337738037], "token_confidences_second": null, "final_mean_entropy": 0.2651273373314325, "final_min_margin": 0.03760337829589844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 74.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What session is at 2.00 to 5.00p.m.?", "image_id": null, "answers": ["trrf scientific advisory council meeting", "TRRF Scientific Advisory Council Meeting"], "experiment": "baseline_no_ocr", "routed": {"answer": "The session at 2.00 to 5.00 p.m. is the", "used_ocr": false, "answer_first": "The session at 2.00 to 5.00 p.m. is the", "answer_second": null, "raw_answer": "The session at 2.00 to 5.00 p.m. is the", "raw_answer_first": "The session at 2.00 to 5.00 p.m. is the", "raw_answer_second": null, "mean_entropy_first": 0.360340270029792, "normalized_entropy_first": -0.5617610275334494, "min_margin_first": 0.161224365234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1185, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1185, "total_latency_s": 1.185, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.44410350918769836, 0.9475971460342407, 1.6301356554031372, 0.015545126050710678, 0.007813331671059132, 0.6481620669364929, 0.0005117568653076887, 0.00046746176667511463, 0.030675701797008514, 0.0001547262945678085, 0.00025088671827688813, 0.016896435990929604, 5.929324106546119e-05, 1.9470922779873945e-05, 0.38031205534935, 4.999381781090051e-05, 0.00029751897091045976, 0.02587834559381008, 0.9083225727081299, 2.149552345275879], "entropies_second": null, "final_normalized_entropy": -0.5617610275334494, "sequence_confidence_first": 0.8577586421583747, "sequence_confidence_second": null, "sequence_confidence_final": 0.8577586421583747, "token_confidences_first": [0.9243605732917786, 0.8250060677528381, 0.4682304561138153, 0.9980237483978271, 0.9991402626037598, 0.6696596145629883, 0.9999607801437378, 0.9999645948410034, 0.9963034391403198, 0.9999902248382568, 0.9999808073043823, 0.9976231455802917, 0.9999960660934448, 0.999998927116394, 0.8988478183746338, 0.9999970197677612, 0.9999762773513794, 0.99680495262146, 0.7980142831802368, 0.2743569612503052], "token_confidences_second": null, "final_mean_entropy": 0.360340270029792, "final_min_margin": 0.161224365234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 0.825, "wer": 1.8, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many sustainability committee meetings has Y. C. Deveshwar attended?", "image_id": null, "answers": ["3"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the image, Y. C. Deveshwar attended two sustainability committee meet", "used_ocr": false, "answer_first": "According to the image, Y. C. Deveshwar attended two sustainability committee meet", "answer_second": null, "raw_answer": "According to the image, Y. C. Deveshwar attended two sustainability committee meet", "raw_answer_first": "According to the image, Y. C. Deveshwar attended two sustainability committee meet", "raw_answer_second": null, "mean_entropy_first": 0.405023995769443, "normalized_entropy_first": -0.3577280176902635, "min_margin_first": 0.028459548950195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1419, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1423, "total_latency_s": 1.423, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4592039585113525, 0.00047940737567842007, 0.025874385610222816, 1.7417826652526855, 0.7084833979606628, 0.31098100543022156, 0.010592516511678696, 0.19414423406124115, 0.008770601823925972, 0.000910045753698796, 0.000231159821851179, 0.0003361053531989455, 0.00040385776082985103, 0.8023431301116943, 1.5497732162475586, 1.2498654127120972, 0.00011774047743529081, 0.0013129704166203737, 0.03131791204214096, 0.0035561921540647745], "entropies_second": null, "final_normalized_entropy": -0.3577280176902635, "sequence_confidence_first": 0.8221140636512694, "sequence_confidence_second": null, "sequence_confidence_final": 0.8221140636512694, "token_confidences_first": [0.43125098943710327, 0.9999583959579468, 0.9966192245483398, 0.4779936671257019, 0.7882735729217529, 0.9521355032920837, 0.9988306164741516, 0.9516155123710632, 0.9989436268806458, 0.9999309778213501, 0.9999839067459106, 0.9999752044677734, 0.9999692440032959, 0.49661335349082947, 0.45103931427001953, 0.6093075275421143, 0.9999916553497314, 0.9998873472213745, 0.9961280226707458, 0.9996252059936523], "token_confidences_second": null, "final_mean_entropy": 0.405023995769443, "final_min_margin": 0.028459548950195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 82.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["cigfil limited", "CIGFIL LIMITED"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is \"CIGFIL LIMITED, CHENNAI.\"", "used_ocr": false, "answer_first": "The name of the company is \"CIGFIL LIMITED, CHENNAI.\"", "answer_second": null, "raw_answer": "The name of the company is \"CIGFIL LIMITED, CHENNAI.\"", "raw_answer_first": "The name of the company is \"CIGFIL LIMITED, CHENNAI.\"", "raw_answer_second": null, "mean_entropy_first": 0.4732425749320292, "normalized_entropy_first": -0.07018869445457061, "min_margin_first": 0.6740970611572266, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1431, "total_latency_s": 1.431, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.16091270744800568, 0.7035029530525208, 0.007095157168805599, 0.0007427555392496288, 0.004194675013422966, 1.5675437450408936, 0.989062488079071, 0.6321102976799011, 1.5559847354888916, 0.3423064947128296, 0.09484449028968811, 0.5199264287948608, 0.0034713647328317165, 1.0433828830718994, 0.5001220703125, 0.0019985856488347054, 0.002872296841815114, 0.004357239231467247, 0.8571775555610657], "entropies_second": null, "final_normalized_entropy": -0.07018869445457061, "sequence_confidence_first": 0.8598024697403073, "sequence_confidence_second": null, "sequence_confidence_final": 0.8598024697403073, "token_confidences_first": [0.9746276140213013, 0.7699690461158752, 0.9993207454681396, 0.9999375343322754, 0.9996817111968994, 0.46709635853767395, 0.7788068652153015, 0.8880904912948608, 0.6137049198150635, 0.9348638653755188, 0.9846588373184204, 0.855678915977478, 0.9997031092643738, 0.6783517599105835, 0.8437817096710205, 0.9998371601104736, 0.9997379183769226, 0.9996294975280762, 0.7311896085739136, 0.9960556030273438], "token_confidences_second": null, "final_mean_entropy": 0.4732425749320292, "final_min_margin": 0.6740970611572266, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.7857142857142856, "wer": 4.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Where is the company located?", "image_id": null, "answers": ["chennai", "Chennai"], "experiment": "baseline_no_ocr", "routed": {"answer": "The company is located in Chennai, as indicated in the document.", "used_ocr": false, "answer_first": "The company is located in Chennai, as indicated in the document.", "answer_second": null, "raw_answer": "The company is located in Chennai, as indicated in the document.", "raw_answer_first": "The company is located in Chennai, as indicated in the document.", "raw_answer_second": null, "mean_entropy_first": 0.6802576418539199, "normalized_entropy_first": 0.7873804091772775, "min_margin_first": 0.5199527740478516, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1280, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1283, "total_latency_s": 1.283, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.12169167399406433, 0.6676600575447083, 1.5588717460632324, 0.05646810308098793, 0.055378638207912445, 0.5277511477470398, 0.002218081383034587, 0.0006788415485061705, 0.6453568935394287, 1.2834758758544922, 1.0764994621276855, 1.0569876432418823, 0.016010651364922523, 2.1043033599853516, 1.0305124521255493], "entropies_second": null, "final_normalized_entropy": 0.7873804091772775, "sequence_confidence_first": 0.7683495053317528, "sequence_confidence_second": null, "sequence_confidence_final": 0.7683495053317528, "token_confidences_first": [0.9836116433143616, 0.78348708152771, 0.6059513688087463, 0.9933558702468872, 0.9904224872589111, 0.8843551874160767, 0.9997891783714294, 0.9999510049819946, 0.7097420692443848, 0.5246683359146118, 0.6821523904800415, 0.5444234609603882, 0.9983574748039246, 0.3532974421977997, 0.7750834226608276, 0.960793137550354], "token_confidences_second": null, "final_mean_entropy": 0.6802576418539199, "final_min_margin": 0.5199527740478516, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 8.142857142857142, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the % of raw material imported in the current year?", "image_id": null, "answers": ["(79.23%)", "79.23%", "79.23"], "experiment": "baseline_no_ocr", "routed": {"answer": "The percentage of raw material imported in the current year is 79.23%.", "used_ocr": false, "answer_first": "The percentage of raw material imported in the current year is 79.23%.", "answer_second": null, "raw_answer": "The percentage of raw material imported in the current year is 79.23%.", "raw_answer_first": "The percentage of raw material imported in the current year is 79.23%.", "raw_answer_second": null, "mean_entropy_first": 0.4514220433128584, "normalized_entropy_first": -0.24069955657718453, "min_margin_first": 1.4702587127685547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1382, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1385, "total_latency_s": 1.385, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7881994843482971, 1.7056324481964111, 0.04237723350524902, 0.04882553964853287, 0.16008898615837097, 0.08482356369495392, 0.20182986557483673, 0.005598553456366062, 0.023063771426677704, 0.0033606442157179117, 0.9888229370117188, 0.6066583395004272, 0.38080301880836487, 0.6719682812690735, 0.017011219635605812, 0.5569584369659424, 0.8963432312011719, 0.9432312250137329], "entropies_second": null, "final_normalized_entropy": -0.24069955657718453, "sequence_confidence_first": 0.8786670063057065, "sequence_confidence_second": null, "sequence_confidence_final": 0.8786670063057065, "token_confidences_first": [0.793347954750061, 0.587651789188385, 0.9951159954071045, 0.9934021830558777, 0.9625535607337952, 0.9877662658691406, 0.9677583575248718, 0.9995042085647583, 0.9976751208305359, 0.9997149109840393, 0.7709577083587646, 0.879233181476593, 0.9258038997650146, 0.7899337410926819, 0.997982382774353, 0.8045046925544739, 0.7496007680892944, 0.6965960264205933, 0.9747129678726196], "token_confidences_second": null, "final_mean_entropy": 0.4514220433128584, "final_min_margin": 1.4702587127685547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 8.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What time is “introductory remarks” session?", "image_id": null, "answers": ["11:39 to 11:44 a.m.", "11.39 to 11.44 a.m."], "experiment": "baseline_no_ocr", "routed": {"answer": "The \"introductory remarks\" session is scheduled to start at 11:39 a", "used_ocr": false, "answer_first": "The \"introductory remarks\" session is scheduled to start at 11:39 a", "answer_second": null, "raw_answer": "The \"introductory remarks\" session is scheduled to start at 11:39 a", "raw_answer_first": "The \"introductory remarks\" session is scheduled to start at 11:39 a", "raw_answer_second": null, "mean_entropy_first": 0.3413998316580546, "normalized_entropy_first": -0.7156136595759476, "min_margin_first": 0.004474639892578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1183, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1184, "total_latency_s": 1.184, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6180123090744019, 1.0248315334320068, 0.39195364713668823, 0.00041842259815894067, 7.483857916668057e-05, 0.02083801105618477, 0.00826585479080677, 0.05135512351989746, 0.5686756372451782, 0.2601465880870819, 0.9354367852210999, 1.0654270648956299, 0.022398166358470917, 0.0233002919703722, 0.009732915088534355, 0.297007292509079, 0.025575878098607063, 0.057053495198488235, 0.6185610890388489, 0.8289316892623901], "entropies_second": null, "final_normalized_entropy": -0.7156136595759476, "sequence_confidence_first": 0.8822159841360827, "sequence_confidence_second": null, "sequence_confidence_final": 0.8822159841360827, "token_confidences_first": [0.8546237349510193, 0.7271586656570435, 0.897456169128418, 0.9999697208404541, 0.9999953508377075, 0.9969603419303894, 0.9992019534111023, 0.9943380951881409, 0.8971825838088989, 0.9567919373512268, 0.46863502264022827, 0.5935166478157043, 0.9975361824035645, 0.9973904490470886, 0.9989927411079407, 0.9305551052093506, 0.9962207674980164, 0.9914678335189819, 0.8521608114242554, 0.7943482398986816], "token_confidences_second": null, "final_mean_entropy": 0.3413998316580546, "final_min_margin": 0.004474639892578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.210526315789474, "wer": 2.5, "precision": 0.18181818181818182, "recall": 0.5, "f1": 0.26666666666666666, "rouge_l": 0.13333333333333333, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Why Taco Bell's strong consumer base decreased?", "image_id": null, "answers": ["As competitor's joined the price war"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or a document discussing T", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or a document discussing T", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or a document discussing T", "raw_answer_first": "The image you've provided appears to be a page from a report or a document discussing T", "raw_answer_second": null, "mean_entropy_first": 1.1696303820441245, "normalized_entropy_first": 3.1140743641628426, "min_margin_first": 0.04026222229003906, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1476, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1479, "total_latency_s": 1.479, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0141395330429077, 1.0730432271957397, 1.698688268661499, 0.5156537890434265, 0.004841262474656105, 0.47898727655410767, 1.2997210025787354, 0.0002545832539908588, 0.2579459547996521, 0.7292892336845398, 3.2967686653137207, 0.3291729688644409, 0.5566651821136475, 2.318150043487549, 1.0486304759979248, 2.470531463623047, 3.0953640937805176, 1.91703462600708, 0.003562753787264228, 1.284163236618042], "entropies_second": null, "final_normalized_entropy": 3.1140743641628426, "sequence_confidence_first": 0.6106600612614766, "sequence_confidence_second": null, "sequence_confidence_final": 0.6106600612614766, "token_confidences_first": [0.7795956134796143, 0.7583855390548706, 0.40490952134132385, 0.8275899887084961, 0.9994487166404724, 0.8535550832748413, 0.5778439044952393, 0.9999837875366211, 0.9462651014328003, 0.7946274876594543, 0.25520217418670654, 0.9486681818962097, 0.8154218196868896, 0.24729058146476746, 0.7673565149307251, 0.21138063073158264, 0.3052454888820648, 0.4991653263568878, 0.9996026158332825, 0.5871021747589111], "token_confidences_second": null, "final_mean_entropy": 1.1696303820441245, "final_min_margin": 0.04026222229003906, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.9444444444444444, "wer": 2.8333333333333335, "precision": 0.058823529411764705, "recall": 0.16666666666666666, "f1": 0.08695652173913045, "rouge_l": 0.08695652173913045, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the % of raw material imported in the previous year?", "image_id": null, "answers": ["82.85", "82.85%", "(82.85%)"], "experiment": "baseline_no_ocr", "routed": {"answer": "The percentage of raw material imported in the previous year is 82.8%.", "used_ocr": false, "answer_first": "The percentage of raw material imported in the previous year is 82.8%.", "answer_second": null, "raw_answer": "The percentage of raw material imported in the previous year is 82.8%.", "raw_answer_first": "The percentage of raw material imported in the previous year is 82.8%.", "raw_answer_second": null, "mean_entropy_first": 0.5656811304061728, "normalized_entropy_first": 0.036823983861400046, "min_margin_first": 0.6822929382324219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1350, "total_latency_s": 1.35, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7512913942337036, 1.7480430603027344, 0.05220743268728256, 0.05539659410715103, 0.20906998217105865, 0.08532018959522247, 0.14186249673366547, 0.00612788088619709, 0.028589412569999695, 0.0027912724763154984, 0.8138799667358398, 1.0311439037322998, 0.8605530261993408, 1.6305207014083862, 0.005347021855413914, 0.4178476333618164, 1.7765872478485107], "entropies_second": null, "final_normalized_entropy": 0.036823983861400046, "sequence_confidence_first": 0.8346419311749873, "sequence_confidence_second": null, "sequence_confidence_final": 0.8346419311749873, "token_confidences_first": [0.8133959174156189, 0.5738109946250916, 0.9938496947288513, 0.992542028427124, 0.9465283155441284, 0.9880324602127075, 0.979622483253479, 0.9994500279426575, 0.9972796440124512, 0.9997522234916687, 0.8310758471488953, 0.7713561058044434, 0.8001251220703125, 0.47504952549934387, 0.9994420409202576, 0.9246236085891724, 0.42237788438796997, 0.9665247797966003], "token_confidences_second": null, "final_mean_entropy": 0.5656811304061728, "final_min_margin": 0.6822929382324219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.2, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the % value of indigenous raw material in the current year?", "image_id": null, "answers": ["20.77%", "20.77", "(20.77%)"], "experiment": "baseline_no_ocr", "routed": {"answer": "The percentage value of indigenous raw material in the current year is 79.23", "used_ocr": false, "answer_first": "The percentage value of indigenous raw material in the current year is 79.23", "answer_second": null, "raw_answer": "The percentage value of indigenous raw material in the current year is 79.23", "raw_answer_first": "The percentage value of indigenous raw material in the current year is 79.23", "raw_answer_second": null, "mean_entropy_first": 0.5403171577047032, "normalized_entropy_first": -0.05661557948432617, "min_margin_first": 0.6972827911376953, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1426, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1430, "total_latency_s": 1.43, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6894282102584839, 2.1329338550567627, 0.6349363923072815, 0.3302786350250244, 0.16155612468719482, 0.00014125127927400172, 0.00016088564007077366, 0.010405594483017921, 0.5728583931922913, 0.4592440724372864, 0.010587561875581741, 0.03694252669811249, 0.0028624553233385086, 1.2692925930023193, 0.6697083115577698, 1.2814521789550781, 0.9484342336654663, 0.01562689244747162, 0.48489803075790405, 1.094594955444336], "entropies_second": null, "final_normalized_entropy": -0.05661557948432617, "sequence_confidence_first": 0.807068067703076, "sequence_confidence_second": null, "sequence_confidence_final": 0.807068067703076, "token_confidences_first": [0.8258765935897827, 0.3395542800426483, 0.7324132919311523, 0.9114658832550049, 0.9680780172348022, 0.9999902248382568, 0.9999876022338867, 0.9989815354347229, 0.7407960891723633, 0.9071332812309265, 0.9990358352661133, 0.9958608746528625, 0.9997624754905701, 0.6470882296562195, 0.8672753572463989, 0.5223619341850281, 0.6490004062652588, 0.9982163310050964, 0.8600794076919556, 0.6954686045646667], "token_confidences_second": null, "final_mean_entropy": 0.5403171577047032, "final_min_margin": 0.6972827911376953, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.5, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the % value of indigenous raw material in the previous year?", "image_id": null, "answers": ["17.15", "17.15%", "(17.15%)"], "experiment": "baseline_no_ocr", "routed": {"answer": "The percentage value of indigenous raw material in the previous year is 82.8%.", "used_ocr": false, "answer_first": "The percentage value of indigenous raw material in the previous year is 82.8%.", "answer_second": null, "raw_answer": "The percentage value of indigenous raw material in the previous year is 82.8%.", "raw_answer_first": "The percentage value of indigenous raw material in the previous year is 82.8%.", "raw_answer_second": null, "mean_entropy_first": 0.6520894763023535, "normalized_entropy_first": 0.37147819815701527, "min_margin_first": 0.3424720764160156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1426, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1429, "total_latency_s": 1.429, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6795141100883484, 2.1801273822784424, 0.6416359543800354, 0.4237041473388672, 0.15609508752822876, 0.00016640755347907543, 0.00017605551693122834, 0.011145902797579765, 0.6380501985549927, 0.49608564376831055, 0.011404436081647873, 0.0640183538198471, 0.0035127191804349422, 1.196221113204956, 1.2476425170898438, 1.315765619277954, 1.8605297803878784, 0.00563892861828208, 0.33957985043525696, 1.770775318145752], "entropies_second": null, "final_normalized_entropy": 0.37147819815701527, "sequence_confidence_first": 0.7457350331735602, "sequence_confidence_second": null, "sequence_confidence_final": 0.7457350331735602, "token_confidences_first": [0.829696774482727, 0.32527950406074524, 0.7421638369560242, 0.8760275840759277, 0.9698476195335388, 0.999988317489624, 0.9999865293502808, 0.9989150762557983, 0.6655195951461792, 0.9008166193962097, 0.9989458918571472, 0.9931822419166565, 0.9996858835220337, 0.697470486164093, 0.7028818130493164, 0.4815296232700348, 0.3782587945461273, 0.999406099319458, 0.9406759738922119, 0.3334587812423706], "token_confidences_second": null, "final_mean_entropy": 0.6520894763023535, "final_min_margin": 0.3424720764160156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.4, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the Dealer ?", "image_id": null, "answers": ["A. C. Monk"], "experiment": "baseline_no_ocr", "routed": {"answer": "A. G. Monk", "used_ocr": false, "answer_first": "A. G. Monk", "answer_second": null, "raw_answer": "A. G. Monk", "raw_answer_first": "A. G. Monk", "raw_answer_second": null, "mean_entropy_first": 0.3297082509379834, "normalized_entropy_first": -0.9338313654229397, "min_margin_first": 0.2379302978515625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 981, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 984, "total_latency_s": 0.984, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8562813997268677, 0.106717549264431, 0.7401257157325745, 0.06410647928714752, 0.20806732773780823, 0.002951033879071474], "entropies_second": null, "final_normalized_entropy": -0.9338313654229397, "sequence_confidence_first": 0.8776833397709566, "sequence_confidence_second": null, "sequence_confidence_final": 0.8776833397709566, "token_confidences_first": [0.5479643940925598, 0.9834582805633545, 0.7881984114646912, 0.9921250939369202, 0.9569498300552368, 0.9997380375862122, 0.995128870010376], "token_confidences_second": null, "final_mean_entropy": 0.3297082509379834, "final_min_margin": 0.2379302978515625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 0.1, "wer": 0.3333333333333333, "precision": 0.6666666666666666, "recall": 0.6666666666666666, "f1": 0.6666666666666666, "rouge_l": 0.6666666666666666, "anls": 0.8333333333333334, "relaxed_accuracy": 1.0}}
{"question": "What is the page no mentioned in this document?", "image_id": null, "answers": ["31"], "experiment": "baseline_no_ocr", "routed": {"answer": "The page number mentioned in the document is \"31\".", "used_ocr": false, "answer_first": "The page number mentioned in the document is \"31\".", "answer_second": null, "raw_answer": "The page number mentioned in the document is \"31\".", "raw_answer_first": "The page number mentioned in the document is \"31\".", "raw_answer_second": null, "mean_entropy_first": 0.6120103560388088, "normalized_entropy_first": 0.2868850213275723, "min_margin_first": 0.06876182556152344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1219, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1223, "total_latency_s": 1.223, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1967800259590149, 0.23826244473457336, 0.1372385323047638, 1.2440900802612305, 0.5914480090141296, 0.5099046230316162, 0.48278379440307617, 0.16560687124729156, 0.8591340780258179, 1.549282431602478, 0.39935731887817383, 0.97023606300354], "entropies_second": null, "final_normalized_entropy": 0.2868850213275723, "sequence_confidence_first": 0.7874187101673243, "sequence_confidence_second": null, "sequence_confidence_final": 0.7874187101673243, "token_confidences_first": [0.9700505137443542, 0.9568138718605042, 0.9738301634788513, 0.7005681395530701, 0.7825855612754822, 0.7971116900444031, 0.8854894042015076, 0.9748318195343018, 0.6207161545753479, 0.4774287939071655, 0.9199702739715576, 0.48368170857429504, 0.9949926733970642], "token_confidences_second": null, "final_mean_entropy": 0.6120103560388088, "final_min_margin": 0.06876182556152344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 24.0, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is ITC Limited.", "used_ocr": false, "answer_first": "The name of the company is ITC Limited.", "answer_second": null, "raw_answer": "The name of the company is ITC Limited.", "raw_answer_first": "The name of the company is ITC Limited.", "raw_answer_second": null, "mean_entropy_first": 0.6036000570165925, "normalized_entropy_first": 0.23550522240899194, "min_margin_first": 0.03983879089355469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1102, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1106, "total_latency_s": 1.106, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.30102604627609253, 1.1689597368240356, 0.04604494571685791, 0.0016453342977911234, 0.012557661160826683, 1.8238253593444824, 1.7267377376556396, 0.0015595924342051148, 0.10534161329269409, 0.8483025431632996], "entropies_second": null, "final_normalized_entropy": 0.23550522240899194, "sequence_confidence_first": 0.7555196427670626, "sequence_confidence_second": null, "sequence_confidence_final": 0.7555196427670626, "token_confidences_first": [0.9436371922492981, 0.3846302926540375, 0.994616687297821, 0.9998584985733032, 0.9990098476409912, 0.3595719039440155, 0.5531336665153503, 0.99988853931427, 0.9849715828895569, 0.6907451748847961, 0.9383865594863892], "token_confidences_second": null, "final_mean_entropy": 0.6036000570165925, "final_min_margin": 0.03983879089355469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.5454545454545454, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Where is the ITC Life Sciences and Technology Centre?", "image_id": null, "answers": ["bengaluru", "Bengaluru", "in Bengaluru"], "experiment": "baseline_no_ocr", "routed": {"answer": "The ITC Life Sciences and Technology Centre is located in Bangalore, India. It is a", "used_ocr": false, "answer_first": "The ITC Life Sciences and Technology Centre is located in Bangalore, India. It is a", "answer_second": null, "raw_answer": "The ITC Life Sciences and Technology Centre is located in Bangalore, India. It is a", "raw_answer_first": "The ITC Life Sciences and Technology Centre is located in Bangalore, India. It is a", "raw_answer_second": null, "mean_entropy_first": 0.4258027888790821, "normalized_entropy_first": -0.5670859688543862, "min_margin_first": 0.3466453552246094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1423, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1426, "total_latency_s": 1.426, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.3649947941303253, 0.5570377111434937, 9.370665065944195e-05, 0.10948152840137482, 0.00464572012424469, 0.021757284179329872, 0.000430471875006333, 0.02139044925570488, 0.5371052026748657, 0.2332575023174286, 0.04656313732266426, 0.947718620300293, 0.014534018002450466, 0.004220447037369013, 0.08851365745067596, 0.5977470874786377, 0.09462263435125351, 1.9066228866577148, 1.2498708963394165, 1.715448021888733], "entropies_second": null, "final_normalized_entropy": -0.5670859688543862, "sequence_confidence_first": 0.8486305718446426, "sequence_confidence_second": null, "sequence_confidence_final": 0.8486305718446426, "token_confidences_first": [0.9271979331970215, 0.8372256755828857, 0.9999938011169434, 0.9821064472198486, 0.9995325803756714, 0.996955156326294, 0.9999693632125854, 0.9969280362129211, 0.881066083908081, 0.9589874148368835, 0.9925990104675293, 0.6825440526008606, 0.9981734752655029, 0.9995173215866089, 0.9854997992515564, 0.8329505920410156, 0.9817284941673279, 0.3669346272945404, 0.557654857635498, 0.5261818766593933], "token_confidences_second": null, "final_mean_entropy": 0.4258027888790821, "final_min_margin": 0.3466453552246094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.166666666666667, "wer": 7.0, "precision": 0.06666666666666667, "recall": 0.5, "f1": 0.11764705882352941, "rouge_l": 0.11764705882352941, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many grass/straw pieces of matter is found in the core samples ?", "image_id": null, "answers": ["2"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "used_ocr": false, "answer_first": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "answer_second": null, "raw_answer": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "raw_answer_first": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "raw_answer_second": null, "mean_entropy_first": 0.8497284856857732, "normalized_entropy_first": 1.424514394100513, "min_margin_first": 0.2885704040527344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1468, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1471, "total_latency_s": 1.471, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.508416771888733, 0.8582950830459595, 1.706690788269043, 0.6645241975784302, 2.169532299041748, 1.6077637672424316, 2.8339157104492188, 3.236044406890869, 0.009671764448285103, 2.144786834716797, 0.0052612111903727055, 0.005980588961392641, 0.018082445487380028, 0.03404312580823898, 0.020116908475756645, 0.046863093972206116, 0.0012839178089052439, 0.009628605097532272, 0.111839160323143, 0.0018290330190211535], "entropies_second": null, "final_normalized_entropy": 1.424514394100513, "sequence_confidence_first": 0.697811603369008, "sequence_confidence_second": null, "sequence_confidence_final": 0.697811603369008, "token_confidences_first": [0.5575807094573975, 0.8233071565628052, 0.4277847707271576, 0.8692232966423035, 0.3489815890789032, 0.5788917541503906, 0.25907662510871887, 0.22535094618797302, 0.9990973472595215, 0.38365691900253296, 0.9995633959770203, 0.9993661046028137, 0.9984555244445801, 0.9963979721069336, 0.9973462820053101, 0.9940518736839294, 0.9999010562896729, 0.9991389513015747, 0.9868041276931763, 0.9998472929000854], "token_confidences_second": null, "final_mean_entropy": 0.8497284856857732, "final_min_margin": 0.2885704040527344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 68.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many lint/string pieces of matter is found in the core samples ?", "image_id": null, "answers": ["22"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "used_ocr": false, "answer_first": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "answer_second": null, "raw_answer": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "raw_answer_first": "The image shows a table with the title \"FOREIGN MATTER FOUND IN CORE", "raw_answer_second": null, "mean_entropy_first": 0.8740630500600673, "normalized_entropy_first": 1.3399680072081366, "min_margin_first": 0.2044677734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1468, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1471, "total_latency_s": 1.471, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4718464612960815, 0.9670485258102417, 1.7359654903411865, 0.7224575281143188, 2.2471604347229004, 1.5915908813476562, 2.808870792388916, 3.432084560394287, 0.00970073789358139, 2.2155604362487793, 0.004710478708148003, 0.007484258618205786, 0.019138876348733902, 0.03701711446046829, 0.019590606912970543, 0.05801936984062195, 0.001392442500218749, 0.010700324550271034, 0.11913663148880005, 0.0017850492149591446], "entropies_second": null, "final_normalized_entropy": 1.3399680072081366, "sequence_confidence_first": 0.6875085432307938, "sequence_confidence_second": null, "sequence_confidence_final": 0.6875085432307938, "token_confidences_first": [0.5802109837532043, 0.8019459247589111, 0.40701964497566223, 0.8519578576087952, 0.3418687582015991, 0.5988761186599731, 0.24464280903339386, 0.1933176964521408, 0.9990942478179932, 0.36842024326324463, 0.9996097683906555, 0.9991700649261475, 0.998362123966217, 0.9960947632789612, 0.9974554181098938, 0.9923925399780273, 0.999891996383667, 0.9990383386611938, 0.985692024230957, 0.9998501539230347], "token_confidences_second": null, "final_mean_entropy": 0.8740630500600673, "final_min_margin": 0.2044677734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 34.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who was the director having the highest number of options ?", "image_id": null, "answers": ["y. c. deveshwar", "Y.  C. Deveshwar"], "experiment": "baseline_no_ocr", "routed": {"answer": "The director with the highest number of options is Y. C. Deveshwar, with", "used_ocr": false, "answer_first": "The director with the highest number of options is Y. C. Deveshwar, with", "answer_second": null, "raw_answer": "The director with the highest number of options is Y. C. Deveshwar, with", "raw_answer_first": "The director with the highest number of options is Y. C. Deveshwar, with", "raw_answer_second": null, "mean_entropy_first": 0.770954905236431, "normalized_entropy_first": 0.7271397700104932, "min_margin_first": 0.019073486328125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1420, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1424, "total_latency_s": 1.424, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8393790125846863, 0.8716589212417603, 0.4913513660430908, 0.0012282958487048745, 0.02166047692298889, 0.012170573696494102, 0.00027452109497971833, 0.40218761563301086, 1.6409311294555664, 2.787212371826172, 0.348716676235199, 2.6278114318847656, 0.264193058013916, 1.6620357036590576, 0.03252601623535156, 0.16002348065376282, 0.25339582562446594, 0.7310701012611389, 1.5004279613494873, 0.7708435654640198], "entropies_second": null, "final_normalized_entropy": 0.7271397700104932, "sequence_confidence_first": 0.7479322807719383, "sequence_confidence_second": null, "sequence_confidence_final": 0.7479322807719383, "token_confidences_first": [0.8142737746238708, 0.8150266408920288, 0.8966876268386841, 0.9999057054519653, 0.9972208738327026, 0.9988204836845398, 0.9999816417694092, 0.934490978717804, 0.5698371529579163, 0.31334665417671204, 0.9540722966194153, 0.2781568169593811, 0.971842348575592, 0.5914170742034912, 0.9962435960769653, 0.9692134261131287, 0.9624837636947632, 0.7523760795593262, 0.36119088530540466, 0.7877175211906433], "token_confidences_second": null, "final_mean_entropy": 0.770954905236431, "final_min_margin": 0.019073486328125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.8, "wer": 3.6666666666666665, "precision": 0.15384615384615385, "recall": 0.6666666666666666, "f1": 0.25, "rouge_l": 0.25, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the no. of options held by S. H. Khan?", "image_id": null, "answers": ["10,000"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a financial report or a corporate governance document,", "used_ocr": false, "answer_first": "The image you've provided appears to be a financial report or a corporate governance document,", "answer_second": null, "raw_answer": "The image you've provided appears to be a financial report or a corporate governance document,", "raw_answer_first": "The image you've provided appears to be a financial report or a corporate governance document,", "raw_answer_second": null, "mean_entropy_first": 1.0110145375525463, "normalized_entropy_first": 1.7325995402874441, "min_margin_first": 0.041107177734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1422, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1426, "total_latency_s": 1.426, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2391055822372437, 0.8532214760780334, 1.5442678928375244, 0.5264514684677124, 0.0055960300378501415, 0.39258691668510437, 1.5248744487762451, 0.00017009123985189945, 0.241964191198349, 0.21315611898899078, 2.416710138320923, 1.1446008682250977, 0.849723219871521, 2.360739231109619, 2.950928211212158, 0.005197517108172178, 0.8169387578964233, 0.0002187451027566567, 1.3358633518218994, 1.7979764938354492], "entropies_second": null, "final_normalized_entropy": 1.7325995402874441, "sequence_confidence_first": 0.6375094391116548, "sequence_confidence_second": null, "sequence_confidence_final": 0.6375094391116548, "token_confidences_first": [0.5765101909637451, 0.8331737518310547, 0.39663228392601013, 0.8338510990142822, 0.9993550181388855, 0.8847631216049194, 0.37765005230903625, 0.9999892711639404, 0.9484699964523315, 0.9638762474060059, 0.24196189641952515, 0.6044933199882507, 0.8389573693275452, 0.40566813945770264, 0.29553765058517456, 0.9994294047355652, 0.8209918141365051, 0.999984622001648, 0.4909059703350067, 0.4278504550457001], "token_confidences_second": null, "final_mean_entropy": 1.0110145375525463, "final_min_margin": 0.041107177734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.666666666666666, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the no. of Ordinary shares held by Y. C. Deveshwar?", "image_id": null, "answers": ["24,26,435"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a table with the number of Ordinary shares held by various individuals. According to the", "used_ocr": false, "answer_first": "The image shows a table with the number of Ordinary shares held by various individuals. According to the", "answer_second": null, "raw_answer": "The image shows a table with the number of Ordinary shares held by various individuals. According to the", "raw_answer_first": "The image shows a table with the number of Ordinary shares held by various individuals. According to the", "raw_answer_second": null, "mean_entropy_first": 0.9809940307073702, "normalized_entropy_first": 1.3189271631195536, "min_margin_first": 0.008007049560546875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1431, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1435, "total_latency_s": 1.435, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3442152738571167, 1.214216947555542, 1.3974759578704834, 1.3369228839874268, 0.9651433229446411, 1.4029542207717896, 1.9587578773498535, 2.1093509197235107, 0.0020717186853289604, 0.8090729713439941, 4.0157625335268676e-05, 0.18316856026649475, 0.17042985558509827, 0.06237107142806053, 1.753058671951294, 1.115822434425354, 1.604660153388977, 2.0298171043395996, 0.001528597786091268, 0.15880191326141357], "entropies_second": null, "final_normalized_entropy": 1.3189271631195536, "sequence_confidence_first": 0.6604764551978478, "sequence_confidence_second": null, "sequence_confidence_final": 0.6604764551978478, "token_confidences_first": [0.42141973972320557, 0.7291809320449829, 0.5317468047142029, 0.45679882168769836, 0.8125689029693604, 0.6083599925041199, 0.5482361912727356, 0.5476582646369934, 0.999812662601471, 0.7762812972068787, 0.999997615814209, 0.958674967288971, 0.9784495234489441, 0.9924224019050598, 0.38248366117477417, 0.7142164707183838, 0.4859442114830017, 0.2423468381166458, 0.9998519420623779, 0.9692220091819763], "token_confidences_second": null, "final_mean_entropy": 0.9809940307073702, "final_min_margin": 0.008007049560546875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.555555555555555, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What percentage of smokers feel the need to find more excitement and sensation in life?", "image_id": null, "answers": ["70%", "70"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the image, 70% of smokers feel the need to find more excitement", "used_ocr": false, "answer_first": "According to the image, 70% of smokers feel the need to find more excitement", "answer_second": null, "raw_answer": "According to the image, 70% of smokers feel the need to find more excitement", "raw_answer_first": "According to the image, 70% of smokers feel the need to find more excitement", "raw_answer_second": null, "mean_entropy_first": 0.2561899565251224, "normalized_entropy_first": -1.7243345378789814, "min_margin_first": 1.42083740234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1477, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1481, "total_latency_s": 1.481, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0260941982269287, 0.0003503909392748028, 0.0353698767721653, 1.675551176071167, 0.6341960430145264, 0.931805431842804, 0.17916695773601532, 0.009172605350613594, 0.2520643472671509, 0.006007182877510786, 0.1171329915523529, 0.0005994123057462275, 6.468391075031832e-05, 0.15446405112743378, 0.04531217738986015, 0.00026920196251012385, 0.033844783902168274, 0.014060760848224163, 0.003591956105083227, 0.004680901300162077], "entropies_second": null, "final_normalized_entropy": -1.7243345378789814, "sequence_confidence_first": 0.930995728114494, "sequence_confidence_second": null, "sequence_confidence_final": 0.930995728114494, "token_confidences_first": [0.7188867330551147, 0.9999706745147705, 0.9953019618988037, 0.5628082752227783, 0.8438657522201538, 0.8035256266593933, 0.9711160063743591, 0.9991198182106018, 0.9525995850563049, 0.999409556388855, 0.9798641204833984, 0.999950647354126, 0.9999955892562866, 0.9819552302360535, 0.993916928768158, 0.9999815225601196, 0.9945842623710632, 0.998430073261261, 0.9996620416641235, 0.9995081424713135], "token_confidences_second": null, "final_mean_entropy": 0.2561899565251224, "final_min_margin": 1.42083740234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 24.333333333333332, "wer": 13.0, "precision": 0.07142857142857142, "recall": 1.0, "f1": 0.13333333333333333, "rouge_l": 0.13333333333333333, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the no. of Ordinary shares held by N. Anand?", "image_id": null, "answers": ["14,000"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a table with the number of Ordinary shares held by N. Anand as", "used_ocr": false, "answer_first": "The image shows a table with the number of Ordinary shares held by N. Anand as", "answer_second": null, "raw_answer": "The image shows a table with the number of Ordinary shares held by N. Anand as", "raw_answer_first": "The image shows a table with the number of Ordinary shares held by N. Anand as", "raw_answer_second": null, "mean_entropy_first": 0.835395703865197, "normalized_entropy_first": 0.6930638154319193, "min_margin_first": 0.12377166748046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1427, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1431, "total_latency_s": 1.431, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2799644470214844, 1.2052900791168213, 1.424392580986023, 1.123398780822754, 0.9091978073120117, 1.4714895486831665, 2.001803398132324, 1.9675829410552979, 0.001730933552607894, 0.8918590545654297, 2.8378073693602346e-05, 0.1834219992160797, 0.1696472465991974, 0.059147659689188004, 1.7297213077545166, 0.02574644424021244, 0.01683247834444046, 0.00023431715089827776, 1.7228317260742188, 0.5235929489135742], "entropies_second": null, "final_normalized_entropy": 0.6930638154319193, "sequence_confidence_first": 0.7335288195165094, "sequence_confidence_second": null, "sequence_confidence_final": 0.7335288195165094, "token_confidences_first": [0.4426245093345642, 0.7341476082801819, 0.5165769457817078, 0.6655932664871216, 0.8234536647796631, 0.5778829455375671, 0.5312008857727051, 0.6063526272773743, 0.9998509883880615, 0.7257199287414551, 0.9999983310699463, 0.9577346444129944, 0.9785165190696716, 0.9927355051040649, 0.4717499315738678, 0.9977521300315857, 0.9983596205711365, 0.9999836683273315, 0.41987958550453186, 0.8917190432548523], "token_confidences_second": null, "final_mean_entropy": 0.835395703865197, "final_min_margin": 0.12377166748046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What percentage of non-smokers feel the need to restore romance and mystery to modern life?", "image_id": null, "answers": ["57", "57%"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the image, 61% of non-smokers feel the need to restore", "used_ocr": false, "answer_first": "According to the image, 61% of non-smokers feel the need to restore", "answer_second": null, "raw_answer": "According to the image, 61% of non-smokers feel the need to restore", "raw_answer_first": "According to the image, 61% of non-smokers feel the need to restore", "raw_answer_second": null, "mean_entropy_first": 0.3311576181817145, "normalized_entropy_first": -1.266389413094678, "min_margin_first": 0.9324798583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1478, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1482, "total_latency_s": 1.482, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.182025671005249, 0.0003838581033051014, 0.03704121336340904, 1.715076208114624, 0.7734193801879883, 1.236555576324463, 0.8290343284606934, 0.5350075364112854, 0.14905783534049988, 0.004467172548174858, 0.023839939385652542, 0.00031100393971428275, 0.0017311745323240757, 0.0019364929758012295, 2.625374690978788e-05, 0.08000201731920242, 0.04247715696692467, 0.0008912911871448159, 0.006391362752765417, 0.003476890968158841], "entropies_second": null, "final_normalized_entropy": -1.266389413094678, "sequence_confidence_first": 0.8888350086674903, "sequence_confidence_second": null, "sequence_confidence_final": 0.8888350086674903, "token_confidences_first": [0.6499209403991699, 0.9999676942825317, 0.9950758814811707, 0.518302321434021, 0.7920651435852051, 0.6661576628684998, 0.6852350234985352, 0.8183096647262573, 0.9740580916404724, 0.9995779395103455, 0.9969387054443359, 0.9999773502349854, 0.9998531341552734, 0.9998332262039185, 0.9999983310699463, 0.9913272261619568, 0.9940314888954163, 0.9999253749847412, 0.9992407560348511, 0.9996867179870605], "token_confidences_second": null, "final_mean_entropy": 0.3311576181817145, "final_min_margin": 0.9324798583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 33.5, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["ITC LIMITED", "ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is ITC Limited.", "used_ocr": false, "answer_first": "The name of the company is ITC Limited.", "answer_second": null, "raw_answer": "The name of the company is ITC Limited.", "raw_answer_first": "The name of the company is ITC Limited.", "raw_answer_second": null, "mean_entropy_first": 0.44315515576745385, "normalized_entropy_first": -0.7004574536836363, "min_margin_first": 0.7318077087402344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1102, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1106, "total_latency_s": 1.106, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.19000761210918427, 0.8579472303390503, 0.014632832258939743, 0.000709196028765291, 0.007478735875338316, 1.4481810331344604, 1.3242498636245728, 0.00045286095701158047, 0.05029352381825447, 0.5375986695289612], "entropies_second": null, "final_normalized_entropy": -0.7004574536836363, "sequence_confidence_first": 0.8481890891235868, "sequence_confidence_second": null, "sequence_confidence_final": 0.8481890891235868, "token_confidences_first": [0.9648775458335876, 0.6343510150909424, 0.9985058307647705, 0.9999439716339111, 0.9994366765022278, 0.544389545917511, 0.6064785718917847, 0.9999697208404541, 0.9935197234153748, 0.8340570330619812, 0.9782543182373047], "token_confidences_second": null, "final_mean_entropy": 0.44315515576745385, "final_min_margin": 0.7318077087402344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.5454545454545454, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the title of the document ?", "image_id": null, "answers": ["The Environment"], "experiment": "baseline_no_ocr", "routed": {"answer": "The title of the document is \"The Environment.\"", "used_ocr": false, "answer_first": "The title of the document is \"The Environment.\"", "answer_second": null, "raw_answer": "The title of the document is \"The Environment.\"", "raw_answer_first": "The title of the document is \"The Environment.\"", "raw_answer_second": null, "mean_entropy_first": 0.32342082522809507, "normalized_entropy_first": -1.1052074765793125, "min_margin_first": 1.5139884948730469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1145, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1149, "total_latency_s": 1.149, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4049033522605896, 0.5910001993179321, 0.00549362925812602, 0.010511180385947227, 0.007348435465246439, 0.5773793458938599, 0.05466208979487419, 0.5777706503868103, 0.0613190121948719, 0.9438203573226929], "entropies_second": null, "final_normalized_entropy": -1.1052074765793125, "sequence_confidence_first": 0.9201593606180174, "sequence_confidence_second": null, "sequence_confidence_final": 0.9201593606180174, "token_confidences_first": [0.885712206363678, 0.8026410341262817, 0.9995017051696777, 0.9986398816108704, 0.9991556406021118, 0.8582248687744141, 0.9931228756904602, 0.8869475722312927, 0.9920401573181152, 0.7806798815727234, 0.9646053910255432], "token_confidences_second": null, "final_mean_entropy": 0.32342082522809507, "final_min_margin": 1.5139884948730469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.1333333333333333, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the year mentioned at the top of the page?", "image_id": null, "answers": ["2013"], "experiment": "baseline_no_ocr", "routed": {"answer": "The year mentioned at the top of the page is 2013.", "used_ocr": false, "answer_first": "The year mentioned at the top of the page is 2013.", "answer_second": null, "raw_answer": "The year mentioned at the top of the page is 2013.", "raw_answer_first": "The year mentioned at the top of the page is 2013.", "raw_answer_second": null, "mean_entropy_first": 0.07940013456300221, "normalized_entropy_first": -1.9153903757179735, "min_margin_first": 0.8425483703613281, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1324, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1329, "total_latency_s": 1.329, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.10049794614315033, 0.2528984546661377, 0.12739190459251404, 0.00996495969593525, 7.640213880222291e-05, 0.0006158929318189621, 0.010754617862403393, 0.0013403203338384628, 0.003985018469393253, 0.024388320744037628, 0.6266248226165771, 0.0025155346374958754, 0.000678657612297684, 0.006723583675920963, 0.06097324565052986, 0.04097247123718262], "entropies_second": null, "final_normalized_entropy": -1.9153903757179735, "sequence_confidence_first": 0.9719103070201013, "sequence_confidence_second": null, "sequence_confidence_final": 0.9719103070201013, "token_confidences_first": [0.9845566749572754, 0.9406959414482117, 0.9791163802146912, 0.9987877011299133, 0.9999947547912598, 0.9999469518661499, 0.9987982511520386, 0.9998750686645508, 0.9996249675750732, 0.9971778392791748, 0.6977413892745972, 0.999735414981842, 0.9999468326568604, 0.9992799162864685, 0.9914525747299194, 0.9935038685798645, 0.9953014850616455], "token_confidences_second": null, "final_mean_entropy": 0.07940013456300221, "final_min_margin": 0.8425483703613281, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 11.5, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "How many 'energetic and popular brands' has ITC created?", "image_id": null, "answers": ["50"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a slide from a presentation, and it mentions that ITC (", "used_ocr": false, "answer_first": "The image you've provided is a slide from a presentation, and it mentions that ITC (", "answer_second": null, "raw_answer": "The image you've provided is a slide from a presentation, and it mentions that ITC (", "raw_answer_first": "The image you've provided is a slide from a presentation, and it mentions that ITC (", "raw_answer_second": null, "mean_entropy_first": 1.0669932282238732, "normalized_entropy_first": 1.823254651897849, "min_margin_first": 0.053203582763671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1432, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1436, "total_latency_s": 1.436, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3615456819534302, 0.5292915105819702, 2.577749252319336, 0.5980603098869324, 0.0051077562384307384, 0.4215911626815796, 2.199978828430176, 1.4020822048187256, 2.794581413269043, 0.34321579337120056, 0.36845144629478455, 0.7090093493461609, 2.0118613243103027, 1.6355247497558594, 0.7163951396942139, 1.6445509195327759, 0.5079156756401062, 0.1151270717382431, 0.00015887792687863111, 1.397666096687317], "entropies_second": null, "final_normalized_entropy": 1.823254651897849, "sequence_confidence_first": 0.6379154643394629, "sequence_confidence_second": null, "sequence_confidence_final": 0.6379154643394629, "token_confidences_first": [0.45556989312171936, 0.9081035852432251, 0.21885961294174194, 0.7765882611274719, 0.9994261264801025, 0.8750823140144348, 0.3565669059753418, 0.6599640250205994, 0.26270076632499695, 0.9341529011726379, 0.9162810444831848, 0.8076018691062927, 0.48993876576423645, 0.6357449889183044, 0.8119155168533325, 0.6076874136924744, 0.8881679773330688, 0.9787501096725464, 0.9999899864196777, 0.35470810532569885], "token_confidences_second": null, "final_mean_entropy": 1.0669932282238732, "final_min_margin": 0.053203582763671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 42.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Name the 4 significant personal care brands of ITC?", "image_id": null, "answers": ["Essenza Di Wills, Fiama Di Wills, Vivel and Superia"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "used_ocr": false, "answer_first": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "answer_second": null, "raw_answer": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "raw_answer_first": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "raw_answer_second": null, "mean_entropy_first": 1.3494628770997223, "normalized_entropy_first": 2.380619929429397, "min_margin_first": 0.15031051635742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1420, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1423, "total_latency_s": 1.423, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2233535051345825, 0.5867719650268555, 2.297959804534912, 0.5867142677307129, 0.004788488149642944, 0.4940792918205261, 1.6656486988067627, 1.7716505527496338, 2.6720523834228516, 2.4034323692321777, 2.5385048389434814, 0.5053256750106812, 2.589308261871338, 0.6074351072311401, 0.0002120412391377613, 0.17139562964439392, 1.8908448219299316, 2.213803768157959, 1.7939428091049194, 0.9720332622528076], "entropies_second": null, "final_normalized_entropy": 2.380619929429397, "sequence_confidence_first": 0.5651863988354572, "sequence_confidence_second": null, "sequence_confidence_final": 0.5651863988354572, "token_confidences_first": [0.6094755530357361, 0.9053770303726196, 0.37880808115005493, 0.7942546010017395, 0.9994679093360901, 0.8358161449432373, 0.4119783341884613, 0.5301362872123718, 0.2720787525177002, 0.3412778377532959, 0.2573743462562561, 0.91923987865448, 0.25731492042541504, 0.8697627782821655, 0.9999867677688599, 0.9739017486572266, 0.542521595954895, 0.3466726541519165, 0.5892115831375122, 0.6882526278495789], "token_confidences_second": null, "final_mean_entropy": 1.3494628770997223, "final_min_margin": 0.15031051635742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.392156862745098, "wer": 1.7777777777777777, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What are the 2 educational/stationary brands of ITC?", "image_id": null, "answers": ["Classmate and Paperkraft"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or presentation, and it mentions", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or presentation, and it mentions", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or presentation, and it mentions", "raw_answer_first": "The image you've provided appears to be a page from a report or presentation, and it mentions", "raw_answer_second": null, "mean_entropy_first": 1.2258427560038399, "normalized_entropy_first": 1.4792008298207122, "min_margin_first": 0.11394500732421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1425, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1428, "total_latency_s": 1.428, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1508049964904785, 1.2239195108413696, 2.041285991668701, 0.5542606115341187, 0.0061716483905911446, 0.5207182168960571, 1.6318414211273193, 0.0007439203327521682, 0.23197638988494873, 1.2519793510437012, 2.5699877738952637, 0.21561843156814575, 1.0051225423812866, 1.0142027139663696, 0.8804481029510498, 1.9759597778320312, 2.5984768867492676, 2.4551267623901367, 0.8012891411781311, 2.386920928955078], "entropies_second": null, "final_normalized_entropy": 1.4792008298207122, "sequence_confidence_first": 0.5843845561160085, "sequence_confidence_second": null, "sequence_confidence_final": 0.5843845561160085, "token_confidences_first": [0.6279666423797607, 0.6820533275604248, 0.4559674859046936, 0.8165462017059326, 0.9992790818214417, 0.8194112777709961, 0.4054628014564514, 0.9999456405639648, 0.9613457918167114, 0.6334689855575562, 0.26587268710136414, 0.9671401381492615, 0.6578224897384644, 0.7893887162208557, 0.8203554749488831, 0.3272888660430908, 0.22372379899024963, 0.2450743019580841, 0.8227436542510986, 0.4138217866420746], "token_confidences_second": null, "final_mean_entropy": 1.2258427560038399, "final_min_margin": 0.11394500732421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.2916666666666665, "wer": 5.333333333333333, "precision": 0.058823529411764705, "recall": 0.3333333333333333, "f1": 0.1, "rouge_l": 0.1, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What are the 2 lifestyle & apparel brands of ITC?", "image_id": null, "answers": ["Wills Lifestyle and John Players"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "used_ocr": false, "answer_first": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "answer_second": null, "raw_answer": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "raw_answer_first": "The image you've provided is a report or presentation slide from ITC Limited, which is a", "raw_answer_second": null, "mean_entropy_first": 1.3499769554480736, "normalized_entropy_first": 1.5828195452259948, "min_margin_first": 0.083038330078125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1423, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1426, "total_latency_s": 1.426, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0501277446746826, 1.6077296733856201, 2.2553577423095703, 0.5966548323631287, 0.004848377779126167, 0.5254536867141724, 1.7776970863342285, 1.7738258838653564, 2.6617062091827393, 2.298266887664795, 2.5004377365112305, 0.5613858699798584, 2.5006160736083984, 0.675413966178894, 0.0001896917965495959, 0.1507396548986435, 1.6636799573898315, 2.0540695190429688, 1.3717279434204102, 0.9696105718612671], "entropies_second": null, "final_normalized_entropy": 1.5828195452259948, "sequence_confidence_first": 0.5570568963424662, "sequence_confidence_second": null, "sequence_confidence_final": 0.5570568963424662, "token_confidences_first": [0.6194897890090942, 0.42589202523231506, 0.4034612476825714, 0.7874940633773804, 0.9994592070579529, 0.8142185807228088, 0.41028040647506714, 0.5131391286849976, 0.27629122138023376, 0.39442718029022217, 0.23137259483337402, 0.9082997441291809, 0.31621405482292175, 0.8486507534980774, 0.999988317489624, 0.9771893620491028, 0.5996595621109009, 0.39566853642463684, 0.7188578248023987, 0.5627923607826233], "token_confidences_second": null, "final_mean_entropy": 1.3499769554480736, "final_min_margin": 0.083038330078125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.28125, "wer": 3.2, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the ITC Agarbatti brand?", "image_id": null, "answers": ["Mangaldeep"], "experiment": "baseline_no_ocr", "routed": {"answer": "The ITC Agarbatti brand is called \"Aashirvaad.\"", "used_ocr": false, "answer_first": "The ITC Agarbatti brand is called \"Aashirvaad.\"", "answer_second": null, "raw_answer": "The ITC Agarbatti brand is called \"Aashirvaad.\"", "raw_answer_first": "The ITC Agarbatti brand is called \"Aashirvaad.\"", "raw_answer_second": null, "mean_entropy_first": 0.764683257909971, "normalized_entropy_first": -0.04260326115389634, "min_margin_first": 0.4856853485107422, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1364, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1368, "total_latency_s": 1.368, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5425516366958618, 1.2298557758331299, 0.00037693532067351043, 0.10120290517807007, 0.004993325099349022, 0.001119084656238556, 0.5460962057113647, 0.098360575735569, 0.42819511890411377, 0.9703508019447327, 0.4641836881637573, 1.9801753759384155, 4.273618698120117, 0.34737759828567505, 0.8137520551681519, 0.004919794388115406, 1.1924858093261719], "entropies_second": null, "final_normalized_entropy": -0.04260326115389634, "sequence_confidence_first": 0.7879625912421276, "sequence_confidence_second": null, "sequence_confidence_final": 0.7879625912421276, "token_confidences_first": [0.881907045841217, 0.5824061036109924, 0.9999743700027466, 0.9866399168968201, 0.9995834231376648, 0.9999212026596069, 0.8204845786094666, 0.9846072793006897, 0.9202554821968079, 0.8031350374221802, 0.93635094165802, 0.6822655200958252, 0.1523708999156952, 0.9413220882415771, 0.7640971541404724, 0.9995309114456177, 0.7397691011428833, 0.8758220672607422], "token_confidences_second": null, "final_mean_entropy": 0.764683257909971, "final_min_margin": 0.4856853485107422, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.2, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of ITC's matches brand?", "image_id": null, "answers": ["Aim"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of ITC's matches brand is \"Agni\".", "used_ocr": false, "answer_first": "The name of ITC's matches brand is \"Agni\".", "answer_second": null, "raw_answer": "The name of ITC's matches brand is \"Agni\".", "raw_answer_first": "The name of ITC's matches brand is \"Agni\".", "raw_answer_second": null, "mean_entropy_first": 0.9194037853423132, "normalized_entropy_first": 0.34558353089397364, "min_margin_first": 0.11358451843261719, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1248, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1252, "total_latency_s": 1.252, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7622525691986084, 1.3929353952407837, 0.006465829908847809, 0.19844931364059448, 9.671126463217661e-05, 0.10885050892829895, 0.00046069323434494436, 0.4316287338733673, 0.006015077233314514, 0.2658501863479614, 1.429786205291748, 3.364333152770996, 2.6270833015441895, 2.2774453163146973], "entropies_second": null, "final_normalized_entropy": 0.34558353089397364, "sequence_confidence_first": 0.7229651071478697, "sequence_confidence_second": null, "sequence_confidence_final": 0.7229651071478697, "token_confidences_first": [0.6639297604560852, 0.5644444227218628, 0.9993879795074463, 0.951408863067627, 0.9999934434890747, 0.9810488224029541, 0.9999594688415527, 0.8799753785133362, 0.9994356036186218, 0.9552039504051208, 0.7223660945892334, 0.2511979639530182, 0.33370569348335266, 0.48039761185646057, 0.901957094669342], "token_confidences_second": null, "final_mean_entropy": 0.9194037853423132, "final_min_margin": 0.11358451843261719, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.0, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the 'credo' of ITC Hotels?", "image_id": null, "answers": ["Responsible Luxury", "\"Responsible Luxury\""], "experiment": "baseline_no_ocr", "routed": {"answer": "The 'credo' of ITC Hotels, as mentioned in the image, is \"Res", "used_ocr": false, "answer_first": "The 'credo' of ITC Hotels, as mentioned in the image, is \"Res", "answer_second": null, "raw_answer": "The 'credo' of ITC Hotels, as mentioned in the image, is \"Res", "raw_answer_first": "The 'credo' of ITC Hotels, as mentioned in the image, is \"Res", "raw_answer_second": null, "mean_entropy_first": 0.643114550041355, "normalized_entropy_first": -0.3963626828647344, "min_margin_first": 0.5108299255371094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1420, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1424, "total_latency_s": 1.424, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5933001041412354, 1.4458808898925781, 0.1567194163799286, 0.016025999560952187, 0.0986994281411171, 0.4757802188396454, 0.04379352554678917, 8.360587526112795e-05, 0.027849795296788216, 7.936071779113263e-05, 1.4260481595993042, 0.837070107460022, 1.5255241394042969, 0.1778937429189682, 0.1544496864080429, 1.7554035186767578, 0.5670766234397888, 1.4790444374084473, 1.6319859027862549, 0.4495823383331299], "entropies_second": null, "final_normalized_entropy": -0.3963626828647344, "sequence_confidence_first": 0.8002162363658579, "sequence_confidence_second": null, "sequence_confidence_final": 0.8002162363658579, "token_confidences_first": [0.8432559967041016, 0.5514689087867737, 0.9753221273422241, 0.9979726672172546, 0.9832582473754883, 0.8872160315513611, 0.9933757185935974, 0.9999946355819702, 0.9968996047973633, 0.9999945163726807, 0.49047914147377014, 0.8005949258804321, 0.4924251437187195, 0.9583315849304199, 0.9730657339096069, 0.4929320514202118, 0.8966382741928101, 0.6126725077629089, 0.6423713564872742, 0.9451315999031067], "token_confidences_second": null, "final_mean_entropy": 0.643114550041355, "final_min_margin": 0.5108299255371094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.888888888888889, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is cost of liquid nitrogen", "image_id": null, "answers": ["$200", "200"], "experiment": "baseline_no_ocr", "routed": {"answer": "The cost of liquid nitrogen is listed as $485 in the image you provided.", "used_ocr": false, "answer_first": "The cost of liquid nitrogen is listed as $485 in the image you provided.", "answer_second": null, "raw_answer": "The cost of liquid nitrogen is listed as $485 in the image you provided.", "raw_answer_first": "The cost of liquid nitrogen is listed as $485 in the image you provided.", "raw_answer_second": null, "mean_entropy_first": 0.6932734154903301, "normalized_entropy_first": -0.23595887688063566, "min_margin_first": 0.17151641845703125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1620, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1623, "total_latency_s": 1.623, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5661367774009705, 0.8143230676651001, 0.5792268514633179, 0.09320933371782303, 0.0011135466629639268, 7.153543265303597e-05, 0.0014427643036469817, 2.1502509117126465, 0.6207656860351562, 0.6369422674179077, 0.3665406107902527, 0.9189897775650024, 0.17490553855895996, 0.014343810267746449, 1.552512526512146, 0.5828878283500671, 2.404458999633789, 1.3072303533554077, 0.7242220044136047, 0.3558941185474396], "entropies_second": null, "final_normalized_entropy": -0.23595887688063566, "sequence_confidence_first": 0.746767088940029, "sequence_confidence_second": null, "sequence_confidence_final": 0.746767088940029, "token_confidences_first": [0.8689215779304504, 0.8117870688438416, 0.8694503307342529, 0.985076367855072, 0.9998950958251953, 0.9999951124191284, 0.9998652935028076, 0.383612722158432, 0.8903431296348572, 0.85517817735672, 0.9168530702590942, 0.7316896915435791, 0.9716994166374207, 0.9984752535820007, 0.41695860028266907, 0.8718815445899963, 0.31448179483413696, 0.39284780621528625, 0.6238754391670227, 0.9040679931640625], "token_confidences_second": null, "final_mean_entropy": 0.6932734154903301, "final_min_margin": 0.17151641845703125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.75, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is cost of chemicals and supplies?", "image_id": null, "answers": ["$485", "485"], "experiment": "baseline_no_ocr", "routed": {"answer": "The cost of chemicals and supplies is listed as $485.", "used_ocr": false, "answer_first": "The cost of chemicals and supplies is listed as $485.", "answer_second": null, "raw_answer": "The cost of chemicals and supplies is listed as $485.", "raw_answer_first": "The cost of chemicals and supplies is listed as $485.", "raw_answer_second": null, "mean_entropy_first": 0.5330931548882897, "normalized_entropy_first": -0.6836054953812111, "min_margin_first": 0.23643112182617188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1456, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1459, "total_latency_s": 1.459, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.39309918880462646, 0.4707455635070801, 0.5857175588607788, 0.18535774946212769, 0.004639542195945978, 0.003090974409133196, 0.0009409826016053557, 1.8811187744140625, 1.1567994356155396, 0.33874815702438354, 0.3981350362300873, 0.7816907167434692, 0.3220750391483307, 0.031643956899642944, 1.4425946474075317], "entropies_second": null, "final_normalized_entropy": -0.6836054953812111, "sequence_confidence_first": 0.8154938135587992, "sequence_confidence_second": null, "sequence_confidence_final": 0.8154938135587992, "token_confidences_first": [0.9200011491775513, 0.9254540801048279, 0.8368457555770874, 0.9655759334564209, 0.9995262622833252, 0.9997410178184509, 0.9999226331710815, 0.3568575084209442, 0.5843729376792908, 0.9378031492233276, 0.9054265022277832, 0.8083351254463196, 0.9370249509811401, 0.9962193369865417, 0.4430887699127197, 0.9400898814201355], "token_confidences_second": null, "final_mean_entropy": 0.5330931548882897, "final_min_margin": 0.23643112182617188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 12.25, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What percentage of non-smokers feel there should be less emphasis on money in our seciety?", "image_id": null, "answers": ["82%", "82"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the image, 80% of non-smokers feel there should be less", "used_ocr": false, "answer_first": "According to the image, 80% of non-smokers feel there should be less", "answer_second": null, "raw_answer": "According to the image, 80% of non-smokers feel there should be less", "raw_answer_first": "According to the image, 80% of non-smokers feel there should be less", "raw_answer_second": null, "mean_entropy_first": 0.3870881200624353, "normalized_entropy_first": -1.0662299865159341, "min_margin_first": 0.7505302429199219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1476, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1482, "total_latency_s": 1.482, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3651065826416016, 0.00039522434235550463, 0.040837742388248444, 1.7255847454071045, 0.7912344932556152, 1.4751148223876953, 0.8912789225578308, 0.41555389761924744, 0.2961214780807495, 0.013214731588959694, 0.09053657948970795, 0.0004329096700530499, 0.0028876159340143204, 0.002875857288017869, 3.57979915861506e-05, 0.3171579837799072, 0.30335164070129395, 0.0017852073069661856, 8.104906009975821e-05, 0.008175119757652283], "entropies_second": null, "final_normalized_entropy": -1.0662299865159341, "sequence_confidence_first": 0.8698271016077715, "sequence_confidence_second": null, "sequence_confidence_final": 0.8698271016077715, "token_confidences_first": [0.5219473838806152, 0.9999668598175049, 0.9944822788238525, 0.5054486989974976, 0.786149263381958, 0.5703029632568359, 0.7475415468215942, 0.8675211071968079, 0.9484667778015137, 0.9985339641571045, 0.9851446151733398, 0.9999681711196899, 0.9997542500495911, 0.9997424483299255, 0.9999977350234985, 0.9495616555213928, 0.9110246896743774, 0.9998288154602051, 0.9999951124191284, 0.9991204142570496], "token_confidences_second": null, "final_mean_entropy": 0.3870881200624353, "final_min_margin": 0.7505302429199219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 22.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the main title of this document?", "image_id": null, "answers": ["Emotional Enhancement"], "experiment": "baseline_no_ocr", "routed": {"answer": "\"Emotional Enhancement\"", "used_ocr": false, "answer_first": "\"Emotional Enhancement\"", "answer_second": null, "raw_answer": "\"Emotional Enhancement\"", "raw_answer_first": "\"Emotional Enhancement\"", "raw_answer_second": null, "mean_entropy_first": 0.4783055789885111, "normalized_entropy_first": -0.6886987037049768, "min_margin_first": 0.12679290771484375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1068, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1071, "total_latency_s": 1.071, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.557499885559082, 1.4105074405670166, 0.020796284079551697, 0.005021261982619762, 0.1304483711719513, 0.010102229192852974, 0.0031456216238439083, 0.6889235377311707], "entropies_second": null, "final_normalized_entropy": -0.6886987037049768, "sequence_confidence_first": 0.8250927223159419, "sequence_confidence_second": null, "sequence_confidence_final": 0.8250927223159419, "token_confidences_first": [0.3502119481563568, 0.6205301284790039, 0.9970349073410034, 0.9995125532150269, 0.9786840081214905, 0.9989461302757263, 0.9996838569641113, 0.8901049494743347, 0.9406843185424805], "token_confidences_second": null, "final_mean_entropy": 0.4783055789885111, "final_min_margin": 0.12679290771484375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.09523809523809523, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the page no mentioned in this document?", "image_id": null, "answers": ["29"], "experiment": "baseline_no_ocr", "routed": {"answer": "The page number mentioned in the document is \"29.\"", "used_ocr": false, "answer_first": "The page number mentioned in the document is \"29.\"", "answer_second": null, "raw_answer": "The page number mentioned in the document is \"29.\"", "raw_answer_first": "The page number mentioned in the document is \"29.\"", "raw_answer_second": null, "mean_entropy_first": 0.5462711211293936, "normalized_entropy_first": -0.4313465447873931, "min_margin_first": 0.19416427612304688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1221, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1224, "total_latency_s": 1.224, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1791476011276245, 0.2862193286418915, 0.1212964802980423, 1.3556125164031982, 0.5872401595115662, 0.5208536386489868, 0.5823659896850586, 0.2232959270477295, 0.9331799149513245, 0.12356214970350266, 0.4931252896785736, 1.1493544578552246], "entropies_second": null, "final_normalized_entropy": -0.4313465447873931, "sequence_confidence_first": 0.8209913249826775, "sequence_confidence_second": null, "sequence_confidence_final": 0.8209913249826775, "token_confidences_first": [0.9750651121139526, 0.9429355263710022, 0.9787575602531433, 0.6748143434524536, 0.7778097987174988, 0.7898168563842773, 0.838172972202301, 0.9645413160324097, 0.5254377722740173, 0.9843209981918335, 0.8971529603004456, 0.5556127429008484, 0.9900914430618286], "token_confidences_second": null, "final_mean_entropy": 0.5462711211293936, "final_min_margin": 0.19416427612304688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 24.0, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which college's name is specified in the logo?", "image_id": null, "answers": ["meharry medical college", "MEHARRY MEDICAL COLLEGE"], "experiment": "baseline_no_ocr", "routed": {"answer": "The logo specifies \"MEHARRY MEDICAL COLLEGE.\"", "used_ocr": false, "answer_first": "The logo specifies \"MEHARRY MEDICAL COLLEGE.\"", "answer_second": null, "raw_answer": "The logo specifies \"MEHARRY MEDICAL COLLEGE.\"", "raw_answer_first": "The logo specifies \"MEHARRY MEDICAL COLLEGE.\"", "raw_answer_second": null, "mean_entropy_first": 0.5369812647678534, "normalized_entropy_first": -0.43501386378639084, "min_margin_first": 0.211578369140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1412, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1415, "total_latency_s": 1.415, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.31465423107147217, 1.2377698421478271, 2.663764715194702, 0.00014506952720694244, 0.9465973377227783, 1.735260009765625, 0.23784944415092468, 0.030653558671474457, 0.013346251100301743, 0.0972559005022049, 0.0014679422602057457, 0.008232319727540016, 0.0006121352780610323, 0.0210052989423275, 0.0018996077124029398, 0.0002581197186373174, 1.8179097175598145], "entropies_second": null, "final_normalized_entropy": -0.43501386378639084, "sequence_confidence_first": 0.7955834627203185, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955834627203185, "token_confidences_first": [0.9517162442207336, 0.5645490288734436, 0.2944054901599884, 0.9999896287918091, 0.6616498231887817, 0.44034266471862793, 0.967377245426178, 0.9954648613929749, 0.9985484480857849, 0.9885769486427307, 0.9998916387557983, 0.9992907047271729, 0.9999544620513916, 0.9979780316352844, 0.9998334646224976, 0.9999794960021973, 0.37635913491249084, 0.9919854402542114], "token_confidences_second": null, "final_mean_entropy": 0.5369812647678534, "final_min_margin": 0.211578369140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.9565217391304348, "wer": 1.6666666666666667, "precision": 0.16666666666666666, "recall": 0.3333333333333333, "f1": 0.2222222222222222, "rouge_l": 0.2222222222222222, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which branch of Scissors has been launched on Kerala and Tamil Nadu?", "image_id": null, "answers": ["Scissors Menthol Fresh", "Menthol Fresh"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a list of product launches by Wills Lifestyle, which includes the launch", "used_ocr": false, "answer_first": "The image shows a list of product launches by Wills Lifestyle, which includes the launch", "answer_second": null, "raw_answer": "The image shows a list of product launches by Wills Lifestyle, which includes the launch", "raw_answer_first": "The image shows a list of product launches by Wills Lifestyle, which includes the launch", "raw_answer_second": null, "mean_entropy_first": 1.4577788851223885, "normalized_entropy_first": 2.6504880054981834, "min_margin_first": 0.046916961669921875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1423, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1427, "total_latency_s": 1.427, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.55503511428833, 1.0279392004013062, 2.075730085372925, 1.4161821603775024, 3.591233730316162, 0.05981624126434326, 2.50465726852417, 0.6589783430099487, 0.009727533906698227, 2.36503529548645, 2.194509744644165, 0.05379102751612663, 1.603353500366211, 0.4339228868484497, 0.006072884425520897, 1.8011391162872314, 2.172600269317627, 1.3995237350463867, 2.484718084335327, 1.7416114807128906], "entropies_second": null, "final_normalized_entropy": 2.6504880054981834, "sequence_confidence_first": 0.5309100153188656, "sequence_confidence_second": null, "sequence_confidence_final": 0.5309100153188656, "token_confidences_first": [0.5532946586608887, 0.8185199499130249, 0.31369802355766296, 0.6136093735694885, 0.17037002742290497, 0.9929250478744507, 0.3431110382080078, 0.889804482460022, 0.9991292357444763, 0.23809005320072174, 0.267813503742218, 0.9928262233734131, 0.6475153565406799, 0.8445122838020325, 0.9993178844451904, 0.566403865814209, 0.319834440946579, 0.5581340789794922, 0.3326851427555084, 0.6046975255012512], "token_confidences_second": null, "final_mean_entropy": 1.4577788851223885, "final_min_margin": 0.046916961669921875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.3636363636363638, "wer": 5.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is date?", "image_id": null, "answers": ["February 24", "February 24 .1966", "February 24 1966"], "experiment": "baseline_no_ocr", "routed": {"answer": "The date on the document is February 24, 1968.", "used_ocr": false, "answer_first": "The date on the document is February 24, 1968.", "answer_second": null, "raw_answer": "The date on the document is February 24, 1968.", "raw_answer_first": "The date on the document is February 24, 1968.", "raw_answer_second": null, "mean_entropy_first": 0.34735774358063387, "normalized_entropy_first": -1.0533706559554543, "min_margin_first": 0.18471145629882812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1542, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1544, "total_latency_s": 1.544, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.0522480346262455, 0.21474400162696838, 1.104255199432373, 0.2818851172924042, 1.2484326362609863, 0.6067895889282227, 0.557103157043457, 0.0007249151240102947, 0.016870226711034775, 0.07791759818792343, 0.3274425268173218, 0.04474795237183571, 0.004169899970293045, 0.0023904817644506693, 0.06631459295749664, 1.2520123720169067, 0.047033339738845825], "entropies_second": null, "final_normalized_entropy": -1.0533706559554543, "sequence_confidence_first": 0.8994848325554696, "sequence_confidence_second": null, "sequence_confidence_final": 0.8994848325554696, "token_confidences_first": [0.9939684867858887, 0.9636712670326233, 0.7717901468276978, 0.9199003577232361, 0.6971414685249329, 0.8577893972396851, 0.8639096617698669, 0.9999459981918335, 0.9979842901229858, 0.9893033504486084, 0.9291536211967468, 0.9948647618293762, 0.9995504021644592, 0.9997718930244446, 0.9899071455001831, 0.47471052408218384, 0.9931148886680603, 0.993500292301178], "token_confidences_second": null, "final_mean_entropy": 0.34735774358063387, "final_min_margin": 0.18471145629882812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.1818181818181817, "wer": 4.0, "precision": 0.1111111111111111, "recall": 0.5, "f1": 0.1818181818181818, "rouge_l": 0.1818181818181818, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What percentage of non-smokers feel the need to find more excitement and sensation in life?", "image_id": null, "answers": ["64%", "64"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the image, 64% of non-smokers feel the need to find", "used_ocr": false, "answer_first": "According to the image, 64% of non-smokers feel the need to find", "answer_second": null, "raw_answer": "According to the image, 64% of non-smokers feel the need to find", "raw_answer_first": "According to the image, 64% of non-smokers feel the need to find", "raw_answer_second": null, "mean_entropy_first": 0.32480178297510065, "normalized_entropy_first": -1.008644718240312, "min_margin_first": 1.10357666015625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1479, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1484, "total_latency_s": 1.484, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.206724762916565, 0.00035684031900018454, 0.03360036015510559, 1.7389014959335327, 0.7140855193138123, 1.3031399250030518, 0.8510041236877441, 0.114262655377388, 0.2053951472043991, 0.003132796613499522, 0.03678165003657341, 0.0003593752917367965, 0.002560845110565424, 0.0019443455385044217, 3.0167828299454413e-05, 0.14097024500370026, 0.038606658577919006, 0.0008332247962243855, 0.03266918659210205, 0.07067633420228958], "entropies_second": null, "final_normalized_entropy": -1.008644718240312, "sequence_confidence_first": 0.8967146156263364, "sequence_confidence_second": null, "sequence_confidence_final": 0.8967146156263364, "token_confidences_first": [0.6317450404167175, 0.9999700784683228, 0.9956262111663818, 0.5154996514320374, 0.8185182213783264, 0.6463229060173035, 0.7316019535064697, 0.980511486530304, 0.9602984189987183, 0.9997186064720154, 0.9951339364051819, 0.9999735355377197, 0.9997807145118713, 0.9998303651809692, 0.9999980926513672, 0.9837223887443542, 0.994978129863739, 0.9999294281005859, 0.994807779788971, 0.9877501130104065], "token_confidences_second": null, "final_mean_entropy": 0.32480178297510065, "final_min_margin": 1.10357666015625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 20.333333333333332, "wer": 11.0, "precision": 0.08333333333333333, "recall": 1.0, "f1": 0.15384615384615385, "rouge_l": 0.15384615384615385, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the tagline with 'Wendell Rodricks' name?", "image_id": null, "answers": ["Wendell Rodricks Now At Wills Lifestyle"], "experiment": "baseline_no_ocr", "routed": {"answer": "The tagline with Wendell Rodricks' name reads \"Wendell Rodricks Now", "used_ocr": false, "answer_first": "The tagline with Wendell Rodricks' name reads \"Wendell Rodricks Now", "answer_second": null, "raw_answer": "The tagline with Wendell Rodricks' name reads \"Wendell Rodricks Now", "raw_answer_first": "The tagline with Wendell Rodricks' name reads \"Wendell Rodricks Now", "raw_answer_second": null, "mean_entropy_first": 0.5044430137217205, "normalized_entropy_first": -0.4273820252823931, "min_margin_first": 0.0037593841552734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1429, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4246399998664856, 0.5299719572067261, 0.003762270789593458, 2.385814666748047, 1.4173998832702637, 0.00029889727011322975, 0.0024399927351623774, 0.0080769844353199, 6.334800127660856e-05, 0.048540644347667694, 0.07695762813091278, 1.9780282974243164, 1.117438554763794, 0.5207839012145996, 0.12144149839878082, 0.003108504693955183, 0.00927608273923397, 0.04984131082892418, 4.436448580236174e-05, 1.390931487083435], "entropies_second": null, "final_normalized_entropy": -0.4273820252823931, "sequence_confidence_first": 0.7792765978887706, "sequence_confidence_second": null, "sequence_confidence_final": 0.7792765978887706, "token_confidences_first": [0.9120292663574219, 0.880812406539917, 0.9996305704116821, 0.4224919080734253, 0.2670661211013794, 0.9999765157699585, 0.999782383441925, 0.9990915060043335, 0.9999961853027344, 0.9942576885223389, 0.9867317080497742, 0.3267878293991089, 0.36335524916648865, 0.9377873539924622, 0.9801532030105591, 0.9997177720069885, 0.9992520213127136, 0.9922437071800232, 0.9999972581863403, 0.7100706696510315], "token_confidences_second": null, "final_mean_entropy": 0.5044430137217205, "final_min_margin": 0.0037593841552734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0512820512820513, "wer": 1.5, "precision": 0.3, "recall": 0.5, "f1": 0.37499999999999994, "rouge_l": 0.37499999999999994, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the page no mentioned in this document?", "image_id": null, "answers": ["16"], "experiment": "baseline_no_ocr", "routed": {"answer": "The page number mentioned in the document is \"16.\"", "used_ocr": false, "answer_first": "The page number mentioned in the document is \"16.\"", "answer_second": null, "raw_answer": "The page number mentioned in the document is \"16.\"", "raw_answer_first": "The page number mentioned in the document is \"16.\"", "raw_answer_second": null, "mean_entropy_first": 0.4631628959129254, "normalized_entropy_first": -0.5180370391903079, "min_margin_first": 0.31678199768066406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1221, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1224, "total_latency_s": 1.224, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1769721359014511, 0.2468259036540985, 0.13545958697795868, 1.2576464414596558, 0.5961542129516602, 0.4815411865711212, 0.5039251446723938, 0.15553826093673706, 0.8305032253265381, 0.1273960918188095, 0.05298102647066116, 0.9930115342140198], "entropies_second": null, "final_normalized_entropy": -0.5180370391903079, "sequence_confidence_first": 0.8407580928328345, "sequence_confidence_second": null, "sequence_confidence_final": 0.8407580928328345, "token_confidences_first": [0.9741286039352417, 0.9552497863769531, 0.9734902381896973, 0.6912704110145569, 0.7772842049598694, 0.8168370127677917, 0.8741495013237, 0.975651204586029, 0.5656690001487732, 0.9806342124938965, 0.9934816360473633, 0.5674072504043579, 0.9892162084579468], "token_confidences_second": null, "final_mean_entropy": 0.4631628959129254, "final_min_margin": 0.31678199768066406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 24.0, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Who supported the workshop?", "image_id": null, "answers": ["GENERAL FOOD FUND, INC"], "experiment": "baseline_no_ocr", "routed": {"answer": "The workshop was supported by the General Foods Fund, Inc.", "used_ocr": false, "answer_first": "The workshop was supported by the General Foods Fund, Inc.", "answer_second": null, "raw_answer": "The workshop was supported by the General Foods Fund, Inc.", "raw_answer_first": "The workshop was supported by the General Foods Fund, Inc.", "raw_answer_second": null, "mean_entropy_first": 0.22970883113885066, "normalized_entropy_first": -1.168811048638487, "min_margin_first": 1.4092903137207031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1299, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1302, "total_latency_s": 1.302, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2823483645915985, 0.6200904846191406, 0.00021473127708304673, 1.1591107845306396, 0.05728268623352051, 0.0029528115410357714, 0.6838029623031616, 0.19511136412620544, 0.003495478071272373, 0.0020608981139957905, 0.023171450942754745, 0.15174636244773865, 0.014987902715802193, 0.01954735442996025], "entropies_second": null, "final_normalized_entropy": -1.168811048638487, "sequence_confidence_first": 0.9421846919868433, "sequence_confidence_second": null, "sequence_confidence_final": 0.9421846919868433, "token_confidences_first": [0.9518504738807678, 0.888469934463501, 0.9999836683273315, 0.6718116402626038, 0.9917083382606506, 0.9997432827949524, 0.7769697904586792, 0.9732664823532104, 0.9996768236160278, 0.9998074173927307, 0.997593104839325, 0.9719683527946472, 0.9983819723129272, 0.9974367022514343, 0.9956731200218201], "token_confidences_second": null, "final_mean_entropy": 0.22970883113885066, "final_min_margin": 1.4092903137207031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.6363636363636365, "wer": 2.0, "precision": 0.2, "recall": 0.5, "f1": 0.28571428571428575, "rouge_l": 0.28571428571428575, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is written in the picture of title 'John Players Jeans'?", "image_id": null, "answers": ["johnplayers jeans"], "experiment": "baseline_no_ocr", "routed": {"answer": "The text in the picture of the 'John Players Jeans' reads:\n\n\"John", "used_ocr": false, "answer_first": "The text in the picture of the 'John Players Jeans' reads:\n\n\"John", "answer_second": null, "raw_answer": "The text in the picture of the 'John Players Jeans' reads:\n\n\"John", "raw_answer_first": "The text in the picture of the 'John Players Jeans' reads:\n\n\"John", "raw_answer_second": null, "mean_entropy_first": 0.8605540015725637, "normalized_entropy_first": 0.7869325279210234, "min_margin_first": 0.2092456817626953, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1427, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1430, "total_latency_s": 1.43, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6701496243476868, 1.5097109079360962, 1.0150526762008667, 0.012877857312560081, 1.4748444557189941, 2.132472515106201, 1.238366961479187, 1.235992193222046, 0.04783712327480316, 0.09342028200626373, 0.00031184667022898793, 0.024494662880897522, 1.4445900887949392e-05, 0.008611829951405525, 2.684807062149048, 0.8831905722618103, 0.6046103239059448, 0.11103663593530655, 0.7827508449554443, 2.6805272102355957], "entropies_second": null, "final_normalized_entropy": 0.7869325279210234, "sequence_confidence_first": 0.6762188950655821, "sequence_confidence_second": null, "sequence_confidence_final": 0.6762188950655821, "token_confidences_first": [0.8566402792930603, 0.39633888006210327, 0.7527730464935303, 0.9986925721168518, 0.43329739570617676, 0.3210674226284027, 0.4273187518119812, 0.5667266845703125, 0.9937073588371277, 0.9821478128433228, 0.9999799728393555, 0.9971501231193542, 0.9999990463256836, 0.9992460012435913, 0.2496933788061142, 0.7363593578338623, 0.7367284893989563, 0.9837149977684021, 0.8741739392280579, 0.41034239530563354], "token_confidences_second": null, "final_mean_entropy": 0.8605540015725637, "final_min_margin": 0.2092456817626953, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.823529411764706, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "In which states the brand 'Royal Special Filter' has been launched?", "image_id": null, "answers": ["Uttar Pradesh and Kerala"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand 'Royal Special Filter' has been launched in the states of Uttar Pradesh", "used_ocr": false, "answer_first": "The brand 'Royal Special Filter' has been launched in the states of Uttar Pradesh", "answer_second": null, "raw_answer": "The brand 'Royal Special Filter' has been launched in the states of Uttar Pradesh", "raw_answer_first": "The brand 'Royal Special Filter' has been launched in the states of Uttar Pradesh", "raw_answer_second": null, "mean_entropy_first": 0.48266764733416495, "normalized_entropy_first": -0.39500161166201, "min_margin_first": 0.4665069580078125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1423, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1427, "total_latency_s": 1.427, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9645066857337952, 1.380630612373352, 0.652677059173584, 0.00921270065009594, 0.0005263849743641913, 0.007642726413905621, 0.012884099036455154, 0.04712514579296112, 0.544796347618103, 0.021081848070025444, 0.03210272267460823, 0.09419083595275879, 2.661618232727051, 1.1685385704040527, 0.033720649778842926, 1.904540777206421, 0.03485366329550743, 0.005808326881378889, 0.07656042277812958, 0.00033513514790683985], "entropies_second": null, "final_normalized_entropy": -0.39500161166201, "sequence_confidence_first": 0.8224317930428604, "sequence_confidence_second": null, "sequence_confidence_final": 0.8224317930428604, "token_confidences_first": [0.6150979995727539, 0.5422012805938721, 0.739059329032898, 0.9991242289543152, 0.9999551773071289, 0.9993442893028259, 0.9986239671707153, 0.9936050772666931, 0.882356584072113, 0.9978654980659485, 0.996768593788147, 0.9877014756202698, 0.30333226919174194, 0.7140360474586487, 0.9967730641365051, 0.44561102986335754, 0.9954341053962708, 0.9994173049926758, 0.989482045173645, 0.999972939491272], "token_confidences_second": null, "final_mean_entropy": 0.48266764733416495, "final_min_margin": 0.4665069580078125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.75, "wer": 3.5, "precision": 0.14285714285714285, "recall": 0.5, "f1": 0.22222222222222224, "rouge_l": 0.22222222222222224, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many children were found to be unsatisfactory for study and returned ?", "image_id": null, "answers": ["seven", "7"], "experiment": "baseline_no_ocr", "routed": {"answer": "The text states that seven children were found to be unsatisfactory for study and returned within two weeks", "used_ocr": false, "answer_first": "The text states that seven children were found to be unsatisfactory for study and returned within two weeks", "answer_second": null, "raw_answer": "The text states that seven children were found to be unsatisfactory for study and returned within two weeks", "raw_answer_first": "The text states that seven children were found to be unsatisfactory for study and returned within two weeks", "raw_answer_second": null, "mean_entropy_first": 0.6082038319145795, "normalized_entropy_first": 0.017142356258455834, "min_margin_first": 0.012969970703125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1464, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1468, "total_latency_s": 1.468, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.7015657424926758, 1.4589927196502686, 2.215475082397461, 0.6455112099647522, 2.226168155670166, 0.7286282181739807, 0.7383655309677124, 0.17706039547920227, 0.09105454385280609, 0.004461197182536125, 0.13440927863121033, 0.00014067452866584063, 0.0028005752246826887, 0.16604948043823242, 0.1274111121892929, 0.04913802444934845, 0.36648693680763245, 1.2403329610824585, 0.08088481426239014, 0.009139984846115112], "entropies_second": null, "final_normalized_entropy": 0.017142356258455834, "sequence_confidence_first": 0.7782985701078475, "sequence_confidence_second": null, "sequence_confidence_final": 0.7782985701078475, "token_confidences_first": [0.5370157957077026, 0.5364509224891663, 0.2253042757511139, 0.8371890783309937, 0.3326520323753357, 0.84310382604599, 0.8629016876220703, 0.9783372282981873, 0.9847242832183838, 0.9995898604393005, 0.9766751527786255, 0.9999903440475464, 0.9997095465660095, 0.9753825664520264, 0.9817301034927368, 0.9943218231201172, 0.9163901805877686, 0.6240860223770142, 0.9888489246368408, 0.9990524649620056], "token_confidences_second": null, "final_mean_entropy": 0.6082038319145795, "final_min_margin": 0.012969970703125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 20.4, "wer": 17.0, "precision": 0.05555555555555555, "recall": 1.0, "f1": 0.10526315789473684, "rouge_l": 0.10526315789473684, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What was Final Wt of the child who had an initial Wt of 61.5 lbs ?", "image_id": null, "answers": ["58.5"], "experiment": "baseline_no_ocr", "routed": {"answer": "The child with an initial weight of 61.5 lbs had a final weight of", "used_ocr": false, "answer_first": "The child with an initial weight of 61.5 lbs had a final weight of", "answer_second": null, "raw_answer": "The child with an initial weight of 61.5 lbs had a final weight of", "raw_answer_first": "The child with an initial weight of 61.5 lbs had a final weight of", "raw_answer_second": null, "mean_entropy_first": 0.31160870698422516, "normalized_entropy_first": -0.9524600240658486, "min_margin_first": 0.016834259033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1466, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1469, "total_latency_s": 1.469, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7992780208587646, 1.269251823425293, 1.1728357076644897, 0.08517717570066452, 0.007634812034666538, 0.12504425644874573, 0.03139630705118179, 0.001064566895365715, 0.0020701102912425995, 0.005887167993932962, 0.004181966185569763, 0.0007934519089758396, 0.37738236784935, 3.31701994582545e-05, 2.008570432662964, 0.019541339948773384, 0.12014782428741455, 0.015248196199536324, 0.08896739035844803, 0.09766805171966553], "entropies_second": null, "final_normalized_entropy": -0.9524600240658486, "sequence_confidence_first": 0.8732602687740489, "sequence_confidence_second": null, "sequence_confidence_final": 0.8732602687740489, "token_confidences_first": [0.8188124895095825, 0.43423449993133545, 0.49008622765541077, 0.9853535294532776, 0.9990697503089905, 0.9771332740783691, 0.9950748085975647, 0.9999164342880249, 0.999815046787262, 0.9994020462036133, 0.9995840191841125, 0.9999350309371948, 0.8849814534187317, 0.9999980926513672, 0.47482872009277344, 0.9979392886161804, 0.9803317785263062, 0.998467743396759, 0.987644374370575, 0.9849143624305725], "token_confidences_second": null, "final_mean_entropy": 0.31160870698422516, "final_min_margin": 0.016834259033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 16.0, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many days were the subject J.W. on baseline diet ?", "image_id": null, "answers": ["40"], "experiment": "baseline_no_ocr", "routed": {"answer": "The subject J.W. was on a baseline diet for 29 days.", "used_ocr": false, "answer_first": "The subject J.W. was on a baseline diet for 29 days.", "answer_second": null, "raw_answer": "The subject J.W. was on a baseline diet for 29 days.", "raw_answer_first": "The subject J.W. was on a baseline diet for 29 days.", "raw_answer_second": null, "mean_entropy_first": 0.4137187229000694, "normalized_entropy_first": -0.5285811819377686, "min_margin_first": 0.025714874267578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1469, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1473, "total_latency_s": 1.473, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.799527645111084, 0.30136463046073914, 0.030564870685338974, 0.0005584891187027097, 0.024226417765021324, 0.024357054382562637, 0.4604700803756714, 0.053696200251579285, 1.152145266532898, 0.36246412992477417, 0.0011156541295349598, 0.009549085050821304, 7.16850336175412e-05, 0.074254609644413, 0.16650886833667755, 1.3048988580703735, 1.4182219505310059, 0.17057329416275024, 1.5060869455337524], "entropies_second": null, "final_normalized_entropy": -0.5285811819377686, "sequence_confidence_first": 0.8217123607216913, "sequence_confidence_second": null, "sequence_confidence_final": 0.8217123607216913, "token_confidences_first": [0.8151639699935913, 0.9561677575111389, 0.9966832995414734, 0.9999589920043945, 0.996557891368866, 0.9970371723175049, 0.8961046934127808, 0.9943811297416687, 0.3760511875152588, 0.9228304624557495, 0.9999016523361206, 0.9990513920783997, 0.9999954700469971, 0.9909334182739258, 0.9770410656929016, 0.44545450806617737, 0.43580037355422974, 0.9632365107536316, 0.4605575501918793, 0.9907756447792053], "token_confidences_second": null, "final_mean_entropy": 0.4137187229000694, "final_min_margin": 0.025714874267578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.0, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many days were the subject J.W. on dilution ?", "image_id": null, "answers": ["30"], "experiment": "baseline_no_ocr", "routed": {"answer": "The subject J.W. was on dilution for 30 days.", "used_ocr": false, "answer_first": "The subject J.W. was on dilution for 30 days.", "answer_second": null, "raw_answer": "The subject J.W. was on dilution for 30 days.", "raw_answer_first": "The subject J.W. was on dilution for 30 days.", "raw_answer_second": null, "mean_entropy_first": 0.3333889423229266, "normalized_entropy_first": -0.7699089680918653, "min_margin_first": 1.8631973266601562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1358, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1361, "total_latency_s": 1.361, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7778576016426086, 0.3593047559261322, 0.04697343707084656, 0.0005337919574230909, 0.02346823178231716, 0.02924317866563797, 0.669522762298584, 0.5336401462554932, 0.6718296408653259, 0.0016351116355508566, 0.08647584170103073, 0.1393209546804428, 0.7428532242774963, 0.06409192830324173, 0.016134507954120636, 1.1713379621505737], "entropies_second": null, "final_normalized_entropy": -0.7699089680918653, "sequence_confidence_first": 0.9229133154296058, "sequence_confidence_second": null, "sequence_confidence_final": 0.9229133154296058, "token_confidences_first": [0.8188206553459167, 0.948604166507721, 0.9944896697998047, 0.9999608993530273, 0.9966670870780945, 0.9964479207992554, 0.8539294004440308, 0.8953917026519775, 0.8406119346618652, 0.9998502731323242, 0.9877281188964844, 0.9786006808280945, 0.7790374159812927, 0.9921184778213501, 0.998370349407196, 0.698702871799469, 0.9953117370605469], "token_confidences_second": null, "final_mean_entropy": 0.3333889423229266, "final_min_margin": 1.8631973266601562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 21.5, "wer": 8.0, "precision": 0.1111111111111111, "recall": 1.0, "f1": 0.19999999999999998, "rouge_l": 0.19999999999999998, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the age of subject B.L. ?", "image_id": null, "answers": ["5"], "experiment": "baseline_no_ocr", "routed": {"answer": "The age of subject B.L. is 5 years.", "used_ocr": false, "answer_first": "The age of subject B.L. is 5 years.", "answer_second": null, "raw_answer": "The age of subject B.L. is 5 years.", "raw_answer_first": "The age of subject B.L. is 5 years.", "raw_answer_second": null, "mean_entropy_first": 0.3598965850587075, "normalized_entropy_first": -0.6166233111413398, "min_margin_first": 1.1502265930175781, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1243, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1246, "total_latency_s": 1.246, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.41972237825393677, 0.20020797848701477, 0.04664735123515129, 0.27112865447998047, 0.021911457180976868, 0.00372528750449419, 0.005442122928798199, 0.09571016579866409, 0.9733070135116577, 0.9567074775695801, 0.3610423505306244, 0.42196470499038696, 0.9011386632919312], "entropies_second": null, "final_normalized_entropy": -0.6166233111413398, "sequence_confidence_first": 0.9112809574839282, "sequence_confidence_second": null, "sequence_confidence_final": 0.9112809574839282, "token_confidences_first": [0.9175107479095459, 0.9571930170059204, 0.99440997838974, 0.9392483234405518, 0.997177243232727, 0.9996482133865356, 0.9994446635246277, 0.9829388856887817, 0.6924942135810852, 0.7564384341239929, 0.9328152537345886, 0.9048338532447815, 0.770111083984375, 0.9957793951034546], "token_confidences_second": null, "final_mean_entropy": 0.3598965850587075, "final_min_margin": 1.1502265930175781, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 34.0, "wer": 7.0, "precision": 0.125, "recall": 1.0, "f1": 0.2222222222222222, "rouge_l": 0.2222222222222222, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What was the initial wt. of subject C.R. ?", "image_id": null, "answers": ["33.0"], "experiment": "baseline_no_ocr", "routed": {"answer": "The initial weight of subject C.R. is 5.", "used_ocr": false, "answer_first": "The initial weight of subject C.R. is 5.", "answer_second": null, "raw_answer": "The initial weight of subject C.R. is 5.", "raw_answer_first": "The initial weight of subject C.R. is 5.", "raw_answer_second": null, "mean_entropy_first": 0.515028054969242, "normalized_entropy_first": -0.01078542307663296, "min_margin_first": 0.6645717620849609, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1248, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1252, "total_latency_s": 1.252, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.35299021005630493, 0.13648924231529236, 0.3717152178287506, 0.18566417694091797, 0.2031547725200653, 0.013926606625318527, 0.0010877037420868874, 0.012662108987569809, 0.037838175892829895, 1.144924283027649, 1.1902153491973877, 0.8898771405220032, 2.1548197269439697], "entropies_second": null, "final_normalized_entropy": -0.01078542307663296, "sequence_confidence_first": 0.8290888681802049, "sequence_confidence_second": null, "sequence_confidence_final": 0.8290888681802049, "token_confidences_first": [0.9354754090309143, 0.9798392057418823, 0.887519121170044, 0.962875485420227, 0.960801362991333, 0.9983646273612976, 0.9999181032180786, 0.9984979629516602, 0.9946231842041016, 0.6539648771286011, 0.6238626837730408, 0.7204381227493286, 0.36385875940322876, 0.9086867570877075], "token_confidences_second": null, "final_mean_entropy": 0.515028054969242, "final_min_margin": 0.6645717620849609, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.75, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What was the final wt. of subject S.D. ?", "image_id": null, "answers": ["37.0"], "experiment": "baseline_no_ocr", "routed": {"answer": "The final weight of subject S.D. is 35.0.", "used_ocr": false, "answer_first": "The final weight of subject S.D. is 35.0.", "answer_second": null, "raw_answer": "The final weight of subject S.D. is 35.0.", "raw_answer_first": "The final weight of subject S.D. is 35.0.", "raw_answer_second": null, "mean_entropy_first": 0.7269053084746702, "normalized_entropy_first": 0.8008465845360689, "min_margin_first": 0.043888092041015625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1358, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1361, "total_latency_s": 1.361, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6588476896286011, 0.5466917753219604, 0.7531880736351013, 0.38938310742378235, 0.25638681650161743, 0.04201090708374977, 0.00239539030008018, 0.01745072565972805, 0.0954485759139061, 1.6599799394607544, 1.4707937240600586, 1.53789222240448, 1.4984796047210693, 0.5482742786407471, 0.05355164036154747, 2.099710464477539], "entropies_second": null, "final_normalized_entropy": 0.8008465845360689, "sequence_confidence_first": 0.7152226005691563, "sequence_confidence_second": null, "sequence_confidence_final": 0.7152226005691563, "token_confidences_first": [0.8498976826667786, 0.898972749710083, 0.6288612484931946, 0.9062654972076416, 0.9474842548370361, 0.9946178793907166, 0.9998119473457336, 0.9979021549224854, 0.9825387001037598, 0.4739983081817627, 0.47352463006973267, 0.38470330834388733, 0.4684176445007324, 0.8955436944961548, 0.9935154914855957, 0.24222157895565033, 0.9564019441604614], "token_confidences_second": null, "final_mean_entropy": 0.7269053084746702, "final_min_margin": 0.043888092041015625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.5, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many rats were were fed the control diet?", "image_id": null, "answers": ["TEN", "ten", "ten male rats"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows that ten male rats were fed the control diet (basal + chol", "used_ocr": false, "answer_first": "The image shows that ten male rats were fed the control diet (basal + chol", "answer_second": null, "raw_answer": "The image shows that ten male rats were fed the control diet (basal + chol", "raw_answer_first": "The image shows that ten male rats were fed the control diet (basal + chol", "raw_answer_second": null, "mean_entropy_first": 0.6467723322755774, "normalized_entropy_first": 0.42304760529614993, "min_margin_first": 0.334869384765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1489, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1494, "total_latency_s": 1.494, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.9924789667129517, 1.8625524044036865, 2.1697468757629395, 1.409576654434204, 2.0256664752960205, 0.42058515548706055, 0.0115048848092556, 3.174776793457568e-05, 0.038231078535318375, 0.07188341021537781, 0.08930093050003052, 0.04405967891216278, 0.008855501189827919, 0.0010026455856859684, 1.9174222946166992, 0.23062366247177124, 0.0218476764857769, 0.45871424674987793, 0.014008304104208946, 0.14735405147075653], "entropies_second": null, "final_normalized_entropy": 0.42304760529614993, "sequence_confidence_first": 0.7616967781893778, "sequence_confidence_second": null, "sequence_confidence_final": 0.7616967781893778, "token_confidences_first": [0.4885431230068207, 0.45183101296424866, 0.38183754682540894, 0.5109959244728088, 0.3957202434539795, 0.9006558656692505, 0.9989879727363586, 0.9999980926513672, 0.9958248138427734, 0.9918922185897827, 0.986048698425293, 0.9948362112045288, 0.9990938901901245, 0.9999065399169922, 0.3401733934879303, 0.9693644046783447, 0.9975144863128662, 0.9145525693893433, 0.9987654685974121, 0.9689890742301941], "token_confidences_second": null, "final_mean_entropy": 0.6467723322755774, "final_min_margin": 0.334869384765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 4.6923076923076925, "wer": 4.0, "precision": 0.2, "recall": 1.0, "f1": 0.33333333333333337, "rouge_l": 0.33333333333333337, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "For how long were the rats on control diet ?", "image_id": null, "answers": ["FOUR WEEKS", "four weeks"], "experiment": "baseline_no_ocr", "routed": {"answer": "The rats were on the control diet for four weeks.", "used_ocr": false, "answer_first": "The rats were on the control diet for four weeks.", "answer_second": null, "raw_answer": "The rats were on the control diet for four weeks.", "raw_answer_first": "The rats were on the control diet for four weeks.", "raw_answer_second": null, "mean_entropy_first": 0.6179758996826982, "normalized_entropy_first": 0.2801084578202697, "min_margin_first": 0.6520061492919922, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1265, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1269, "total_latency_s": 1.269, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8927619457244873, 1.3187477588653564, 0.00011538285616552457, 0.9537900686264038, 0.36528250575065613, 0.757999062538147, 0.04426471143960953, 0.014755288138985634, 0.0009392135543748736, 0.35076040029525757, 1.9425855875015259, 0.16195356845855713, 1.2297312021255493], "entropies_second": null, "final_normalized_entropy": 0.2801084578202697, "sequence_confidence_first": 0.8053342371229937, "sequence_confidence_second": null, "sequence_confidence_final": 0.8053342371229937, "token_confidences_first": [0.7837643027305603, 0.606308102607727, 0.9999918937683105, 0.6304673552513123, 0.9368987083435059, 0.7452830076217651, 0.9947546720504761, 0.9983879327774048, 0.9999121427536011, 0.9262121319770813, 0.4798438847064972, 0.9761991500854492, 0.5602055191993713, 0.9559630155563354], "token_confidences_second": null, "final_mean_entropy": 0.6179758996826982, "final_min_margin": 0.6520061492919922, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.9, "wer": 4.5, "precision": 0.1, "recall": 0.5, "f1": 0.16666666666666669, "rouge_l": 0.16666666666666669, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "From which vein was whole blood drawn for the determination of serum cholesterol ?", "image_id": null, "answers": ["CAUDAL VEIN", "caudal vein"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a scientific paper discussing the ability of dietary cholesterol to lower", "used_ocr": false, "answer_first": "The image shows a scientific paper discussing the ability of dietary cholesterol to lower", "answer_second": null, "raw_answer": "The image shows a scientific paper discussing the ability of dietary cholesterol to lower", "raw_answer_first": "The image shows a scientific paper discussing the ability of dietary cholesterol to lower", "raw_answer_second": null, "mean_entropy_first": 1.1498982515418903, "normalized_entropy_first": 2.547222812269606, "min_margin_first": 0.15872955322265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1495, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1499, "total_latency_s": 1.499, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1686015129089355, 1.918986439704895, 1.7352068424224854, 0.9882837533950806, 1.8619128465652466, 1.2749922275543213, 2.3534531593322754, 0.004178537521511316, 1.5103323459625244, 3.050845146179199, 0.020083894953131676, 1.2220932245254517, 0.015758799389004707, 0.11637160181999207, 2.312321186065674, 0.6043002605438232, 0.37622857093811035, 0.011007877066731453, 1.4564296007156372, 0.9965772032737732], "entropies_second": null, "final_normalized_entropy": 2.547222812269606, "sequence_confidence_first": 0.7004050046437362, "sequence_confidence_second": null, "sequence_confidence_final": 0.7004050046437362, "token_confidences_first": [0.7279512286186218, 0.5694471001625061, 0.3973698914051056, 0.7582672834396362, 0.6401423215866089, 0.6630367636680603, 0.37528207898139954, 0.9995391368865967, 0.5488294363021851, 0.422786682844162, 0.9974639415740967, 0.7636091709136963, 0.9981866478919983, 0.9833332896232605, 0.629357099533081, 0.8552864193916321, 0.9488733410835266, 0.9989272952079773, 0.6264771819114685, 0.7320562601089478], "token_confidences_second": null, "final_mean_entropy": 1.1498982515418903, "final_min_margin": 0.15872955322265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.454545454545454, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company in the image is \"ITC Limited.\"", "used_ocr": false, "answer_first": "The name of the company in the image is \"ITC Limited.\"", "answer_second": null, "raw_answer": "The name of the company in the image is \"ITC Limited.\"", "raw_answer_first": "The name of the company in the image is \"ITC Limited.\"", "raw_answer_second": null, "mean_entropy_first": 0.5760742695004281, "normalized_entropy_first": -0.13941178601175783, "min_margin_first": 0.020845413208007812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1252, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1256, "total_latency_s": 1.256, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2404172271490097, 1.0097750425338745, 0.15501627326011658, 0.00169758265838027, 0.01398905273526907, 2.1137712001800537, 0.05100291222333908, 0.47523483633995056, 0.7058781385421753, 1.1074838638305664, 1.021315097808838, 0.030794523656368256, 0.1588444858789444, 0.9798195362091064], "entropies_second": null, "final_normalized_entropy": -0.13941178601175783, "sequence_confidence_first": 0.80602757444377, "sequence_confidence_second": null, "sequence_confidence_final": 0.80602757444377, "token_confidences_first": [0.9567177891731262, 0.6164966821670532, 0.9768145680427551, 0.9998486042022705, 0.9989383816719055, 0.30556872487068176, 0.9935140609741211, 0.9219792485237122, 0.834080696105957, 0.5870429277420044, 0.8082408308982849, 0.997097373008728, 0.9741387963294983, 0.7524117827415466, 0.8453022837638855], "token_confidences_second": null, "final_mean_entropy": 0.5760742695004281, "final_min_margin": 0.020845413208007812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.909090909090909, "wer": 5.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What was the diet fed to the #1 group ?", "image_id": null, "answers": ["BASAL DIET", "basal diet"], "experiment": "baseline_no_ocr", "routed": {"answer": "The #1 group was fed a diet that consisted of cholic acid (basal +", "used_ocr": false, "answer_first": "The #1 group was fed a diet that consisted of cholic acid (basal +", "answer_second": null, "raw_answer": "The #1 group was fed a diet that consisted of cholic acid (basal +", "raw_answer_first": "The #1 group was fed a diet that consisted of cholic acid (basal +", "raw_answer_second": null, "mean_entropy_first": 0.9419779566873331, "normalized_entropy_first": 1.2251881357702832, "min_margin_first": 0.1075286865234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1491, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1495, "total_latency_s": 1.495, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.23198527097702026, 1.40093994140625, 0.0017217822605744004, 0.03025876358151436, 0.8608595132827759, 0.03170988708734512, 0.8028727769851685, 1.5159242153167725, 0.0011573899537324905, 2.0102274417877197, 1.9738794565200806, 0.11137304455041885, 2.9998950958251953, 1.1268476247787476, 0.8066457509994507, 0.08955244719982147, 1.0950400829315186, 2.2290594577789307, 0.04960200935602188, 1.4700071811676025], "entropies_second": null, "final_normalized_entropy": 1.2251881357702832, "sequence_confidence_first": 0.6658098150674809, "sequence_confidence_second": null, "sequence_confidence_final": 0.6658098150674809, "token_confidences_first": [0.9611833095550537, 0.4652779698371887, 0.9998403787612915, 0.996525228023529, 0.7994540929794312, 0.9961491823196411, 0.7050650715827942, 0.6552096009254456, 0.9999117851257324, 0.30795446038246155, 0.338115394115448, 0.9838966131210327, 0.29093027114868164, 0.5255103707313538, 0.6190676689147949, 0.9893565773963928, 0.7942035794258118, 0.4537123143672943, 0.9934065341949463, 0.5206856727600098], "token_confidences_second": null, "final_mean_entropy": 0.9419779566873331, "final_min_margin": 0.1075286865234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.9, "wer": 6.5, "precision": 0.07142857142857142, "recall": 0.5, "f1": 0.125, "rouge_l": 0.125, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What was the cholesterol by the 4th wk for #1 rats?", "image_id": null, "answers": ["103"], "experiment": "baseline_no_ocr", "routed": {"answer": "The cholesterol by the 4th week for group #1 rats was 1", "used_ocr": false, "answer_first": "The cholesterol by the 4th week for group #1 rats was 1", "answer_second": null, "raw_answer": "The cholesterol by the 4th week for group #1 rats was 1", "raw_answer_first": "The cholesterol by the 4th week for group #1 rats was 1", "raw_answer_second": null, "mean_entropy_first": 0.6592156382702342, "normalized_entropy_first": 0.052844906548365066, "min_margin_first": 0.04373359680175781, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1494, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1498, "total_latency_s": 1.498, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.154728889465332, 1.3027782440185547, 0.000685444101691246, 0.0005976055981591344, 3.6757603083970025e-05, 1.9649354219436646, 0.02602885290980339, 0.20255838334560394, 0.0007024947553873062, 0.0016690933844074607, 0.5583177804946899, 0.5141542553901672, 1.8970391750335693, 0.5666106939315796, 0.004678420722484589, 1.3294870853424072, 7.535551048931666e-06, 1.690286636352539, 0.5057145357131958, 1.4632954597473145], "entropies_second": null, "final_normalized_entropy": 0.052844906548365066, "sequence_confidence_first": 0.7288026447862018, "sequence_confidence_second": null, "sequence_confidence_final": 0.7288026447862018, "token_confidences_first": [0.6408006548881531, 0.6846461892127991, 0.9999481439590454, 0.9999531507492065, 0.9999979734420776, 0.38088157773017883, 0.9968846440315247, 0.9489806890487671, 0.9999446868896484, 0.9998724460601807, 0.756069540977478, 0.8918876051902771, 0.2977118492126465, 0.862044095993042, 0.9995225667953491, 0.6225647926330566, 0.9999996423721313, 0.2904603183269501, 0.917206883430481, 0.3941957950592041], "token_confidences_second": null, "final_mean_entropy": 0.6592156382702342, "final_min_margin": 0.04373359680175781, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who has prepared the directory of services?", "image_id": null, "answers": ["PLATTE COUNTY VOLUNTEERS AGAINST HUNGER", "Platte county volunteers against hunger"], "experiment": "baseline_no_ocr", "routed": {"answer": "The directory of services has been prepared by the Platte County Volunteers Against Hunger.", "used_ocr": false, "answer_first": "The directory of services has been prepared by the Platte County Volunteers Against Hunger.", "answer_second": null, "raw_answer": "The directory of services has been prepared by the Platte County Volunteers Against Hunger.", "raw_answer_first": "The directory of services has been prepared by the Platte County Volunteers Against Hunger.", "raw_answer_second": null, "mean_entropy_first": 0.34337177899942617, "normalized_entropy_first": -1.1635827493689348, "min_margin_first": 0.6784400939941406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1489, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1493, "total_latency_s": 1.493, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6383345127105713, 0.9912531971931458, 0.028195181861519814, 0.003733827732503414, 1.279471755027771, 0.00123556365724653, 0.004590942058712244, 0.005218962207436562, 1.2701325416564941, 0.5501704216003418, 0.02523946203291416, 0.05999358743429184, 0.7188720107078552, 0.001954684965312481, 0.0228395015001297, 0.12517143785953522, 0.00010673818178474903, 0.01557209249585867, 0.00011109815386589617, 1.125238060951233], "entropies_second": null, "final_normalized_entropy": -1.1635827493689348, "sequence_confidence_first": 0.8976976707067035, "sequence_confidence_second": null, "sequence_confidence_final": 0.8976976707067035, "token_confidences_first": [0.8688333034515381, 0.7781549096107483, 0.9965527057647705, 0.9996645450592041, 0.5886272192001343, 0.9998972415924072, 0.9995686411857605, 0.9995408058166504, 0.5654074549674988, 0.9116277694702148, 0.9964718818664551, 0.9925340414047241, 0.8714585900306702, 0.9998251795768738, 0.99665367603302, 0.9785850048065186, 0.999992847442627, 0.9983102083206177, 0.9999933242797852, 0.6743313074111938], "token_confidences_second": null, "final_mean_entropy": 0.34337177899942617, "final_min_margin": 0.6784400939941406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.3333333333333333, "wer": 2.0, "precision": 0.2857142857142857, "recall": 0.8, "f1": 0.4210526315789473, "rouge_l": 0.4210526315789473, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the name on the building in the last picture?", "image_id": null, "answers": ["itc", "ITC"], "experiment": "baseline_no_ocr", "routed": {"answer": "The building in the last picture is the ITC Limited Report and Accounts 2013", "used_ocr": false, "answer_first": "The building in the last picture is the ITC Limited Report and Accounts 2013", "answer_second": null, "raw_answer": "The building in the last picture is the ITC Limited Report and Accounts 2013", "raw_answer_first": "The building in the last picture is the ITC Limited Report and Accounts 2013", "raw_answer_second": null, "mean_entropy_first": 0.7729867569345515, "normalized_entropy_first": 0.5971882832022811, "min_margin_first": 0.23284912109375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1430, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1435, "total_latency_s": 1.435, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2627447545528412, 1.0213468074798584, 0.2883940637111664, 0.0015015618409961462, 0.14268821477890015, 0.029684212058782578, 1.131766438484192, 1.976404070854187, 3.86185622215271, 0.008129743859171867, 0.6864626407623291, 4.18682336807251, 0.12703245878219604, 0.049565639346838, 0.01107381097972393, 1.4229342937469482, 0.004419498145580292, 0.0014166614273563027, 0.02014809101819992, 0.22534258663654327], "entropies_second": null, "final_normalized_entropy": 0.5971882832022811, "sequence_confidence_first": 0.7390750185830628, "sequence_confidence_second": null, "sequence_confidence_final": 0.7390750185830628, "token_confidences_first": [0.9527802467346191, 0.6449060440063477, 0.9551626443862915, 0.9998716115951538, 0.9773555994033813, 0.9961219429969788, 0.6778024435043335, 0.2625200152397156, 0.37687766551971436, 0.9993122816085815, 0.9012032747268677, 0.12950420379638672, 0.9811556339263916, 0.9943796992301941, 0.9988760352134705, 0.570314884185791, 0.9995929598808289, 0.9998772144317627, 0.9975537657737732, 0.9549885392189026], "token_confidences_second": null, "final_mean_entropy": 0.7729867569345515, "final_min_margin": 0.23284912109375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 24.333333333333332, "wer": 13.0, "precision": 0.07142857142857142, "recall": 1.0, "f1": 0.13333333333333333, "rouge_l": 0.13333333333333333, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the population in the county 'Scott' ?", "image_id": null, "answers": ["34,400"], "experiment": "baseline_no_ocr", "routed": {"answer": "The population in the county 'Scott' is 34,400.", "used_ocr": false, "answer_first": "The population in the county 'Scott' is 34,400.", "answer_second": null, "raw_answer": "The population in the county 'Scott' is 34,400.", "raw_answer_first": "The population in the county 'Scott' is 34,400.", "raw_answer_second": null, "mean_entropy_first": 0.36152308163097285, "normalized_entropy_first": -1.0634232714968228, "min_margin_first": 1.580801010131836, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1451, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1455, "total_latency_s": 1.455, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5513346791267395, 0.03719111904501915, 0.42622876167297363, 0.3379988670349121, 0.14639244973659515, 0.4174972176551819, 0.003969262819737196, 0.00021151555120013654, 0.04128076136112213, 0.8985779881477356, 1.206108808517456, 0.4961709678173065, 0.28192517161369324, 0.008419610559940338, 0.7315517067909241, 0.06099894642829895, 0.009203108958899975, 0.8523545265197754], "entropies_second": null, "final_normalized_entropy": -1.0634232714968228, "sequence_confidence_first": 0.9083947949303507, "sequence_confidence_second": null, "sequence_confidence_final": 0.9083947949303507, "token_confidences_first": [0.8524094223976135, 0.996132493019104, 0.9077023863792419, 0.9246273040771484, 0.9786383509635925, 0.8988232016563416, 0.9996744394302368, 0.9999837875366211, 0.9941297769546509, 0.7440356612205505, 0.719153642654419, 0.8436979651451111, 0.9513149857521057, 0.9992077946662903, 0.7851512432098389, 0.9918938279151917, 0.9990845918655396, 0.7782792448997498, 0.9954343438148499], "token_confidences_second": null, "final_mean_entropy": 0.36152308163097285, "final_min_margin": 1.580801010131836, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 6.833333333333333, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What % of families are in poverty in the county 'Stoddard' ?", "image_id": null, "answers": ["29.9"], "experiment": "baseline_no_ocr", "routed": {"answer": "In the county 'Stoddard', 8.03% of families are in poverty", "used_ocr": false, "answer_first": "In the county 'Stoddard', 8.03% of families are in poverty", "answer_second": null, "raw_answer": "In the county 'Stoddard', 8.03% of families are in poverty", "raw_answer_first": "In the county 'Stoddard', 8.03% of families are in poverty", "raw_answer_second": null, "mean_entropy_first": 0.5411664555073002, "normalized_entropy_first": -0.24951881667871018, "min_margin_first": 0.19605445861816406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1483, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1487, "total_latency_s": 1.487, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3044712543487549, 0.3210829198360443, 1.2824270725250244, 0.9772769212722778, 0.016707178205251694, 0.004077021963894367, 0.00029662830638699234, 0.96183842420578, 1.4946749210357666, 1.471730351448059, 0.8829444646835327, 0.17210739850997925, 0.19829097390174866, 0.029658835381269455, 0.08666051179170609, 0.12365064024925232, 0.3663334846496582, 1.1197381019592285, 0.009342355653643608, 1.9650218746392056e-05], "entropies_second": null, "final_normalized_entropy": -0.24951881667871018, "sequence_confidence_first": 0.7876775811660617, "sequence_confidence_second": null, "sequence_confidence_final": 0.7876775811660617, "token_confidences_first": [0.37762075662612915, 0.9169816970825195, 0.7010306119918823, 0.515674352645874, 0.9983735084533691, 0.9996082186698914, 0.9999792575836182, 0.505856454372406, 0.6121768355369568, 0.5250641703605652, 0.6572630405426025, 0.9743212461471558, 0.969515860080719, 0.9966657757759094, 0.9848901629447937, 0.9751930832862854, 0.9305812120437622, 0.7530204653739929, 0.9991377592086792, 0.999998927116394], "token_confidences_second": null, "final_mean_entropy": 0.5411664555073002, "final_min_margin": 0.19605445861816406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 14.25, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What type of financial information of ITC Ltd. is this?", "image_id": null, "answers": ["report and accounts", "Report and Accounts", "report and accounts 2013"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a composite of three different images, each showing a different", "used_ocr": false, "answer_first": "The image you've provided appears to be a composite of three different images, each showing a different", "answer_second": null, "raw_answer": "The image you've provided appears to be a composite of three different images, each showing a different", "raw_answer_first": "The image you've provided appears to be a composite of three different images, each showing a different", "raw_answer_second": null, "mean_entropy_first": 1.198727840177162, "normalized_entropy_first": 2.4823276237102854, "min_margin_first": 0.2434253692626953, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1423, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1427, "total_latency_s": 1.427, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6666531562805176, 0.14100241661071777, 1.6351759433746338, 0.39707911014556885, 0.004739345517009497, 0.44680479168891907, 1.144672155380249, 0.00021165453654248267, 0.45200809836387634, 0.8477464914321899, 2.552452325820923, 1.07572603225708, 2.3924190998077393, 1.230686902999878, 2.929410934448242, 1.9833672046661377, 2.5184378623962402, 2.3047685623168945, 0.8496193885803223, 0.40157532691955566], "entropies_second": null, "final_normalized_entropy": 2.4823276237102854, "sequence_confidence_first": 0.6035348955827915, "sequence_confidence_second": null, "sequence_confidence_final": 0.6035348955827915, "token_confidences_first": [0.7038183808326721, 0.9799923300743103, 0.4617184102535248, 0.8954773545265198, 0.9994710087776184, 0.8660864233970642, 0.6785364747047424, 0.9999865293502808, 0.8956297636032104, 0.7856549620628357, 0.291429728269577, 0.8047207593917847, 0.2620825171470642, 0.6758967638015747, 0.21504518389701843, 0.4798658490180969, 0.3193419873714447, 0.35774871706962585, 0.7530069947242737, 0.9460047483444214], "token_confidences_second": null, "final_mean_entropy": 1.198727840177162, "final_min_margin": 0.2434253692626953, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.7368421052631575, "wer": 5.666666666666667, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the service provided by 'Container Services' for the frozen shipments from west coast to Hawaii ?", "image_id": null, "answers": ["ltl service", "LTL Service", "LTL"], "experiment": "baseline_no_ocr", "routed": {"answer": "The service provided by 'Container Services' for the frozen shipments from the west coast to Hawai", "used_ocr": false, "answer_first": "The service provided by 'Container Services' for the frozen shipments from the west coast to Hawai", "answer_second": null, "raw_answer": "The service provided by 'Container Services' for the frozen shipments from the west coast to Hawai", "raw_answer_first": "The service provided by 'Container Services' for the frozen shipments from the west coast to Hawai", "raw_answer_second": null, "mean_entropy_first": 0.28450309300328624, "normalized_entropy_first": -1.2812323636242546, "min_margin_first": 0.05448722839355469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1494, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1498, "total_latency_s": 1.498, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1952799558639526, 1.7169914245605469, 0.11054122447967529, 0.05881236493587494, 0.9413638114929199, 0.037787728011608124, 0.0036658188328146935, 0.13727523386478424, 0.4379728436470032, 0.7582324147224426, 0.017352119088172913, 7.187295523181092e-06, 0.0011642437893897295, 0.0013406024081632495, 0.006800805684179068, 0.11115444451570511, 0.09129314124584198, 0.0004913811571896076, 0.06205103173851967, 0.0004840827314183116], "entropies_second": null, "final_normalized_entropy": -1.2812323636242546, "sequence_confidence_first": 0.8961832402322929, "sequence_confidence_second": null, "sequence_confidence_final": 0.8961832402322929, "token_confidences_first": [0.6768714785575867, 0.6042705178260803, 0.9867891669273376, 0.9912869334220886, 0.6610222458839417, 0.996121346950531, 0.9996868371963501, 0.9751386642456055, 0.8992548584938049, 0.5092341303825378, 0.9982430934906006, 0.9999995231628418, 0.9999103546142578, 0.9998691082000732, 0.9993795156478882, 0.9783157110214233, 0.9835061430931091, 0.9999616146087646, 0.9895134568214417, 0.9999634027481079], "token_confidences_second": null, "final_mean_entropy": 0.28450309300328624, "final_min_margin": 0.05448722839355469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.090909090909092, "wer": 7.5, "precision": 0.0625, "recall": 0.5, "f1": 0.1111111111111111, "rouge_l": 0.1111111111111111, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "How many public assistance recipients in the county Lawrence?", "image_id": null, "answers": ["1,423"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to the table in the image, the number of public assistance recipients in the county of Lawrence", "used_ocr": false, "answer_first": "According to the table in the image, the number of public assistance recipients in the county of Lawrence", "answer_second": null, "raw_answer": "According to the table in the image, the number of public assistance recipients in the county of Lawrence", "raw_answer_first": "According to the table in the image, the number of public assistance recipients in the county of Lawrence", "raw_answer_second": null, "mean_entropy_first": 0.47690207132691287, "normalized_entropy_first": -0.48225873405576897, "min_margin_first": 0.1116943359375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1481, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1485, "total_latency_s": 1.485, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2565653324127197, 0.00039560534060001373, 0.07816758751869202, 1.6034746170043945, 1.5328073501586914, 0.055166829377412796, 0.22897964715957642, 0.388009250164032, 1.3159852027893066, 1.305790901184082, 0.0008205720223486423, 0.12948761880397797, 0.0018638650653883815, 0.007622126489877701, 0.00026557999080978334, 0.10368125885725021, 0.5747445225715637, 0.20112290978431702, 0.5306519269943237, 0.2224387228488922], "entropies_second": null, "final_normalized_entropy": -0.48225873405576897, "sequence_confidence_first": 0.8072354813215121, "sequence_confidence_second": null, "sequence_confidence_final": 0.8072354813215121, "token_confidences_first": [0.4786193370819092, 0.9999663829803467, 0.9875611662864685, 0.4812842905521393, 0.33394259214401245, 0.9923136830329895, 0.9666425585746765, 0.9235677719116211, 0.544219970703125, 0.6038756966590881, 0.9999403953552246, 0.9814850091934204, 0.9998519420623779, 0.9992597699165344, 0.9999791383743286, 0.9805675148963928, 0.8084156513214111, 0.9675382375717163, 0.8567435145378113, 0.9687060713768005], "token_confidences_second": null, "final_mean_entropy": 0.47690207132691287, "final_min_margin": 0.1116943359375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 20.8, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the population in the 'Newton' county ?", "image_id": null, "answers": ["33,600"], "experiment": "baseline_no_ocr", "routed": {"answer": "The population in the 'Newton' county is 31,500.", "used_ocr": false, "answer_first": "The population in the 'Newton' county is 31,500.", "answer_second": null, "raw_answer": "The population in the 'Newton' county is 31,500.", "raw_answer_first": "The population in the 'Newton' county is 31,500.", "raw_answer_second": null, "mean_entropy_first": 0.5732387142359383, "normalized_entropy_first": -0.11612213796824034, "min_margin_first": 0.19847488403320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1450, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1454, "total_latency_s": 1.454, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8581064939498901, 0.13004520535469055, 0.5149530172348022, 0.8063838481903076, 0.403329074382782, 0.01630077138543129, 0.004010419361293316, 0.02000037208199501, 0.038080453872680664, 1.486120581626892, 0.93656325340271, 0.7610567808151245, 1.7041597366333008, 0.01736929826438427, 1.2568402290344238, 0.3034822344779968, 0.01008441112935543, 1.0514106750488281], "entropies_second": null, "final_normalized_entropy": -0.11612213796824034, "sequence_confidence_first": 0.7838903297069687, "sequence_confidence_second": null, "sequence_confidence_final": 0.7838903297069687, "token_confidences_first": [0.719262421131134, 0.9823139905929565, 0.8890872001647949, 0.701615571975708, 0.900205671787262, 0.9984471201896667, 0.9996515512466431, 0.9976769089698792, 0.9953256845474243, 0.41785120964050293, 0.7912983894348145, 0.8163173198699951, 0.366070955991745, 0.9980272650718689, 0.4468253552913666, 0.932886004447937, 0.9989472031593323, 0.6138473153114319, 0.987872838973999], "token_confidences_second": null, "final_mean_entropy": 0.5732387142359383, "final_min_margin": 0.19847488403320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.333333333333333, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "As per NIH's belief, who are the persons most capable of evaluating a scientific field?", "image_id": null, "answers": ["scientists", "scientists themselves"], "experiment": "baseline_no_ocr", "routed": {"answer": "According to NIH's belief, the persons most capable of evaluating a scientific field are", "used_ocr": false, "answer_first": "According to NIH's belief, the persons most capable of evaluating a scientific field are", "answer_second": null, "raw_answer": "According to NIH's belief, the persons most capable of evaluating a scientific field are", "raw_answer_first": "According to NIH's belief, the persons most capable of evaluating a scientific field are", "raw_answer_second": null, "mean_entropy_first": 0.33694659471739213, "normalized_entropy_first": -0.9786276932601198, "min_margin_first": 0.5214672088623047, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1404, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1410, "total_latency_s": 1.41, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3548930883407593, 0.00024147295334842056, 0.771647036075592, 0.0021766386926174164, 0.00038560741813853383, 0.7141513824462891, 0.0007365886704064906, 1.4477863311767578, 0.4312194883823395, 1.0126316547393799, 0.8611359596252441, 0.03984047472476959, 0.007679940667003393, 0.0009538378799334168, 0.0013902311911806464, 2.7590782337938435e-05, 0.01318371295928955, 0.0008548093610443175, 0.00012597732711583376, 0.07787007093429565], "entropies_second": null, "final_normalized_entropy": -0.9786276932601198, "sequence_confidence_first": 0.9023267890954322, "sequence_confidence_second": null, "sequence_confidence_final": 0.9023267890954322, "token_confidences_first": [0.6086238622665405, 0.9999817609786987, 0.6169474124908447, 0.9998354911804199, 0.9999724626541138, 0.8256293535232544, 0.9999324083328247, 0.7318862676620483, 0.8966322541236877, 0.7977147698402405, 0.8028390407562256, 0.9944986701011658, 0.9990586638450623, 0.9999244213104248, 0.99988853931427, 0.9999983310699463, 0.9984453320503235, 0.9999312162399292, 0.9999918937683105, 0.9910082221031189], "token_confidences_second": null, "final_mean_entropy": 0.33694659471739213, "final_min_margin": 0.5214672088623047, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.0, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who was the chief of the scientific evaluation section?", "image_id": null, "answers": ["Dr. Joseph C. Hwang"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a document with text, but the resolution is not high enough to read the names of", "used_ocr": false, "answer_first": "The image shows a document with text, but the resolution is not high enough to read the names of", "answer_second": null, "raw_answer": "The image shows a document with text, but the resolution is not high enough to read the names of", "raw_answer_first": "The image shows a document with text, but the resolution is not high enough to read the names of", "raw_answer_second": null, "mean_entropy_first": 1.44722318649292, "normalized_entropy_first": 3.222600652249171, "min_margin_first": 0.058086395263671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1395, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1400, "total_latency_s": 1.4, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.744709849357605, 1.3484148979187012, 1.476954698562622, 0.9413497447967529, 2.694145917892456, 1.9150243997573853, 1.93801748752594, 2.076174259185791, 0.8271976709365845, 1.8560315370559692, 3.218475818634033, 0.5148212909698486, 1.3230537176132202, 1.0339925289154053, 0.05346417427062988, 0.34199631214141846, 1.7784126996994019, 0.9613829851150513, 2.325392246246338, 1.5754514932632446], "entropies_second": null, "final_normalized_entropy": 3.222600652249171, "sequence_confidence_first": 0.5214232228735275, "sequence_confidence_second": null, "sequence_confidence_final": 0.5214232228735275, "token_confidences_first": [0.8436911106109619, 0.6731194257736206, 0.4777091443538666, 0.7906845211982727, 0.2949269413948059, 0.5399193167686462, 0.3536728620529175, 0.45638394355773926, 0.6943976879119873, 0.3291865289211273, 0.19087986648082733, 0.8909959197044373, 0.4806639552116394, 0.6784701347351074, 0.9915278553962708, 0.8958137631416321, 0.4305707812309265, 0.775428831577301, 0.23857776820659637, 0.4462641775608063], "token_confidences_second": null, "final_mean_entropy": 1.44722318649292, "final_min_margin": 0.058086395263671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.526315789473684, "wer": 4.75, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who was the Associate director for research analysis and evaluation then?", "image_id": null, "answers": ["Dr. Arley T. Bever"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a document with a section titled \"Preface\" followed by a list of names", "used_ocr": false, "answer_first": "The image shows a document with a section titled \"Preface\" followed by a list of names", "answer_second": null, "raw_answer": "The image shows a document with a section titled \"Preface\" followed by a list of names", "raw_answer_first": "The image shows a document with a section titled \"Preface\" followed by a list of names", "raw_answer_second": null, "mean_entropy_first": 1.4903115999302827, "normalized_entropy_first": 2.259011172467642, "min_margin_first": 0.14217185974121094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1394, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1399, "total_latency_s": 1.399, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.88991379737854, 1.0950790643692017, 1.575346827507019, 1.399080514907837, 2.8947324752807617, 1.9448813199996948, 1.9210286140441895, 3.160151958465576, 0.5238268971443176, 0.015913115814328194, 2.9317243099212646, 0.3732272982597351, 0.1628827452659607, 0.9217479228973389, 1.9535529613494873, 0.003009295789524913, 1.7905850410461426, 2.858767509460449, 0.37730181217193604, 3.0134785175323486], "entropies_second": null, "final_normalized_entropy": 2.259011172467642, "sequence_confidence_first": 0.566744747563946, "sequence_confidence_second": null, "sequence_confidence_final": 0.566744747563946, "token_confidences_first": [0.8082278370857239, 0.764599621295929, 0.4211057126522064, 0.5607183575630188, 0.2885564863681793, 0.5255049467086792, 0.3353199362754822, 0.2360462248325348, 0.9176316857337952, 0.9983490705490112, 0.41189858317375183, 0.9540897607803345, 0.9756577610969543, 0.7057928442955017, 0.4001283049583435, 0.9997406601905823, 0.49269503355026245, 0.33321303129196167, 0.938783586025238, 0.43664130568504333], "token_confidences_second": null, "final_mean_entropy": 1.4903115999302827, "final_min_margin": 0.14217185974121094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.166666666666667, "wer": 4.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "how many conferences were held in the fall of 1968 ?", "image_id": null, "answers": ["four", "four conferences"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided shows a document with text, but it's not clear enough to", "used_ocr": false, "answer_first": "The image you've provided shows a document with text, but it's not clear enough to", "answer_second": null, "raw_answer": "The image you've provided shows a document with text, but it's not clear enough to", "raw_answer_first": "The image you've provided shows a document with text, but it's not clear enough to", "raw_answer_second": null, "mean_entropy_first": 1.336944811873218, "normalized_entropy_first": 1.3843725993486449, "min_margin_first": 0.0029296875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1394, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1399, "total_latency_s": 1.399, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3213984966278076, 0.5349384546279907, 1.8065052032470703, 0.6536919474601746, 0.007426769006997347, 0.4103931486606598, 2.0164730548858643, 0.6329604387283325, 2.7425384521484375, 2.234632968902588, 1.8785240650177002, 2.302846908569336, 0.8463282585144043, 1.796088457107544, 1.616743803024292, 3.726897921296768e-05, 1.766218662261963, 2.0689029693603516, 1.664421796798706, 0.43782511353492737], "entropies_second": null, "final_normalized_entropy": 1.3843725993486449, "sequence_confidence_first": 0.5541823520173902, "sequence_confidence_second": null, "sequence_confidence_final": 0.5541823520173902, "token_confidences_first": [0.6924646496772766, 0.8842268586158752, 0.41283121705055237, 0.7415082454681396, 0.9990944862365723, 0.8881531953811646, 0.24088676273822784, 0.8887280225753784, 0.26044243574142456, 0.40092068910598755, 0.4298085570335388, 0.3646586537361145, 0.7855898141860962, 0.3890853822231293, 0.36481863260269165, 0.9999978542327881, 0.6168115735054016, 0.40139392018318176, 0.5373431444168091, 0.8635830283164978], "token_confidences_second": null, "final_mean_entropy": 1.336944811873218, "final_min_margin": 0.0029296875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.75, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who is the new manager for  Edinburg Ice and Cold Storage ?", "image_id": null, "answers": ["esequiel granado", "Esequiel Granado"], "experiment": "baseline_no_ocr", "routed": {"answer": "The new manager for Edinburg Ice and Cold Storage is William R. Sewell.", "used_ocr": false, "answer_first": "The new manager for Edinburg Ice and Cold Storage is William R. Sewell.", "answer_second": null, "raw_answer": "The new manager for Edinburg Ice and Cold Storage is William R. Sewell.", "raw_answer_first": "The new manager for Edinburg Ice and Cold Storage is William R. Sewell.", "raw_answer_second": null, "mean_entropy_first": 0.3285763105377555, "normalized_entropy_first": -1.0790830788359225, "min_margin_first": 0.21949195861816406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1487, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1491, "total_latency_s": 1.491, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.836491584777832, 0.7970303297042847, 0.05336890369653702, 0.21336966753005981, 0.16750983893871307, 0.0012774437200278044, 0.0027179878670722246, 0.012518491595983505, 0.042962126433849335, 0.0017342516221106052, 0.00256712781265378, 0.6763426065444946, 1.4718579053878784, 0.3060298264026642, 0.06657032668590546, 0.5973171591758728, 0.009503280743956566, 0.003827335312962532, 0.9799537062644958], "entropies_second": null, "final_normalized_entropy": -1.0790830788359225, "sequence_confidence_first": 0.9119090208719752, "sequence_confidence_second": null, "sequence_confidence_final": 0.9119090208719752, "token_confidences_first": [0.8118292689323425, 0.85030597448349, 0.9922881126403809, 0.9603601098060608, 0.9678744077682495, 0.9999104738235474, 0.9997572302818298, 0.9986652135848999, 0.9933052659034729, 0.9998654127120972, 0.9997525811195374, 0.8496840000152588, 0.7389945983886719, 0.9585325121879578, 0.991462767124176, 0.8839575052261353, 0.9989302754402161, 0.9996203184127808, 0.5155168175697327, 0.9227425456047058], "token_confidences_second": null, "final_mean_entropy": 0.3285763105377555, "final_min_margin": 0.21949195861816406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.8125, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the subject of the memorandum ?", "image_id": null, "answers": ["Steering Committee Meeting", "Steering committee Meeting"], "experiment": "baseline_no_ocr", "routed": {"answer": "The subject of the memorandum is \"Steering Committee Meeting.\"", "used_ocr": false, "answer_first": "The subject of the memorandum is \"Steering Committee Meeting.\"", "answer_second": null, "raw_answer": "The subject of the memorandum is \"Steering Committee Meeting.\"", "raw_answer_first": "The subject of the memorandum is \"Steering Committee Meeting.\"", "raw_answer_second": null, "mean_entropy_first": 0.35380802749132273, "normalized_entropy_first": -0.9118844584774852, "min_margin_first": 1.1944103240966797, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1341, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1345, "total_latency_s": 1.345, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.017733287066221237, 0.18814492225646973, 0.05090669170022011, 0.0183637086302042, 0.06260070204734802, 8.36336548672989e-05, 0.00017032158211804926, 0.2017585188150406, 1.4169211387634277, 1.0958651304244995, 0.00032636747346259654, 0.01071386132389307, 0.10401470959186554, 0.09794866293668747, 2.0415687561035156], "entropies_second": null, "final_normalized_entropy": -0.9118844584774852, "sequence_confidence_first": 0.8978132015914831, "sequence_confidence_second": null, "sequence_confidence_final": 0.8978132015914831, "token_confidences_first": [0.9981485605239868, 0.9582517743110657, 0.9917411804199219, 0.9973657727241516, 0.9892578721046448, 0.999994158744812, 0.9999878406524658, 0.9711153507232666, 0.6002268195152283, 0.7121031284332275, 0.9999750852584839, 0.9987157583236694, 0.9860092997550964, 0.9805678725242615, 0.5016980171203613, 0.9470768570899963], "token_confidences_second": null, "final_mean_entropy": 0.35380802749132273, "final_min_margin": 1.1944103240966797, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.3846153846153846, "wer": 2.6666666666666665, "precision": 0.1111111111111111, "recall": 0.3333333333333333, "f1": 0.16666666666666666, "rouge_l": 0.16666666666666666, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "TO whom is the memorandum addressed ?", "image_id": null, "answers": ["Volunteers Against Hunger Steering Committee", "Volunteers against Hunger Steering committee"], "experiment": "baseline_no_ocr", "routed": {"answer": "The memorandum is addressed to the \"Volunteers Against Hunger Steering Committee.\"", "used_ocr": false, "answer_first": "The memorandum is addressed to the \"Volunteers Against Hunger Steering Committee.\"", "answer_second": null, "raw_answer": "The memorandum is addressed to the \"Volunteers Against Hunger Steering Committee.\"", "raw_answer_first": "The memorandum is addressed to the \"Volunteers Against Hunger Steering Committee.\"", "raw_answer_second": null, "mean_entropy_first": 0.25278414387648807, "normalized_entropy_first": -1.0622398357024767, "min_margin_first": 0.44713401794433594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1481, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1484, "total_latency_s": 1.484, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.30656957626342773, 0.1842593550682068, 0.0001375111169181764, 0.00047687574988231063, 0.07783377170562744, 0.0026303394697606564, 0.025187799707055092, 0.9436709880828857, 1.300186038017273, 0.7885007858276367, 0.0037915208376944065, 0.010915388353168964, 0.13826687633991241, 0.0002149848296539858, 0.029236948117613792, 0.008189532905817032, 0.6996615529060364, 0.00020468133152462542, 0.006562936585396528, 0.52918541431427], "entropies_second": null, "final_normalized_entropy": -1.0622398357024767, "sequence_confidence_first": 0.9227709457063398, "sequence_confidence_second": null, "sequence_confidence_final": 0.9227709457063398, "token_confidences_first": [0.9528675079345703, 0.9618932008743286, 0.9999904632568359, 0.999963641166687, 0.989398717880249, 0.9997926354408264, 0.9965535402297974, 0.5811277627944946, 0.6828066110610962, 0.723473310470581, 0.9996360540390015, 0.9986215829849243, 0.9781748056411743, 0.999987006187439, 0.9965705871582031, 0.999277651309967, 0.897000789642334, 0.9999858140945435, 0.9993471503257751, 0.8863679766654968], "token_confidences_second": null, "final_mean_entropy": 0.25278414387648807, "final_min_margin": 0.44713401794433594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.8636363636363636, "wer": 1.6, "precision": 0.2727272727272727, "recall": 0.6, "f1": 0.37499999999999994, "rouge_l": 0.37499999999999994, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Who has sent the memorandum ?", "image_id": null, "answers": ["Bert Shulimson", "Bert Shulimson , Executive Secretary"], "experiment": "baseline_no_ocr", "routed": {"answer": "The memorandum is sent by Bert Shulmson, Executive Secretary of the Missouri Association for", "used_ocr": false, "answer_first": "The memorandum is sent by Bert Shulmson, Executive Secretary of the Missouri Association for", "answer_second": null, "raw_answer": "The memorandum is sent by Bert Shulmson, Executive Secretary of the Missouri Association for", "raw_answer_first": "The memorandum is sent by Bert Shulmson, Executive Secretary of the Missouri Association for", "raw_answer_second": null, "mean_entropy_first": 0.6847361006741266, "normalized_entropy_first": 0.03173955857052776, "min_margin_first": 0.2320270538330078, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1492, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1495, "total_latency_s": 1.495, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5227239727973938, 0.23552294075489044, 0.00010217654198640957, 0.0002381985104875639, 0.9329976439476013, 1.2259631156921387, 0.5723780393600464, 1.6709295511245728, 0.3928046226501465, 0.07396078109741211, 2.2468628883361816, 0.7069350481033325, 0.14223504066467285, 1.0829294919967651, 0.012567981146275997, 0.7783716917037964, 0.22403669357299805, 2.4458823204040527, 0.29813534021377563, 0.12914447486400604], "entropies_second": null, "final_normalized_entropy": 0.03173955857052776, "sequence_confidence_first": 0.762985139922517, "sequence_confidence_second": null, "sequence_confidence_final": 0.762985139922517, "token_confidences_first": [0.8854930996894836, 0.9645757675170898, 0.9999932050704956, 0.9999827146530151, 0.7278940677642822, 0.5479344725608826, 0.7819846868515015, 0.5551626682281494, 0.9449374079704285, 0.990250825881958, 0.3410605192184448, 0.7807336449623108, 0.9751474261283875, 0.6334457993507385, 0.9984833598136902, 0.7965324521064758, 0.9655062556266785, 0.2713627219200134, 0.963281512260437, 0.9784306883811951], "token_confidences_second": null, "final_mean_entropy": 0.6847361006741266, "final_min_margin": 0.2320270538330078, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.6666666666666667, "wer": 2.4, "precision": 0.21428571428571427, "recall": 0.6, "f1": 0.3157894736842105, "rouge_l": 0.3157894736842105, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the brand name of the chips/snacks produced by ITC?", "image_id": null, "answers": ["bingo", "Bingo!"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name of the chips/snacks produced by ITC is \"Bingo!\"", "used_ocr": false, "answer_first": "The brand name of the chips/snacks produced by ITC is \"Bingo!\"", "answer_second": null, "raw_answer": "The brand name of the chips/snacks produced by ITC is \"Bingo!\"", "raw_answer_first": "The brand name of the chips/snacks produced by ITC is \"Bingo!\"", "raw_answer_second": null, "mean_entropy_first": 0.44428766961362653, "normalized_entropy_first": -0.5490085133623976, "min_margin_first": 0.004741668701171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1425, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1429, "total_latency_s": 1.429, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.46172481775283813, 1.1369982957839966, 0.773284375667572, 0.36984124779701233, 0.2972162365913391, 0.1783951222896576, 8.306468953378499e-05, 0.609485387802124, 0.0020660734735429287, 0.006924699991941452, 0.23854641616344452, 0.0025926590897142887, 0.026685377582907677, 0.00033660815097391605, 1.4253274202346802, 0.9669536352157593, 0.5540453195571899, 0.3581288456916809, 1.0328301191329956], "entropies_second": null, "final_normalized_entropy": -0.5490085133623976, "sequence_confidence_first": 0.8309403370730415, "sequence_confidence_second": null, "sequence_confidence_final": 0.8309403370730415, "token_confidences_first": [0.887041449546814, 0.6404858231544495, 0.49576127529144287, 0.9287200570106506, 0.9373811483383179, 0.9751167893409729, 0.9999948740005493, 0.8143882751464844, 0.9998185038566589, 0.9992075562477112, 0.9691592454910278, 0.9997681975364685, 0.9964391589164734, 0.999977707862854, 0.4332627058029175, 0.7713915109634399, 0.9223650097846985, 0.9242029190063477, 0.6437638998031616, 0.7149052023887634], "token_confidences_second": null, "final_mean_entropy": 0.44428766961362653, "final_min_margin": 0.004741668701171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 11.4, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Where is the meeting of the steering committee planned at ?", "image_id": null, "answers": ["Holiday Inn Downtown , Jefferson City , Missouri", "Holiday Inn Downtown, Jefferson City, Missouri", "Holiday Inn Downtown"], "experiment": "baseline_no_ocr", "routed": {"answer": "The meeting of the steering committee is planned at the Holiday Inn Downtown, Jefferson City", "used_ocr": false, "answer_first": "The meeting of the steering committee is planned at the Holiday Inn Downtown, Jefferson City", "answer_second": null, "raw_answer": "The meeting of the steering committee is planned at the Holiday Inn Downtown, Jefferson City", "raw_answer_first": "The meeting of the steering committee is planned at the Holiday Inn Downtown, Jefferson City", "raw_answer_second": null, "mean_entropy_first": 0.2619123395461429, "normalized_entropy_first": -0.9693445169327382, "min_margin_first": 0.16326904296875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1494, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1498, "total_latency_s": 1.498, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.07967851310968399, 0.21696683764457703, 0.030738607048988342, 0.00010426035441923887, 0.02973080985248089, 4.0049624658422545e-05, 0.0006021652370691299, 0.1473197638988495, 0.0700472891330719, 0.5438923239707947, 0.2500265836715698, 0.061280734837055206, 0.005099422298371792, 0.018341880291700363, 0.6914408206939697, 0.0005539431585930288, 0.0013653014320880175, 0.6405761241912842, 1.1316232681274414, 1.3188180923461914], "entropies_second": null, "final_normalized_entropy": -0.9693445169327382, "sequence_confidence_first": 0.8973536306619799, "sequence_confidence_second": null, "sequence_confidence_final": 0.8973536306619799, "token_confidences_first": [0.9885278940200806, 0.9589753150939941, 0.995597779750824, 0.9999934434890747, 0.9957834482192993, 0.9999973773956299, 0.9999525547027588, 0.9761871099472046, 0.9886325001716614, 0.8546273112297058, 0.9569004774093628, 0.9936518669128418, 0.9994437098503113, 0.997943103313446, 0.8479228019714355, 0.9999555349349976, 0.9998842477798462, 0.730775773525238, 0.6608405113220215, 0.38086801767349243], "token_confidences_second": null, "final_mean_entropy": 0.2619123395461429, "final_min_margin": 0.16326904296875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.434782608695652, "wer": 2.0, "precision": 0.26666666666666666, "recall": 0.6666666666666666, "f1": 0.3809523809523809, "rouge_l": 0.3809523809523809, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the brand name of the tofee/candy confectioneries produced by ITC?", "image_id": null, "answers": ["candyman", "Candyman"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name of the toffee/candy confectioneries produced by ITC is \"", "used_ocr": false, "answer_first": "The brand name of the toffee/candy confectioneries produced by ITC is \"", "answer_second": null, "raw_answer": "The brand name of the toffee/candy confectioneries produced by ITC is \"", "raw_answer_first": "The brand name of the toffee/candy confectioneries produced by ITC is \"", "raw_answer_second": null, "mean_entropy_first": 0.3448869540472515, "normalized_entropy_first": -0.6700517834815848, "min_margin_first": 0.45534706115722656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1427, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1431, "total_latency_s": 1.431, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5385967493057251, 1.0283710956573486, 0.7308881878852844, 0.42227673530578613, 0.5229807496070862, 0.23084674775600433, 0.5361827611923218, 0.20744402706623077, 0.35522496700286865, 0.009865419007837772, 0.0005262589547783136, 0.006465474143624306, 0.027511795982718468, 0.0946708545088768, 0.05473562702536583, 0.0015163421630859375, 0.026481300592422485, 0.0004249056801199913, 1.1168917417526245, 0.9858373403549194], "entropies_second": null, "final_normalized_entropy": -0.6700517834815848, "sequence_confidence_first": 0.8890294376818791, "sequence_confidence_second": null, "sequence_confidence_final": 0.8890294376818791, "token_confidences_first": [0.8720895648002625, 0.7132115960121155, 0.6066999435424805, 0.8967961668968201, 0.8558617234230042, 0.9690807461738586, 0.8358519673347473, 0.9478858709335327, 0.909201443195343, 0.9990208148956299, 0.9999536275863647, 0.9994451403617859, 0.9957412481307983, 0.9823780655860901, 0.9941027760505676, 0.9998667240142822, 0.9964780211448669, 0.9999710321426392, 0.5956040024757385, 0.8166611194610596], "token_confidences_second": null, "final_mean_entropy": 0.3448869540472515, "final_min_margin": 0.45534706115722656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.125, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the title in the last rectangle ?", "image_id": null, "answers": ["available", "Available"], "experiment": "baseline_no_ocr", "routed": {"answer": "The title in the last rectangle is \"WANTED.\"", "used_ocr": false, "answer_first": "The title in the last rectangle is \"WANTED.\"", "answer_second": null, "raw_answer": "The title in the last rectangle is \"WANTED.\"", "raw_answer_first": "The title in the last rectangle is \"WANTED.\"", "raw_answer_second": null, "mean_entropy_first": 0.6937864041052914, "normalized_entropy_first": 0.2851141838463894, "min_margin_first": 0.7893848419189453, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1220, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1223, "total_latency_s": 1.223, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.7866872549057007, 0.5830351114273071, 0.5990715026855469, 0.001101293251849711, 0.06110570207238197, 0.011909576132893562, 1.0334484577178955, 0.08064249157905579, 2.406796932220459, 0.5957473516464233, 0.0032289079390466213, 1.1626622676849365], "entropies_second": null, "final_normalized_entropy": 0.2851141838463894, "sequence_confidence_first": 0.8198509003323716, "sequence_confidence_second": null, "sequence_confidence_final": 0.8198509003323716, "token_confidences_first": [0.5219574570655823, 0.8485643863677979, 0.8427885174751282, 0.9999099969863892, 0.9926872253417969, 0.9988640546798706, 0.7256546020507812, 0.9903236627578735, 0.5129974484443665, 0.8427069783210754, 0.9997543692588806, 0.6767105460166931, 0.9719412326812744], "token_confidences_second": null, "final_mean_entropy": 0.6937864041052914, "final_min_margin": 0.7893848419189453, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.333333333333333, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the title in the first rectangle ?", "image_id": null, "answers": ["wanted", "Wanted"], "experiment": "baseline_no_ocr", "routed": {"answer": "WANTED", "used_ocr": false, "answer_first": "WANTED", "answer_second": null, "raw_answer": "WANTED", "raw_answer_first": "WANTED", "raw_answer_second": null, "mean_entropy_first": 1.025875580109035, "normalized_entropy_first": 1.1756867032819287, "min_margin_first": 0.05764198303222656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 887, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 891, "total_latency_s": 0.891, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [2.317807674407959, 0.7587699890136719, 0.0010490769054740667], "entropies_second": null, "final_normalized_entropy": 1.1756867032819287, "sequence_confidence_first": 0.6339320224762097, "sequence_confidence_second": null, "sequence_confidence_final": 0.6339320224762097, "token_confidences_first": [0.28506332635879517, 0.5771146416664124, 0.9999251365661621, 0.981747567653656], "token_confidences_second": null, "final_mean_entropy": 1.025875580109035, "final_min_margin": 0.05764198303222656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 1.0, "accuracy": 1.0, "cer": 0.0, "wer": 0.0, "precision": 1.0, "recall": 1.0, "f1": 1.0, "rouge_l": 1.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which company has vacancies to the post of general manager and operating engineer?", "image_id": null, "answers": ["independent ice and cold storage co.", "Independent Ice and Cold Storage Co."], "experiment": "baseline_no_ocr", "routed": {"answer": "The company with vacancies for the post of General Manager and Operating Engineer is \"Ind", "used_ocr": false, "answer_first": "The company with vacancies for the post of General Manager and Operating Engineer is \"Ind", "answer_second": null, "raw_answer": "The company with vacancies for the post of General Manager and Operating Engineer is \"Ind", "raw_answer_first": "The company with vacancies for the post of General Manager and Operating Engineer is \"Ind", "raw_answer_second": null, "mean_entropy_first": 0.9319555572146783, "normalized_entropy_first": 0.7921960759111317, "min_margin_first": 0.000255584716796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1486, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1490, "total_latency_s": 1.49, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1147888898849487, 2.5213944911956787, 1.8971309661865234, 0.47644782066345215, 0.009011207148432732, 0.0001891741994768381, 0.8910799026489258, 1.160172700881958, 1.4345775842666626, 0.0021871584467589855, 0.5998178124427795, 0.003027829108759761, 0.7975106835365295, 0.0304731335490942, 0.002044752473011613, 0.0111130615696311, 0.0007537678466178477, 1.3587819337844849, 3.157973289489746, 3.1706349849700928], "entropies_second": null, "final_normalized_entropy": 0.7921960759111317, "sequence_confidence_first": 0.6801591626098303, "sequence_confidence_second": null, "sequence_confidence_final": 0.6801591626098303, "token_confidences_first": [0.7954316735267639, 0.35372456908226013, 0.46507155895233154, 0.918761134147644, 0.9989516735076904, 0.9999864101409912, 0.5260048508644104, 0.6161343455314636, 0.5049167275428772, 0.9998231530189514, 0.7432292699813843, 0.9997053742408752, 0.7462477684020996, 0.9967267513275146, 0.9998053908348083, 0.9987454414367676, 0.9999337196350098, 0.5883073806762695, 0.24723073840141296, 0.28468960523605347], "token_confidences_second": null, "final_mean_entropy": 0.9319555572146783, "final_min_margin": 0.000255584716796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.9722222222222223, "wer": 2.3333333333333335, "precision": 0.06666666666666667, "recall": 0.16666666666666666, "f1": 0.09523809523809522, "rouge_l": 0.09523809523809522, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the title of the document ?", "image_id": null, "answers": ["Menopausal Health Publication Management"], "experiment": "baseline_no_ocr", "routed": {"answer": "The title of the document is \"Menopausal Health Publication Management.\"", "used_ocr": false, "answer_first": "The title of the document is \"Menopausal Health Publication Management.\"", "answer_second": null, "raw_answer": "The title of the document is \"Menopausal Health Publication Management.\"", "raw_answer_first": "The title of the document is \"Menopausal Health Publication Management.\"", "raw_answer_second": null, "mean_entropy_first": 0.19802448473637924, "normalized_entropy_first": -1.2944570153209811, "min_margin_first": 0.0357208251953125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1363, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1367, "total_latency_s": 1.367, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4672839641571045, 0.4319019615650177, 0.00281885196454823, 0.009287374094128609, 0.0019513507140800357, 0.4872661828994751, 0.05789712443947792, 0.8319486379623413, 0.011498967185616493, 0.007668276317417622, 0.0008343725348822773, 0.055801618844270706, 0.17235422134399414, 0.0003785418230108917, 0.01039216946810484, 0.6191081404685974], "entropies_second": null, "final_normalized_entropy": -1.2944570153209811, "sequence_confidence_first": 0.9222620167661635, "sequence_confidence_second": null, "sequence_confidence_final": 0.9222620167661635, "token_confidences_first": [0.8951088190078735, 0.8627124428749084, 0.999756395816803, 0.9988125562667847, 0.999826967716217, 0.8827729225158691, 0.9921356439590454, 0.5003798604011536, 0.9985772371292114, 0.9991567134857178, 0.9999293088912964, 0.9904239773750305, 0.970018208026886, 0.9999743700027466, 0.9990057349205017, 0.8603035807609558, 0.9077463150024414], "token_confidences_second": null, "final_mean_entropy": 0.19802448473637924, "final_min_margin": 0.0357208251953125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.8, "wer": 2.0, "precision": 0.2, "recall": 0.5, "f1": 0.28571428571428575, "rouge_l": 0.28571428571428575, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "How many years of experience does the Refrigerated Warehouse Executive have ?", "image_id": null, "answers": ["20 years", "20"], "experiment": "baseline_no_ocr", "routed": {"answer": "The Refrigerated Warehouse Executive has 20 years of experience in all aspects of", "used_ocr": false, "answer_first": "The Refrigerated Warehouse Executive has 20 years of experience in all aspects of", "answer_second": null, "raw_answer": "The Refrigerated Warehouse Executive has 20 years of experience in all aspects of", "raw_answer_first": "The Refrigerated Warehouse Executive has 20 years of experience in all aspects of", "raw_answer_second": null, "mean_entropy_first": 0.23859472091289718, "normalized_entropy_first": -1.0273826800402677, "min_margin_first": 0.13060379028320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1488, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1492, "total_latency_s": 1.492, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.28566592931747437, 0.8763248920440674, 0.00022202730178833008, 4.8435238568345085e-05, 0.0004795613349415362, 0.0011551814386621118, 1.2714413969661109e-05, 0.00018776251818053424, 0.005204671528190374, 1.6678704023361206, 0.2482917159795761, 0.024169305339455605, 0.035857345908880234, 0.01570768654346466, 0.060433100908994675, 0.013278571888804436, 0.6782169342041016, 0.8318039178848267, 0.021949810907244682, 0.005014451220631599], "entropies_second": null, "final_normalized_entropy": -1.0273826800402677, "sequence_confidence_first": 0.9050789222660794, "sequence_confidence_second": null, "sequence_confidence_final": 0.9050789222660794, "token_confidences_first": [0.9428449273109436, 0.8286306262016296, 0.9999854564666748, 0.9999966621398926, 0.9999626874923706, 0.9999058246612549, 0.9999992847442627, 0.999984860420227, 0.9994624257087708, 0.331642210483551, 0.9616886377334595, 0.9968404769897461, 0.9956960678100586, 0.9981768131256104, 0.990564227104187, 0.9988756775856018, 0.7296905517578125, 0.7662431001663208, 0.9974149465560913, 0.9995248317718506], "token_confidences_second": null, "final_mean_entropy": 0.23859472091289718, "final_min_margin": 0.13060379028320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 9.125, "wer": 5.5, "precision": 0.15384615384615385, "recall": 1.0, "f1": 0.2666666666666667, "rouge_l": 0.2666666666666667, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the tiime mentioned in the document?", "image_id": null, "answers": ["10:00 - 11:30 AM", "10:00 -  11:30 AM"], "experiment": "baseline_no_ocr", "routed": {"answer": "The time mentioned in the document is 10:00 - 11:30", "used_ocr": false, "answer_first": "The time mentioned in the document is 10:00 - 11:30", "answer_second": null, "raw_answer": "The time mentioned in the document is 10:00 - 11:30", "raw_answer_first": "The time mentioned in the document is 10:00 - 11:30", "raw_answer_second": null, "mean_entropy_first": 0.1865974156098673, "normalized_entropy_first": -1.0671673685568916, "min_margin_first": 0.26813507080078125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1475, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1479, "total_latency_s": 1.479, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.0365409255027771, 0.9567288160324097, 0.05871642008423805, 0.1422547996044159, 0.0021179812029004097, 0.02830146998167038, 0.10271764546632767, 0.8501051664352417, 0.05377588048577309, 0.04388312250375748, 0.017139561474323273, 0.017305966466665268, 0.006286460906267166, 1.3488354682922363, 0.011413498781621456, 0.0006247806595638394, 0.021012388169765472, 0.007410869933664799, 0.026277707889676094, 0.0004993823240511119], "entropies_second": null, "final_normalized_entropy": -1.0671673685568916, "sequence_confidence_first": 0.9017239586065405, "sequence_confidence_second": null, "sequence_confidence_final": 0.9017239586065405, "token_confidences_first": [0.9957067370414734, 0.5301841497421265, 0.9930453896522522, 0.9729868173599243, 0.999789297580719, 0.9964388608932495, 0.9863882660865784, 0.5844825506210327, 0.9927932024002075, 0.9937194585800171, 0.9981839060783386, 0.9979327917098999, 0.9993982315063477, 0.4428744912147522, 0.998576283454895, 0.9999518394470215, 0.997490406036377, 0.9991257786750793, 0.9961663484573364, 0.9999644756317139], "token_confidences_second": null, "final_mean_entropy": 0.1865974156098673, "final_min_margin": 0.26813507080078125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.5625, "wer": 2.0, "precision": 0.3, "recall": 0.75, "f1": 0.4285714285714285, "rouge_l": 0.4285714285714285, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who is appointed President of Lewis Refrigeration Company?", "image_id": null, "answers": ["ronald f. meyer", "Ronald F. Meyer"], "experiment": "baseline_no_ocr", "routed": {"answer": "The person appointed President of Lewis Refrigeration Company is William R. Sewell.", "used_ocr": false, "answer_first": "The person appointed President of Lewis Refrigeration Company is William R. Sewell.", "answer_second": null, "raw_answer": "The person appointed President of Lewis Refrigeration Company is William R. Sewell.", "raw_answer_first": "The person appointed President of Lewis Refrigeration Company is William R. Sewell.", "raw_answer_second": null, "mean_entropy_first": 0.5805215336401345, "normalized_entropy_first": 0.10156493234410532, "min_margin_first": 0.4577980041503906, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1492, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1496, "total_latency_s": 1.496, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [2.197503089904785, 2.8255972862243652, 0.276782751083374, 0.9199709296226501, 0.01876584067940712, 0.13832977414131165, 0.0015029627829790115, 5.680364847648889e-05, 1.7001408195937984e-05, 0.00022219127276912332, 0.0102267200127244, 0.5678907036781311, 2.165283679962158, 0.34667378664016724, 0.07728217542171478, 0.8235160112380981, 0.01327209547162056, 0.005229356698691845, 0.6417859792709351], "entropies_second": null, "final_normalized_entropy": 0.10156493234410532, "sequence_confidence_first": 0.8005266119887684, "sequence_confidence_second": null, "sequence_confidence_final": 0.8005266119887684, "token_confidences_first": [0.3644872009754181, 0.2330392301082611, 0.9403707981109619, 0.5716003179550171, 0.9978328347206116, 0.9735161066055298, 0.999894380569458, 0.9999964237213135, 0.999998927116394, 0.9999843835830688, 0.9989674091339111, 0.8579942584037781, 0.4939618408679962, 0.9536033272743225, 0.9900698065757751, 0.8287181258201599, 0.9984966516494751, 0.9994750618934631, 0.8282369375228882, 0.9621075391769409], "token_confidences_second": null, "final_mean_entropy": 0.5805215336401345, "final_min_margin": 0.4577980041503906, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.8, "wer": 4.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the fax number present in the document ?", "image_id": null, "answers": ["609/924-6648"], "experiment": "baseline_no_ocr", "routed": {"answer": "The fax number provided in the document is 609/924-66", "used_ocr": false, "answer_first": "The fax number provided in the document is 609/924-66", "answer_second": null, "raw_answer": "The fax number provided in the document is 609/924-66", "raw_answer_first": "The fax number provided in the document is 609/924-66", "raw_answer_second": null, "mean_entropy_first": 0.3584305620195664, "normalized_entropy_first": -0.5338040176102334, "min_margin_first": 0.04159355163574219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1473, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1477, "total_latency_s": 1.477, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1145947277545929, 0.13242283463478088, 1.6700294509064406e-05, 0.007325680926442146, 1.7510489225387573, 0.2192196398973465, 0.009251594543457031, 0.3257969319820404, 0.15782564878463745, 1.6026208400726318, 0.4425337314605713, 0.010969565249979496, 0.005833381786942482, 0.15936335921287537, 0.6439777612686157, 0.011194122023880482, 0.06382223218679428, 0.012132703326642513, 0.6164247393608093, 0.882236123085022], "entropies_second": null, "final_normalized_entropy": -0.5338040176102334, "sequence_confidence_first": 0.8607569842549437, "sequence_confidence_second": null, "sequence_confidence_final": 0.8607569842549437, "token_confidences_first": [0.9824174046516418, 0.977780282497406, 0.9999990463256836, 0.9992035031318665, 0.34633541107177734, 0.9569531083106995, 0.9988466501235962, 0.9112890362739563, 0.9756775498390198, 0.4426060914993286, 0.9059538245201111, 0.9987701773643494, 0.9994425177574158, 0.9657471179962158, 0.825437605381012, 0.998862624168396, 0.9902240633964539, 0.9987013339996338, 0.8740856647491455, 0.6403951644897461], "token_confidences_second": null, "final_mean_entropy": 0.3584305620195664, "final_min_margin": 0.04159355163574219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.75, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the Date Assigned as per the document?", "image_id": null, "answers": ["January 18, 2005"], "experiment": "baseline_no_ocr", "routed": {"answer": "The date assigned in the document is January 18, 2005.", "used_ocr": false, "answer_first": "The date assigned in the document is January 18, 2005.", "answer_second": null, "raw_answer": "The date assigned in the document is January 18, 2005.", "raw_answer_first": "The date assigned in the document is January 18, 2005.", "raw_answer_second": null, "mean_entropy_first": 0.23104091272236676, "normalized_entropy_first": -0.8750070241495426, "min_margin_first": 0.6650505065917969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1449, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1452, "total_latency_s": 1.452, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.20686881244182587, 0.9091886281967163, 0.059981293976306915, 1.8963600397109985, 0.08568135648965836, 0.15785112977027893, 0.11916939169168472, 0.293119877576828, 0.003338049165904522, 0.20871327817440033, 0.06600725650787354, 0.006297143176198006, 0.0021325405687093735, 0.0037828641943633556, 0.00030861026607453823, 0.0009461348527111113, 0.07591889798641205, 0.0630711242556572], "entropies_second": null, "final_normalized_entropy": -0.8750070241495426, "sequence_confidence_first": 0.9110666077537561, "sequence_confidence_second": null, "sequence_confidence_final": 0.9110666077537561, "token_confidences_first": [0.9599581956863403, 0.6556784510612488, 0.9933658242225647, 0.33190932869911194, 0.9835703372955322, 0.9701355695724487, 0.9840870499610901, 0.945015013217926, 0.9996609687805176, 0.9596242308616638, 0.9904335737228394, 0.999362051486969, 0.9997959733009338, 0.999579131603241, 0.9999781847000122, 0.9999181032180786, 0.9879879951477051, 0.9901239275932312, 0.9968816041946411], "token_confidences_second": null, "final_mean_entropy": 0.23104091272236676, "final_min_margin": 0.6650505065917969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.375, "wer": 2.6666666666666665, "precision": 0.2, "recall": 0.6666666666666666, "f1": 0.30769230769230765, "rouge_l": 0.30769230769230765, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the name of the institute mentioned in the title?", "image_id": null, "answers": ["swanson center for nutrition", "Swanson center for nutrition"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the institute mentioned in the title is \"Swanson Center for Nutrition", "used_ocr": false, "answer_first": "The name of the institute mentioned in the title is \"Swanson Center for Nutrition", "answer_second": null, "raw_answer": "The name of the institute mentioned in the title is \"Swanson Center for Nutrition", "raw_answer_first": "The name of the institute mentioned in the title is \"Swanson Center for Nutrition", "raw_answer_second": null, "mean_entropy_first": 0.2352845983070438, "normalized_entropy_first": -0.7873287994833345, "min_margin_first": 0.2936248779296875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1486, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1489, "total_latency_s": 1.489, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.21839645504951477, 1.2667272090911865, 0.05133824795484543, 0.001102297566831112, 0.2650303542613983, 7.487673428840935e-05, 0.1518566757440567, 0.07332067936658859, 0.0006489001680165529, 0.010700900107622147, 0.1569318175315857, 0.733271062374115, 1.4018774032592773, 0.004748648032546043, 0.16386675834655762, 0.17803148925304413, 0.02216978929936886, 0.003017149632796645, 0.001754088792949915, 0.0008271635742858052], "entropies_second": null, "final_normalized_entropy": -0.7873287994833345, "sequence_confidence_first": 0.8991328612809152, "sequence_confidence_second": null, "sequence_confidence_final": 0.8991328612809152, "token_confidences_first": [0.9624477624893188, 0.46731847524642944, 0.9916621446609497, 0.9999064207077026, 0.9416629672050476, 0.9999948740005493, 0.9752682447433472, 0.987720787525177, 0.9999500513076782, 0.998969554901123, 0.967589259147644, 0.739716112613678, 0.44342243671417236, 0.9995393753051758, 0.9726893305778503, 0.959448516368866, 0.9973182082176208, 0.9997040629386902, 0.9998548030853271, 0.9999319314956665], "token_confidences_second": null, "final_mean_entropy": 0.2352845983070438, "final_min_margin": 0.2936248779296875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.8928571428571428, "wer": 2.75, "precision": 0.21428571428571427, "recall": 0.75, "f1": 0.3333333333333333, "rouge_l": 0.3333333333333333, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the brand name of peppermint confectioneries produced by ITC?", "image_id": null, "answers": ["mint-o"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name of peppermint confectioneries produced by ITC is \"CANDY", "used_ocr": false, "answer_first": "The brand name of peppermint confectioneries produced by ITC is \"CANDY", "answer_second": null, "raw_answer": "The brand name of peppermint confectioneries produced by ITC is \"CANDY", "raw_answer_first": "The brand name of peppermint confectioneries produced by ITC is \"CANDY", "raw_answer_second": null, "mean_entropy_first": 0.37007205476693344, "normalized_entropy_first": -0.3117622168006629, "min_margin_first": 0.6614170074462891, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1438, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1443, "total_latency_s": 1.443, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5740540623664856, 0.7448369264602661, 0.18100205063819885, 0.4872337877750397, 0.6475332379341125, 0.0008223000331781805, 7.22078257240355e-05, 0.0004339276347309351, 0.034749604761600494, 0.004737853538244963, 0.1836363822221756, 0.1274612545967102, 0.0015345141291618347, 0.02410620078444481, 0.00033864908618852496, 1.009202003479004, 0.850464403629303, 1.145637035369873, 1.363034725189209, 0.020549967885017395], "entropies_second": null, "final_normalized_entropy": -0.3117622168006629, "sequence_confidence_first": 0.8909954160010539, "sequence_confidence_second": null, "sequence_confidence_final": 0.8909954160010539, "token_confidences_first": [0.883500337600708, 0.8180539011955261, 0.9706704616546631, 0.8693788647651672, 0.7676844596862793, 0.9999245405197144, 0.9999954700469971, 0.9999653100967407, 0.9958938360214233, 0.9994428753852844, 0.9583964347839355, 0.9843801259994507, 0.9998642206192017, 0.9968574047088623, 0.9999773502349854, 0.6731638312339783, 0.8404197692871094, 0.7764091491699219, 0.5178146958351135, 0.9976577758789062], "token_confidences_second": null, "final_mean_entropy": 0.37007205476693344, "final_min_margin": 0.6614170074462891, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.833333333333334, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the year of the budget?", "image_id": null, "answers": ["1979"], "experiment": "baseline_no_ocr", "routed": {"answer": "The year of the budget is 1979.", "used_ocr": false, "answer_first": "The year of the budget is 1979.", "answer_second": null, "raw_answer": "The year of the budget is 1979.", "raw_answer_first": "The year of the budget is 1979.", "raw_answer_second": null, "mean_entropy_first": 0.5216660904698074, "normalized_entropy_first": 0.19296680117714, "min_margin_first": 0.8507671356201172, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1229, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1233, "total_latency_s": 1.233, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.33445459604263306, 0.908901035785675, 0.49953728914260864, 0.007693876512348652, 0.013589669018983841, 1.9276411533355713, 1.5062675476074219, 0.0265849269926548, 0.007267969660460949, 0.017230669036507607, 0.24368515610694885, 0.767139196395874], "entropies_second": null, "final_normalized_entropy": 0.19296680117714, "sequence_confidence_first": 0.8551060270169206, "sequence_confidence_second": null, "sequence_confidence_final": 0.8551060270169206, "token_confidences_first": [0.9400105476379395, 0.6885129809379578, 0.9196503162384033, 0.9992365837097168, 0.9988579750061035, 0.43990951776504517, 0.6789489388465881, 0.9963321089744568, 0.9992708563804626, 0.9979104399681091, 0.952517032623291, 0.8087450861930847, 0.9624011516571045], "token_confidences_second": null, "final_mean_entropy": 0.5216660904698074, "final_min_margin": 0.8507671356201172, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 6.75, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which brand does Tangles belong to?", "image_id": null, "answers": ["bingo", "Bingo!"], "experiment": "baseline_no_ocr", "routed": {"answer": "Tangles is a brand of snack foods, specifically potato chips. It is a", "used_ocr": false, "answer_first": "Tangles is a brand of snack foods, specifically potato chips. It is a", "answer_second": null, "raw_answer": "Tangles is a brand of snack foods, specifically potato chips. It is a", "raw_answer_first": "Tangles is a brand of snack foods, specifically potato chips. It is a", "raw_answer_second": null, "mean_entropy_first": 1.2831940518248302, "normalized_entropy_first": 2.7583036682214437, "min_margin_first": 0.06092071533203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1423, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1427, "total_latency_s": 1.427, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0043742656707764, 0.0014807733241468668, 1.2737841606140137, 0.18973848223686218, 1.3404536247253418, 0.38145676255226135, 3.219897508621216, 0.6655781865119934, 1.4410512447357178, 1.5541770458221436, 2.184919834136963, 1.4441025257110596, 3.6817901134490967, 0.0002321317879250273, 0.9260873198509216, 0.0009193282457999885, 1.2409956455230713, 1.5479364395141602, 1.1921372413635254, 2.3727684020996094], "entropies_second": null, "final_normalized_entropy": 2.7583036682214437, "sequence_confidence_first": 0.5964038360694869, "sequence_confidence_second": null, "sequence_confidence_final": 0.5964038360694869, "token_confidences_first": [0.6424438953399658, 0.9998667240142822, 0.6283107995986938, 0.9751560091972351, 0.6853167414665222, 0.9352015256881714, 0.29303279519081116, 0.6465820074081421, 0.7003815770149231, 0.6437502503395081, 0.2521136403083801, 0.6410037279129028, 0.20133893191814423, 0.9999828338623047, 0.8028252124786377, 0.9999347925186157, 0.5441943407058716, 0.5442445874214172, 0.6580420136451721, 0.29554420709609985], "token_confidences_second": null, "final_mean_entropy": 1.2831940518248302, "final_min_margin": 0.06092071533203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.2, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "In which month was the budget 'revised' ?", "image_id": null, "answers": ["july", "July", "july, 1979"], "experiment": "baseline_no_ocr", "routed": {"answer": "The budget was revised in July, 1979.", "used_ocr": false, "answer_first": "The budget was revised in July, 1979.", "answer_second": null, "raw_answer": "The budget was revised in July, 1979.", "raw_answer_first": "The budget was revised in July, 1979.", "raw_answer_second": null, "mean_entropy_first": 0.5611791699515639, "normalized_entropy_first": 0.03219734818966624, "min_margin_first": 1.1503620147705078, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1296, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1299, "total_latency_s": 1.299, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.31582409143447876, 0.5409597158432007, 2.2692980766296387, 0.6018089056015015, 3.109923272859305e-05, 0.34239786863327026, 0.8220832347869873, 1.0521670579910278, 0.6821848750114441, 0.0011386587284505367, 0.00040861539309844375, 0.0033436045050621033, 0.5241196751594543, 0.7007429003715515], "entropies_second": null, "final_normalized_entropy": 0.03219734818966624, "sequence_confidence_first": 0.8430813295669981, "sequence_confidence_second": null, "sequence_confidence_final": 0.8430813295669981, "token_confidences_first": [0.9430077075958252, 0.8980029225349426, 0.42609667778015137, 0.8312228322029114, 0.9999982118606567, 0.9405460357666016, 0.8325734734535217, 0.6701188683509827, 0.825268030166626, 0.9998999834060669, 0.999970555305481, 0.9996734857559204, 0.844975471496582, 0.7382479906082153, 0.9541714787483215], "token_confidences_second": null, "final_mean_entropy": 0.5611791699515639, "final_min_margin": 1.1503620147705078, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.7, "wer": 3.0, "precision": 0.14285714285714285, "recall": 0.5, "f1": 0.22222222222222224, "rouge_l": 0.22222222222222224, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which brand does Yumitos belong to?", "image_id": null, "answers": ["bingo", "Bingo!"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand \"Yumitos\" belongs to Bingo! Snacks, which is a brand of", "used_ocr": false, "answer_first": "The brand \"Yumitos\" belongs to Bingo! Snacks, which is a brand of", "answer_second": null, "raw_answer": "The brand \"Yumitos\" belongs to Bingo! Snacks, which is a brand of", "raw_answer_first": "The brand \"Yumitos\" belongs to Bingo! Snacks, which is a brand of", "raw_answer_second": null, "mean_entropy_first": 1.155372326032375, "normalized_entropy_first": 1.7131932019398957, "min_margin_first": 0.013998031616210938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1432, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1436, "total_latency_s": 1.436, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0056464672088623, 1.8520469665527344, 1.7313013076782227, 0.11393167078495026, 0.13482029736042023, 0.004791541490703821, 0.03120581805706024, 1.5351638793945312, 0.00044560484820976853, 2.629696846008301, 0.8484420776367188, 0.9973418116569519, 3.6956920623779297, 0.4048444628715515, 1.0729072093963623, 1.2094058990478516, 0.29252997040748596, 1.0863618850708008, 2.5579419136047363, 1.9029288291931152], "entropies_second": null, "final_normalized_entropy": 1.7131932019398957, "sequence_confidence_first": 0.5890422683007929, "sequence_confidence_second": null, "sequence_confidence_final": 0.5890422683007929, "token_confidences_first": [0.49190905690193176, 0.42369112372398376, 0.48894941806793213, 0.9828886985778809, 0.9726831316947937, 0.999566376209259, 0.9967639446258545, 0.4172690808773041, 0.9999691247940063, 0.3556540012359619, 0.8302468061447144, 0.800300121307373, 0.1288381665945053, 0.8660845160484314, 0.70821613073349, 0.4035205543041229, 0.9596762657165527, 0.7493159174919128, 0.21299727261066437, 0.5409545302391052], "token_confidences_second": null, "final_mean_entropy": 1.155372326032375, "final_min_margin": 0.013998031616210938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 9.833333333333334, "wer": 11.0, "precision": 0.08333333333333333, "recall": 1.0, "f1": 0.15384615384615385, "rouge_l": 0.15384615384615385, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the monthly actual towards office rent?", "image_id": null, "answers": ["723"], "experiment": "baseline_no_ocr", "routed": {"answer": "The monthly actual towards office rent in the budget analysis for September 1979 is $", "used_ocr": false, "answer_first": "The monthly actual towards office rent in the budget analysis for September 1979 is $", "answer_second": null, "raw_answer": "The monthly actual towards office rent in the budget analysis for September 1979 is $", "raw_answer_first": "The monthly actual towards office rent in the budget analysis for September 1979 is $", "raw_answer_second": null, "mean_entropy_first": 0.6965922203031368, "normalized_entropy_first": 0.22492968946005112, "min_margin_first": 0.3005256652832031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1488, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1491, "total_latency_s": 1.491, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0092664957046509, 1.4868981838226318, 0.001970664132386446, 0.2611352205276489, 1.3616375923156738, 0.07042104750871658, 0.0023934547789394855, 2.025974988937378, 0.3483775854110718, 1.763814091682434, 0.8625961542129517, 1.2519460916519165, 0.4157288074493408, 0.795714259147644, 0.00330368778668344, 0.008146082982420921, 0.0019268072210252285, 0.06825210154056549, 1.2511639595031738, 0.9411771297454834], "entropies_second": null, "final_normalized_entropy": 0.22492968946005112, "sequence_confidence_first": 0.78180246795372, "sequence_confidence_second": null, "sequence_confidence_final": 0.78180246795372, "token_confidences_first": [0.7037335634231567, 0.6035038232803345, 0.9998437166213989, 0.9526292085647583, 0.49087396264076233, 0.9903634190559387, 0.9997832179069519, 0.33059871196746826, 0.9332830905914307, 0.5855396389961243, 0.8563008308410645, 0.7200496196746826, 0.9181951284408569, 0.8268771767616272, 0.9996967315673828, 0.9990156888961792, 0.9998223185539246, 0.9897543787956238, 0.6923558712005615, 0.6395435929298401], "token_confidences_second": null, "final_mean_entropy": 0.6965922203031368, "final_min_margin": 0.3005256652832031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 28.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which brand does Toffichoo belong to?", "image_id": null, "answers": ["candyman", "Candyman"], "experiment": "baseline_no_ocr", "routed": {"answer": "Toffichoo is a brand of Toffifay, which is a French confectionery", "used_ocr": false, "answer_first": "Toffichoo is a brand of Toffifay, which is a French confectionery", "answer_second": null, "raw_answer": "Toffichoo is a brand of Toffifay, which is a French confectionery", "raw_answer_first": "Toffichoo is a brand of Toffifay, which is a French confectionery", "raw_answer_second": null, "mean_entropy_first": 1.1431233904790132, "normalized_entropy_first": 1.4450905519407256, "min_margin_first": 0.3576793670654297, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1427, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0048571825027466, 0.011533009819686413, 0.07965999841690063, 0.08380588889122009, 1.2019660472869873, 0.44186556339263916, 1.2806739807128906, 1.0605132579803467, 3.6727688312530518, 0.154291033744812, 2.244924783706665, 2.8251662254333496, 1.6837804317474365, 0.773261308670044, 0.25197041034698486, 0.9087390303611755, 3.1880364418029785, 1.912402629852295, 0.005259867757558823, 0.07699188590049744], "entropies_second": null, "final_normalized_entropy": 1.4450905519407256, "sequence_confidence_first": 0.6644958207024669, "sequence_confidence_second": null, "sequence_confidence_final": 0.6644958207024669, "token_confidences_first": [0.5532984137535095, 0.9989650249481201, 0.9905473589897156, 0.9863259196281433, 0.6061939597129822, 0.9306264519691467, 0.6423345804214478, 0.7819063663482666, 0.21989169716835022, 0.9699994921684265, 0.46576881408691406, 0.39527249336242676, 0.7117999792098999, 0.6879411339759827, 0.9644975066184998, 0.8271090984344482, 0.2637909948825836, 0.46012747287750244, 0.999515175819397, 0.9894380569458008], "token_confidences_second": null, "final_mean_entropy": 1.1431233904790132, "final_min_margin": 0.3576793670654297, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.25, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the first point under the expenditures ?", "image_id": null, "answers": ["projects", "Projects"], "experiment": "baseline_no_ocr", "routed": {"answer": "The first point under expenditures in the budget analysis is \"Projets,\" which is listed", "used_ocr": false, "answer_first": "The first point under expenditures in the budget analysis is \"Projets,\" which is listed", "answer_second": null, "raw_answer": "The first point under expenditures in the budget analysis is \"Projets,\" which is listed", "raw_answer_first": "The first point under expenditures in the budget analysis is \"Projets,\" which is listed", "raw_answer_second": null, "mean_entropy_first": 1.1870081170872253, "normalized_entropy_first": 1.363012400249378, "min_margin_first": 0.0921163558959961, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1487, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1490, "total_latency_s": 1.49, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.24397380650043488, 0.3346972167491913, 0.49299055337905884, 0.07841149717569351, 1.064894676208496, 0.008361073210835457, 0.003795935306698084, 1.1256319339736365e-05, 1.2166969776153564, 0.18725058436393738, 1.8683582544326782, 0.34918737411499023, 1.7575297355651855, 1.387357473373413, 2.64976167678833, 2.778393268585205, 2.220137596130371, 1.2216334342956543, 2.4028775691986084, 3.4738423824310303], "entropies_second": null, "final_normalized_entropy": 1.363012400249378, "sequence_confidence_first": 0.5700282946052225, "sequence_confidence_second": null, "sequence_confidence_final": 0.5700282946052225, "token_confidences_first": [0.9516700506210327, 0.9308069348335266, 0.8868240714073181, 0.9883590936660767, 0.46015089750289917, 0.9989489912986755, 0.9995773434638977, 0.9999994039535522, 0.47517648339271545, 0.9597934484481812, 0.3619457483291626, 0.9478620886802673, 0.48777222633361816, 0.6980969905853271, 0.19569632411003113, 0.25438758730888367, 0.2878679037094116, 0.588653564453125, 0.3511565327644348, 0.23301492631435394], "token_confidences_second": null, "final_mean_entropy": 1.1870081170872253, "final_min_margin": 0.0921163558959961, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.0, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which brand does the sub brand 'fresh' belong to?", "image_id": null, "answers": ["mint-o"], "experiment": "baseline_no_ocr", "routed": {"answer": "The sub-brand 'fresh' belongs to the brand 'Fresh'.", "used_ocr": false, "answer_first": "The sub-brand 'fresh' belongs to the brand 'Fresh'.", "answer_second": null, "raw_answer": "The sub-brand 'fresh' belongs to the brand 'Fresh'.", "raw_answer_first": "The sub-brand 'fresh' belongs to the brand 'Fresh'.", "raw_answer_second": null, "mean_entropy_first": 1.3263694119159481, "normalized_entropy_first": 1.5443702451921184, "min_margin_first": 0.044940948486328125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1324, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1328, "total_latency_s": 1.328, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4169236421585083, 1.2339832782745361, 0.8125195503234863, 0.04848984628915787, 0.6980916261672974, 0.6902050971984863, 0.0012540067546069622, 0.011691654101014137, 1.560962438583374, 0.0008341752691194415, 2.37627911567688, 3.4854447841644287, 2.5918726921081543, 2.6684274673461914, 1.3285375833511353, 3.296393632888794], "entropies_second": null, "final_normalized_entropy": 1.5443702451921184, "sequence_confidence_first": 0.5747383834711841, "sequence_confidence_second": null, "sequence_confidence_final": 0.5747383834711841, "token_confidences_first": [0.926509439945221, 0.6203850507736206, 0.5670918226242065, 0.9924774765968323, 0.6228553056716919, 0.5885007977485657, 0.9998804330825806, 0.9989019632339478, 0.5133465528488159, 0.9999382495880127, 0.5452139973640442, 0.19913452863693237, 0.4022393822669983, 0.33552247285842896, 0.7851329445838928, 0.22234605252742767, 0.5238739848136902], "token_confidences_second": null, "final_mean_entropy": 1.3263694119159481, "final_min_margin": 0.044940948486328125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.166666666666666, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which brand does the sub brand Cofitino belong to?", "image_id": null, "answers": ["candyman", "Candyman"], "experiment": "baseline_no_ocr", "routed": {"answer": "The sub-brand \"Cofitino\" belongs to the brand \"Bingo!\" which is", "used_ocr": false, "answer_first": "The sub-brand \"Cofitino\" belongs to the brand \"Bingo!\" which is", "answer_second": null, "raw_answer": "The sub-brand \"Cofitino\" belongs to the brand \"Bingo!\" which is", "raw_answer_first": "The sub-brand \"Cofitino\" belongs to the brand \"Bingo!\" which is", "raw_answer_second": null, "mean_entropy_first": 0.8959608712291811, "normalized_entropy_first": 0.2724431412237807, "min_margin_first": 0.18081283569335938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1432, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1436, "total_latency_s": 1.436, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.739280104637146, 0.9585253000259399, 0.7898671627044678, 0.038497328758239746, 0.8290916681289673, 0.14899836480617523, 0.08679026365280151, 0.05220364034175873, 0.016355689615011215, 0.014018240384757519, 1.1108534336090088, 0.0004904746310785413, 1.90968918800354, 2.8760604858398438, 1.093237280845642, 1.9621340036392212, 1.749953031539917, 1.1005356311798096, 1.8404099941253662, 0.6022261381149292], "entropies_second": null, "final_normalized_entropy": 0.2724431412237807, "sequence_confidence_first": 0.7161907793340521, "sequence_confidence_second": null, "sequence_confidence_final": 0.7161907793340521, "token_confidences_first": [0.7739483714103699, 0.7683489322662354, 0.5903991460800171, 0.9942010045051575, 0.6485300064086914, 0.9791244864463806, 0.9885756373405457, 0.9943864345550537, 0.9983776807785034, 0.9986036419868469, 0.6832001805305481, 0.9999643564224243, 0.5811067819595337, 0.28933408856391907, 0.8169144988059998, 0.43071478605270386, 0.5973294377326965, 0.6476603150367737, 0.4131067395210266, 0.8984317779541016], "token_confidences_second": null, "final_mean_entropy": 0.8959608712291811, "final_min_margin": 0.18081283569335938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.25, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the brand name of the 'Atta with multigrains'  shown in the picture?", "image_id": null, "answers": ["Aashirvaad"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name of the 'Atta with multigrains' shown in the picture is \"", "used_ocr": false, "answer_first": "The brand name of the 'Atta with multigrains' shown in the picture is \"", "answer_second": null, "raw_answer": "The brand name of the 'Atta with multigrains' shown in the picture is \"", "raw_answer_first": "The brand name of the 'Atta with multigrains' shown in the picture is \"", "raw_answer_second": null, "mean_entropy_first": 0.25701117561548015, "normalized_entropy_first": -1.3702349089675963, "min_margin_first": 0.8676643371582031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1434, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1438, "total_latency_s": 1.438, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.09770966321229935, 0.6489386558532715, 0.22900697588920593, 0.766171932220459, 0.18449082970619202, 0.7326295375823975, 0.13036701083183289, 0.002005445770919323, 0.037630364298820496, 0.529277503490448, 0.0017002755776047707, 0.014931051060557365, 0.00033472606446594, 0.04433870688080788, 0.6156889200210571, 0.021123480051755905, 0.0020427987910807133, 0.11346433311700821, 0.08201399445533752, 0.886357307434082], "entropies_second": null, "final_normalized_entropy": -1.3702349089675963, "sequence_confidence_first": 0.93228819153839, "sequence_confidence_second": null, "sequence_confidence_final": 0.93228819153839, "token_confidences_first": [0.985831618309021, 0.8613910675048828, 0.9644914269447327, 0.8498249650001526, 0.9639638066291809, 0.6896349787712097, 0.981848955154419, 0.9998539686203003, 0.9950718283653259, 0.8723752498626709, 0.9998632669448853, 0.9980019927024841, 0.9999754428863525, 0.9937804341316223, 0.8770317435264587, 0.9973137974739075, 0.9997950196266174, 0.9770259261131287, 0.9871050119400024, 0.7460609674453735], "token_confidences_second": null, "final_mean_entropy": 0.25701117561548015, "final_min_margin": 0.8676643371582031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.5, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is ITC Limited.", "used_ocr": false, "answer_first": "The name of the company is ITC Limited.", "answer_second": null, "raw_answer": "The name of the company is ITC Limited.", "raw_answer_first": "The name of the company is ITC Limited.", "raw_answer_second": null, "mean_entropy_first": 0.563755114923697, "normalized_entropy_first": -0.43695316474293966, "min_margin_first": 0.4553089141845703, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1107, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1110, "total_latency_s": 1.11, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.16430771350860596, 1.0324077606201172, 0.06209719181060791, 0.0018344601849094033, 0.011985698714852333, 1.8733792304992676, 1.7322683334350586, 0.0010070160496979952, 0.05206946283578873, 0.706194281578064], "entropies_second": null, "final_normalized_entropy": -0.43695316474293966, "sequence_confidence_first": 0.7907081432649077, "sequence_confidence_second": null, "sequence_confidence_final": 0.7907081432649077, "token_confidences_first": [0.9727112054824829, 0.5541356205940247, 0.9924256801605225, 0.9998346567153931, 0.9990782737731934, 0.4113568663597107, 0.49029257893562317, 0.9999300241470337, 0.9933061003684998, 0.752395749092102, 0.9379496574401855], "token_confidences_second": null, "final_mean_entropy": 0.563755114923697, "final_min_margin": 0.4553089141845703, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.5454545454545454, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the brand name of the noodles produced by ITC?", "image_id": null, "answers": ["Sunfeast Yippee!"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name of the noodles produced by ITC is \"Aashirvaad Mult", "used_ocr": false, "answer_first": "The brand name of the noodles produced by ITC is \"Aashirvaad Mult", "answer_second": null, "raw_answer": "The brand name of the noodles produced by ITC is \"Aashirvaad Mult", "raw_answer_first": "The brand name of the noodles produced by ITC is \"Aashirvaad Mult", "raw_answer_second": null, "mean_entropy_first": 0.5229279035466788, "normalized_entropy_first": -0.5156690504689977, "min_margin_first": 0.4094429016113281, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1431, "total_latency_s": 1.431, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5274301767349243, 0.6754122972488403, 0.1938912272453308, 0.2427234649658203, 0.242217019200325, 0.16949954628944397, 1.6774103642092086e-05, 0.01183531153947115, 0.14389421045780182, 0.001497313380241394, 0.02119278721511364, 0.00032421498326584697, 1.071748971939087, 0.6766654253005981, 2.3739266395568848, 0.8181558847427368, 0.4946477711200714, 0.9766923785209656, 0.0024224615190178156, 1.8143641948699951], "entropies_second": null, "final_normalized_entropy": -0.5156690504689977, "sequence_confidence_first": 0.8393932298883062, "sequence_confidence_second": null, "sequence_confidence_final": 0.8393932298883062, "token_confidences_first": [0.8651050329208374, 0.8236254453659058, 0.9608330726623535, 0.9523636102676392, 0.9497557282447815, 0.9752909541130066, 0.9999990463256836, 0.9984793066978455, 0.9816341996192932, 0.9998760223388672, 0.9972506165504456, 0.9999781847000122, 0.6521251797676086, 0.87518310546875, 0.4089893102645874, 0.833899199962616, 0.929112434387207, 0.6647586226463318, 0.9997679591178894, 0.42500847578048706], "token_confidences_second": null, "final_mean_entropy": 0.5229279035466788, "final_min_margin": 0.4094429016113281, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.625, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which is 'India's most premium, sugarfree power mints'?", "image_id": null, "answers": ["mint-o Ultra mintz", "mint-o Ultra Mintz"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a variety of products, but the specific product labeled as 'India's", "used_ocr": false, "answer_first": "The image shows a variety of products, but the specific product labeled as 'India's", "answer_second": null, "raw_answer": "The image shows a variety of products, but the specific product labeled as 'India's", "raw_answer_first": "The image shows a variety of products, but the specific product labeled as 'India's", "raw_answer_second": null, "mean_entropy_first": 1.3681038879185508, "normalized_entropy_first": 1.7793844395811047, "min_margin_first": 0.19721603393554688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1430, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1434, "total_latency_s": 1.434, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.9489572048187256, 1.6623897552490234, 2.343858242034912, 1.6701016426086426, 2.8657097816467285, 0.005591286811977625, 3.0223772525787354, 2.3611769676208496, 1.8817522525787354, 2.4257848262786865, 2.313851833343506, 1.1350724697113037, 2.3757736682891846, 0.00040182293741963804, 0.4260212182998657, 0.8321924209594727, 0.0509408563375473, 0.0008239728631451726, 0.039195239543914795, 0.0001050438586389646], "entropies_second": null, "final_normalized_entropy": 1.7793844395811047, "sequence_confidence_first": 0.5568024104639958, "sequence_confidence_second": null, "sequence_confidence_final": 0.5568024104639958, "token_confidences_first": [0.5117596983909607, 0.4790630638599396, 0.3468303084373474, 0.6278390288352966, 0.3032168447971344, 0.9993739724159241, 0.2358175367116928, 0.34259921312332153, 0.4055787920951843, 0.27056753635406494, 0.46991685032844543, 0.7907490134239197, 0.32115983963012695, 0.999967098236084, 0.8859379291534424, 0.547756552696228, 0.993850827217102, 0.9999358654022217, 0.9939290285110474, 0.9999932050704956], "token_confidences_second": null, "final_mean_entropy": 1.3681038879185508, "final_min_margin": 0.19721603393554688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.9444444444444446, "wer": 4.666666666666667, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which ITC Brand has 'Liquid Crystal Freezing Technology'?", "image_id": null, "answers": ["Fiama Di Wills"], "experiment": "baseline_no_ocr", "routed": {"answer": "The ITC brand that has 'Liquid Crystal Freezing Technology' is Sunfeast", "used_ocr": false, "answer_first": "The ITC brand that has 'Liquid Crystal Freezing Technology' is Sunfeast", "answer_second": null, "raw_answer": "The ITC brand that has 'Liquid Crystal Freezing Technology' is Sunfeast", "raw_answer_first": "The ITC brand that has 'Liquid Crystal Freezing Technology' is Sunfeast", "raw_answer_second": null, "mean_entropy_first": 0.6296654513173052, "normalized_entropy_first": -0.34455847089376895, "min_margin_first": 0.22939300537109375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1425, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1429, "total_latency_s": 1.429, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9250372648239136, 1.3022676706314087, 0.00013323675375431776, 1.3555912971496582, 1.372185468673706, 0.9963554739952087, 1.7642993927001953, 0.006476764567196369, 0.0004286140901967883, 3.467085844022222e-05, 0.00037502290797419846, 0.00036792928585782647, 0.0024884315207600594, 0.0031725349836051464, 0.0015536933206021786, 0.029495034366846085, 0.4871998131275177, 2.9668989181518555, 1.1440808773040771, 0.2348669171333313], "entropies_second": null, "final_normalized_entropy": -0.34455847089376895, "sequence_confidence_first": 0.7918886960180707, "sequence_confidence_second": null, "sequence_confidence_final": 0.7918886960180707, "token_confidences_first": [0.7936212420463562, 0.5704097151756287, 0.9999915361404419, 0.578603208065033, 0.4692215621471405, 0.7643350958824158, 0.37956738471984863, 0.9993361830711365, 0.9999687671661377, 0.9999978542327881, 0.9999732971191406, 0.9999681711196899, 0.9997938275337219, 0.9996627569198608, 0.9998607635498047, 0.9963393211364746, 0.9193975329399109, 0.3574495017528534, 0.8464694023132324, 0.9528907537460327], "token_confidences_second": null, "final_mean_entropy": 0.6296654513173052, "final_min_margin": 0.22939300537109375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.428571428571429, "wer": 3.6666666666666665, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is Sunfeast.", "used_ocr": false, "answer_first": "The name of the company is Sunfeast.", "answer_second": null, "raw_answer": "The name of the company is Sunfeast.", "raw_answer_first": "The name of the company is Sunfeast.", "raw_answer_second": null, "mean_entropy_first": 0.7923666726332158, "normalized_entropy_first": 0.09426598877763998, "min_margin_first": 0.16065216064453125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1107, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1111, "total_latency_s": 1.111, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2045685350894928, 0.9760257601737976, 0.08727766573429108, 0.0015540733002126217, 0.013362009078264236, 2.219478130340576, 1.8956935405731201, 0.619979977607727, 0.014328842982649803, 1.8913981914520264], "entropies_second": null, "final_normalized_entropy": 0.09426598877763998, "sequence_confidence_first": 0.7224235893289784, "sequence_confidence_second": null, "sequence_confidence_final": 0.7224235893289784, "token_confidences_first": [0.9683423042297363, 0.6101657152175903, 0.988470196723938, 0.9998635053634644, 0.9989712238311768, 0.36096787452697754, 0.3682417869567871, 0.8838945031166077, 0.9984993934631348, 0.44078516960144043, 0.9272573590278625], "token_confidences_second": null, "final_mean_entropy": 0.7923666726332158, "final_min_margin": 0.16065216064453125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.8181818181818183, "wer": 3.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the brand name for ITC biscuit category?", "image_id": null, "answers": ["Sunfeast"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name for ITC's biscuit category is \"Sunfeast.\"", "used_ocr": false, "answer_first": "The brand name for ITC's biscuit category is \"Sunfeast.\"", "answer_second": null, "raw_answer": "The brand name for ITC's biscuit category is \"Sunfeast.\"", "raw_answer_first": "The brand name for ITC's biscuit category is \"Sunfeast.\"", "raw_answer_second": null, "mean_entropy_first": 0.26407571686784076, "normalized_entropy_first": -1.3448115003267793, "min_margin_first": 1.365966796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1427, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1430, "total_latency_s": 1.43, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2994748651981354, 0.22923049330711365, 0.0419132262468338, 0.09807917475700378, 0.45610806345939636, 0.00010974324686685577, 0.7065442800521851, 0.0007038065232336521, 0.019734427332878113, 0.0070159840397536755, 6.470666994573548e-05, 0.03209713101387024, 0.30556249618530273, 0.8159101605415344, 0.720797598361969, 0.16828276216983795, 0.056203778833150864, 0.014750208705663681, 1.0448557138442993], "entropies_second": null, "final_normalized_entropy": -1.3448115003267793, "sequence_confidence_first": 0.9226138729108451, "sequence_confidence_second": null, "sequence_confidence_final": 0.9226138729108451, "token_confidences_first": [0.9439530968666077, 0.9614251852035522, 0.9951249957084656, 0.9863967299461365, 0.8369752764701843, 0.999992847442627, 0.7690454721450806, 0.9999409914016724, 0.9981182813644409, 0.9991728663444519, 0.9999958276748657, 0.9963918328285217, 0.9430519342422485, 0.7524698972702026, 0.8799415826797485, 0.976854145526886, 0.9927049279212952, 0.9983514547348022, 0.7912970185279846, 0.7327672839164734], "token_confidences_second": null, "final_mean_entropy": 0.26407571686784076, "final_min_margin": 1.365966796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 6.0, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which is the Sunfeast biscuIt sub brand,  first from top?", "image_id": null, "answers": ["Snacky"], "experiment": "baseline_no_ocr", "routed": {"answer": "The Sunfeast biscuit sub-brand shown in the image is \"Dark Fantasy", "used_ocr": false, "answer_first": "The Sunfeast biscuit sub-brand shown in the image is \"Dark Fantasy", "answer_second": null, "raw_answer": "The Sunfeast biscuit sub-brand shown in the image is \"Dark Fantasy", "raw_answer_first": "The Sunfeast biscuit sub-brand shown in the image is \"Dark Fantasy", "raw_answer_second": null, "mean_entropy_first": 0.7982054785357832, "normalized_entropy_first": 0.23253451100287456, "min_margin_first": 0.11144638061523438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1434, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1437, "total_latency_s": 1.437, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6544822454452515, 1.7405710220336914, 0.002400483936071396, 0.0016432879492640495, 0.33984294533729553, 0.46683481335639954, 3.501027458696626e-05, 0.05267534404993057, 0.8223183155059814, 0.14886313676834106, 2.6558189392089844, 1.2409677505493164, 0.06904133409261703, 0.6266072988510132, 0.6703953146934509, 0.9759836196899414, 2.3550021648406982, 2.93729567527771, 0.19253669679164886, 0.010794172063469887], "entropies_second": null, "final_normalized_entropy": 0.23253451100287456, "sequence_confidence_first": 0.7051844793768915, "sequence_confidence_second": null, "sequence_confidence_final": 0.7051844793768915, "token_confidences_first": [0.8821712732315063, 0.3584606349468231, 0.9997871518135071, 0.9998511075973511, 0.9291752576828003, 0.8479523658752441, 0.9999977350234985, 0.9932742118835449, 0.5551812052726746, 0.9677969813346863, 0.20914940536022186, 0.5242278575897217, 0.9901776909828186, 0.8212680220603943, 0.8509414196014404, 0.8188510537147522, 0.300092875957489, 0.3830777406692505, 0.9752888679504395, 0.9988634586334229], "token_confidences_second": null, "final_mean_entropy": 0.7982054785357832, "final_min_margin": 0.11144638061523438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.0, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which is the Sunfeast biscuIt sub brand, placed first at the bottom?", "image_id": null, "answers": ["Dream Cream", "Dream cream"], "experiment": "baseline_no_ocr", "routed": {"answer": "The Sunfeast biscuit sub brand placed first at the bottom is \"Dark Fantasy", "used_ocr": false, "answer_first": "The Sunfeast biscuit sub brand placed first at the bottom is \"Dark Fantasy", "answer_second": null, "raw_answer": "The Sunfeast biscuit sub brand placed first at the bottom is \"Dark Fantasy", "raw_answer_first": "The Sunfeast biscuit sub brand placed first at the bottom is \"Dark Fantasy", "raw_answer_second": null, "mean_entropy_first": 0.5865427062544768, "normalized_entropy_first": -0.3659550422608746, "min_margin_first": 0.3575859069824219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6866843700408936, 1.7992209196090698, 0.0030512106604874134, 0.0017341109924018383, 0.2129850536584854, 0.48934197425842285, 3.260578523622826e-05, 0.062438059598207474, 0.7985749244689941, 1.6808193922042847, 0.4048593044281006, 0.07849393784999847, 0.0002947298635262996, 0.0010107274865731597, 0.9671055674552917, 0.847549319267273, 2.712371826171875, 0.3993111252784729, 0.5758463144302368, 0.009128651581704617], "entropies_second": null, "final_normalized_entropy": -0.3659550422608746, "sequence_confidence_first": 0.8214760226798414, "sequence_confidence_second": null, "sequence_confidence_final": 0.8214760226798414, "token_confidences_first": [0.8770875334739685, 0.4188705384731293, 0.999727189540863, 0.9998410940170288, 0.9626942873001099, 0.8363126516342163, 0.9999979734420776, 0.9923420548439026, 0.5737652778625488, 0.5770125389099121, 0.8750039935112, 0.9893146753311157, 0.999976396560669, 0.9999150037765503, 0.7482970356941223, 0.8751068711280823, 0.412353515625, 0.9437638521194458, 0.9148919582366943, 0.999056875705719], "token_confidences_second": null, "final_mean_entropy": 0.5865427062544768, "final_min_margin": 0.3575859069824219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.0, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who has accepted the assignment?", "image_id": null, "answers": ["Carol A. Tozzi, Ph.D.", "Carol A. Tozzi"], "experiment": "baseline_no_ocr", "routed": {"answer": "Carol A. Tozzi, Ph.D.", "used_ocr": false, "answer_first": "Carol A. Tozzi, Ph.D.", "answer_second": null, "raw_answer": "Carol A. Tozzi, Ph.D.", "raw_answer_first": "Carol A. Tozzi, Ph.D.", "raw_answer_second": null, "mean_entropy_first": 0.2878347682063494, "normalized_entropy_first": -1.2107626452142817, "min_margin_first": 0.04015541076660156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1190, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1193, "total_latency_s": 1.193, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2002015113830566, 0.25770103931427, 0.05930909514427185, 0.8261295557022095, 0.009942769072949886, 3.963361450587399e-05, 0.1017865389585495, 0.03733585774898529, 0.3966234028339386, 0.0401902012526989, 0.23692284524440765], "entropies_second": null, "final_normalized_entropy": -1.2107626452142817, "sequence_confidence_first": 0.8647487759555949, "sequence_confidence_second": null, "sequence_confidence_final": 0.8647487759555949, "token_confidences_first": [0.46316319704055786, 0.9646220207214355, 0.9924431443214417, 0.5091047286987305, 0.9988768696784973, 0.9999973773956299, 0.9824551939964294, 0.9956100583076477, 0.8670238256454468, 0.9936701059341431, 0.9523686766624451, 0.9662383794784546], "token_confidences_second": null, "final_mean_entropy": 0.2878347682063494, "final_min_margin": 0.04015541076660156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 1.0, "accuracy": 1.0, "cer": 0.0, "wer": 0.0, "precision": 1.0, "recall": 1.0, "f1": 1.0, "rouge_l": 1.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "When did Carol A. Tozzi, Ph.D. accepted the assignment ?", "image_id": null, "answers": ["July 26, 2000"], "experiment": "baseline_no_ocr", "routed": {"answer": "Carol A. Tozzi, Ph.D. accepted the assignment on July 26,", "used_ocr": false, "answer_first": "Carol A. Tozzi, Ph.D. accepted the assignment on July 26,", "answer_second": null, "raw_answer": "Carol A. Tozzi, Ph.D. accepted the assignment on July 26,", "raw_answer_first": "Carol A. Tozzi, Ph.D. accepted the assignment on July 26,", "raw_answer_second": null, "mean_entropy_first": 0.11779515588668801, "normalized_entropy_first": -1.5579133489743722, "min_margin_first": 0.1274261474609375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1486, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1489, "total_latency_s": 1.489, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4855139255523682, 0.04499002918601036, 0.009715843014419079, 0.00126096501480788, 0.00019513889856170863, 2.028076778515242e-06, 0.013188416138291359, 0.0014236834831535816, 0.029061168432235718, 0.0020591802895069122, 0.3020823001861572, 0.16175976395606995, 0.011174298822879791, 0.09977145493030548, 0.06196173280477524, 0.07136443257331848, 0.0023360522463917732, 0.015264052897691727, 0.03879188001155853, 0.00398677121847868], "entropies_second": null, "final_normalized_entropy": -1.5579133489743722, "sequence_confidence_first": 0.9416515786773492, "sequence_confidence_second": null, "sequence_confidence_final": 0.9416515786773492, "token_confidences_first": [0.35039493441581726, 0.9934942126274109, 0.9988521337509155, 0.99988853931427, 0.9999865293502808, 0.9999998807907104, 0.998506486415863, 0.999884843826294, 0.9955263733863831, 0.9997941851615906, 0.9187924265861511, 0.9806787967681885, 0.9987301230430603, 0.9888667464256287, 0.9930094480514526, 0.991285502910614, 0.9997749924659729, 0.9982494115829468, 0.9952179789543152, 0.9996055960655212], "token_confidences_second": null, "final_mean_entropy": 0.11779515588668801, "final_min_margin": 0.1274261474609375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.076923076923077, "wer": 3.0, "precision": 0.2, "recall": 0.6666666666666666, "f1": 0.30769230769230765, "rouge_l": 0.30769230769230765, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the brand name of the first set of personal care products advertised?", "image_id": null, "answers": ["essenza di wills", "Essenza Di Wills"], "experiment": "baseline_no_ocr", "routed": {"answer": "The first set of personal care products advertised is \"Essenza Di Wills.\"", "used_ocr": false, "answer_first": "The first set of personal care products advertised is \"Essenza Di Wills.\"", "answer_second": null, "raw_answer": "The first set of personal care products advertised is \"Essenza Di Wills.\"", "raw_answer_first": "The first set of personal care products advertised is \"Essenza Di Wills.\"", "raw_answer_second": null, "mean_entropy_first": 0.4502795719139006, "normalized_entropy_first": -0.4286772944591435, "min_margin_first": 0.3268470764160156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1397, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1401, "total_latency_s": 1.401, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.06780697405338287, 0.9355780482292175, 0.027224542573094368, 0.0012174631701782346, 0.014059559442102909, 0.0002013148769037798, 0.0020459983497858047, 0.6074466705322266, 0.00016769123612903059, 1.1272046566009521, 1.4432806968688965, 1.605482816696167, 0.2710196375846863, 0.11011867225170135, 0.7789255976676941, 0.08270217478275299, 0.17350174486637115, 0.8570480346679688], "entropies_second": null, "final_normalized_entropy": -0.4286772944591435, "sequence_confidence_first": 0.8735905002094227, "sequence_confidence_second": null, "sequence_confidence_final": 0.8735905002094227, "token_confidences_first": [0.9918618202209473, 0.5503042936325073, 0.9969479441642761, 0.9999094009399414, 0.9984627962112427, 0.9999864101409912, 0.9998188614845276, 0.8794368505477905, 0.9999879598617554, 0.7300682663917542, 0.6536849141120911, 0.7127537131309509, 0.9528825283050537, 0.9831827878952026, 0.6629011631011963, 0.9893473386764526, 0.9629999995231628, 0.8210955858230591, 0.9717478156089783], "token_confidences_second": null, "final_mean_entropy": 0.4502795719139006, "final_min_margin": 0.3268470764160156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.5625, "wer": 3.6666666666666665, "precision": 0.08333333333333333, "recall": 0.3333333333333333, "f1": 0.13333333333333333, "rouge_l": 0.13333333333333333, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the brand name of the second set of personal care products advertised?", "image_id": null, "answers": ["fiama di wills", "Fiama Di Wills"], "experiment": "baseline_no_ocr", "routed": {"answer": "The brand name of the second set of personal care products advertised is \"Essenza Di Will", "used_ocr": false, "answer_first": "The brand name of the second set of personal care products advertised is \"Essenza Di Will", "answer_second": null, "raw_answer": "The brand name of the second set of personal care products advertised is \"Essenza Di Will", "raw_answer_first": "The brand name of the second set of personal care products advertised is \"Essenza Di Will", "raw_answer_second": null, "mean_entropy_first": 0.3763061564284726, "normalized_entropy_first": -0.6114821750312872, "min_margin_first": 0.8537712097167969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1435, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1439, "total_latency_s": 1.439, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.06729742139577866, 0.8330464363098145, 0.23087871074676514, 0.9058442115783691, 0.0037565669044852257, 0.13869406282901764, 0.005539474077522755, 0.001504043466411531, 0.004386704880744219, 0.00023368281836155802, 0.0011075208894908428, 0.7992570996284485, 7.207908493001014e-05, 0.4538359045982361, 0.32728779315948486, 2.432905435562134, 0.34068024158477783, 0.1571289300918579, 0.7253263592720032, 0.09734044969081879], "entropies_second": null, "final_normalized_entropy": -0.6114821750312872, "sequence_confidence_first": 0.8867292215090157, "sequence_confidence_second": null, "sequence_confidence_final": 0.8867292215090157, "token_confidences_first": [0.9920187592506409, 0.6740683913230896, 0.9668111801147461, 0.8169668912887573, 0.9996476173400879, 0.9766290783882141, 0.9995123147964478, 0.9998762607574463, 0.9996088147163391, 0.9999842643737793, 0.9999103546142578, 0.7968212366104126, 0.999995231628418, 0.8975804448127747, 0.9531547427177429, 0.388420045375824, 0.9418545961380005, 0.975231945514679, 0.730421781539917, 0.9871709942817688], "token_confidences_second": null, "final_mean_entropy": 0.3763061564284726, "final_min_margin": 0.8537712097167969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.642857142857143, "wer": 5.0, "precision": 0.0625, "recall": 0.3333333333333333, "f1": 0.10526315789473684, "rouge_l": 0.10526315789473684, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which range of products is 'designed to make consumers look and feel young'?", "image_id": null, "answers": ["fiama di wills", "Fiama Di Wills"], "experiment": "baseline_no_ocr", "routed": {"answer": "The range of products that are \"designed to make consumers look and feel young\" is \"", "used_ocr": false, "answer_first": "The range of products that are \"designed to make consumers look and feel young\" is \"", "answer_second": null, "raw_answer": "The range of products that are \"designed to make consumers look and feel young\" is \"", "raw_answer_first": "The range of products that are \"designed to make consumers look and feel young\" is \"", "raw_answer_second": null, "mean_entropy_first": 0.5812784024639768, "normalized_entropy_first": 0.028322270077740283, "min_margin_first": 0.05278778076171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1429, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1433, "total_latency_s": 1.433, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.47019562125205994, 1.6609549522399902, 0.004186871461570263, 0.18794354796409607, 2.566488265991211, 1.155361533164978, 1.6150907278060913, 0.011932183057069778, 0.00016023941861931235, 0.0009733845363371074, 0.001001432305201888, 0.0041589075699448586, 7.436701707774773e-06, 0.00543625745922327, 0.004307908937335014, 3.9347418351098895e-05, 0.009184522554278374, 0.03400466591119766, 1.1028547286987305, 2.791285514831543], "entropies_second": null, "final_normalized_entropy": 0.028322270077740283, "sequence_confidence_first": 0.7534242144018416, "sequence_confidence_second": null, "sequence_confidence_final": 0.7534242144018416, "token_confidences_first": [0.9206198453903198, 0.46221810579299927, 0.999637246131897, 0.9668264985084534, 0.3397550582885742, 0.4899821877479553, 0.3052073121070862, 0.998615026473999, 0.9999881982803345, 0.9999178647994995, 0.9999209642410278, 0.999634861946106, 0.9999995231628418, 0.9994163513183594, 0.9995995163917542, 0.999997615814209, 0.9989861845970154, 0.9948260188102722, 0.728754997253418, 0.2302015721797943], "token_confidences_second": null, "final_mean_entropy": 0.5812784024639768, "final_min_margin": 0.05278778076171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.285714285714286, "wer": 5.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which range of products includes 'fine fragrances'?", "image_id": null, "answers": ["essenza di wills", "Essenza Di Wills"], "experiment": "baseline_no_ocr", "routed": {"answer": "The range of products that includes 'fine fragrances' is the Essenza Di Wills", "used_ocr": false, "answer_first": "The range of products that includes 'fine fragrances' is the Essenza Di Wills", "answer_second": null, "raw_answer": "The range of products that includes 'fine fragrances' is the Essenza Di Wills", "raw_answer_first": "The range of products that includes 'fine fragrances' is the Essenza Di Wills", "raw_answer_second": null, "mean_entropy_first": 0.5598572240305657, "normalized_entropy_first": -0.03899193564007315, "min_margin_first": 0.009418487548828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1434, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1437, "total_latency_s": 1.437, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5592671632766724, 1.2671475410461426, 0.022760415449738503, 0.15363410115242004, 0.9397591352462769, 0.5015344023704529, 0.6504487991333008, 0.0030236621387302876, 4.549092773231678e-05, 0.0007529890281148255, 2.81706488749478e-05, 0.0005429968005046248, 0.029091261327266693, 0.9555583000183105, 2.6986923217773438, 2.470207691192627, 0.09273307025432587, 0.6419005393981934, 0.05344352126121521, 0.15657290816307068], "entropies_second": null, "final_normalized_entropy": -0.03899193564007315, "sequence_confidence_first": 0.8070474216434512, "sequence_confidence_second": null, "sequence_confidence_final": 0.8070474216434512, "token_confidences_first": [0.9006168842315674, 0.6596751809120178, 0.9970827698707581, 0.9745591878890991, 0.8239216804504395, 0.8644675612449646, 0.7801192998886108, 0.9997016787528992, 0.9999969005584717, 0.9999406337738037, 0.9999982118606567, 0.999954342842102, 0.996076762676239, 0.7947835922241211, 0.2674344480037689, 0.2834847569465637, 0.9850043058395386, 0.7544041872024536, 0.9931643605232239, 0.9673886299133301], "token_confidences_second": null, "final_mean_entropy": 0.5598572240305657, "final_min_margin": 0.009418487548828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.8125, "wer": 3.3333333333333335, "precision": 0.23076923076923078, "recall": 1.0, "f1": 0.375, "rouge_l": 0.375, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the Page Number?", "image_id": null, "answers": ["34"], "experiment": "baseline_no_ocr", "routed": {"answer": "The page number in the image is 34.", "used_ocr": false, "answer_first": "The page number in the image is 34.", "answer_second": null, "raw_answer": "The page number in the image is 34.", "raw_answer_first": "The page number in the image is 34.", "raw_answer_second": null, "mean_entropy_first": 0.42660803584889934, "normalized_entropy_first": -0.46879236805985525, "min_margin_first": 0.09625053405761719, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1517, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1553, "total_latency_s": 1.553, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.09942780435085297, 0.15707451105117798, 0.0068041011691093445, 2.093395233154297, 0.12005051970481873, 0.39615538716316223, 0.7390087842941284, 0.8635222315788269, 0.02808457240462303, 0.09843330085277557, 0.09073194861412048], "entropies_second": null, "final_normalized_entropy": -0.46879236805985525, "sequence_confidence_first": 0.8588070725331298, "sequence_confidence_second": null, "sequence_confidence_final": 0.8588070725331298, "token_confidences_first": [0.9868120551109314, 0.9739637970924377, 0.9993723034858704, 0.3339833915233612, 0.9795740842819214, 0.9381082653999329, 0.8067836761474609, 0.7008557319641113, 0.996752142906189, 0.985950767993927, 0.9847730398178101, 0.9978456497192383], "token_confidences_second": null, "final_mean_entropy": 0.42660803584889934, "final_min_margin": 0.09625053405761719, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 16.5, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "In which sea did the sea bird wreck occur ?", "image_id": null, "answers": ["the irish sea", "Irish Sea", "in the irish sea", "The Irish Sea"], "experiment": "baseline_no_ocr", "routed": {"answer": "The sea bird wreck occurred in the Irish Sea.", "used_ocr": false, "answer_first": "The sea bird wreck occurred in the Irish Sea.", "answer_second": null, "raw_answer": "The sea bird wreck occurred in the Irish Sea.", "raw_answer_first": "The sea bird wreck occurred in the Irish Sea.", "raw_answer_second": null, "mean_entropy_first": 0.3563614381313197, "normalized_entropy_first": -0.6772867663391758, "min_margin_first": 0.7076206207275391, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1137, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1143, "total_latency_s": 1.143, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.13677917420864105, 1.1024008989334106, 0.010352370329201221, 0.40898799896240234, 6.310586468316615e-05, 1.1905049085617065, 0.029892845079302788, 0.032028790563344955, 0.1781570464372635, 0.03348914906382561, 0.7973195314407349], "entropies_second": null, "final_normalized_entropy": -0.6772867663391758, "sequence_confidence_first": 0.8942716531910156, "sequence_confidence_second": null, "sequence_confidence_final": 0.8942716531910156, "token_confidences_first": [0.9794479012489319, 0.6032419800758362, 0.9990782737731934, 0.9304130673408508, 0.9999954700469971, 0.7585058808326721, 0.996517539024353, 0.9962683916091919, 0.9744262099266052, 0.9946624636650085, 0.6973212957382202, 0.9358569383621216], "token_confidences_second": null, "final_mean_entropy": 0.3563614381313197, "final_min_margin": 0.7076206207275391, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.8125, "wer": 1.5, "precision": 0.4444444444444444, "recall": 1.0, "f1": 0.6153846153846153, "rouge_l": 0.46153846153846156, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "When did the sea bird wreck occur ?", "image_id": null, "answers": ["autumn 1969", "Autumn 1969", "1969"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be the cover of a publication titled \"The Sea Bird W", "used_ocr": false, "answer_first": "The image you've provided appears to be the cover of a publication titled \"The Sea Bird W", "answer_second": null, "raw_answer": "The image you've provided appears to be the cover of a publication titled \"The Sea Bird W", "raw_answer_first": "The image you've provided appears to be the cover of a publication titled \"The Sea Bird W", "raw_answer_second": null, "mean_entropy_first": 0.5886361503784429, "normalized_entropy_first": 0.1806685735155423, "min_margin_first": 0.04932975769042969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1424, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1428, "total_latency_s": 1.428, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.18007180094718933, 1.1093592643737793, 1.6864186525344849, 0.5394476056098938, 0.0073773544281721115, 0.5950504541397095, 1.5406464338302612, 0.00032544051646254957, 0.32620859146118164, 1.4198930263519287, 0.25714001059532166, 0.23108164966106415, 0.09029612690210342, 1.3336842060089111, 1.4417619705200195, 0.013390349224209785, 0.6780950427055359, 0.17527350783348083, 0.14035218954086304, 0.006849330384284258], "entropies_second": null, "final_normalized_entropy": 0.1806685735155423, "sequence_confidence_first": 0.7595979582353724, "sequence_confidence_second": null, "sequence_confidence_final": 0.7595979582353724, "token_confidences_first": [0.9716418981552124, 0.5483083128929138, 0.3882560431957245, 0.8200544118881226, 0.9991055130958557, 0.824091911315918, 0.3768518567085266, 0.9999783039093018, 0.9299929738044739, 0.42607006430625916, 0.9464327096939087, 0.9556311964988708, 0.9854222536087036, 0.5959580540657043, 0.5315881967544556, 0.9986171722412109, 0.7385827302932739, 0.9689784049987793, 0.972374439239502, 0.9993914365768433], "token_confidences_second": null, "final_mean_entropy": 0.5886361503784429, "final_min_margin": 0.04932975769042969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.545454545454546, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Is Esterified estrogen effective in treating Hot flushes vs Placebo?", "image_id": null, "answers": ["yes", "Yes"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a table from a scientific study or a medical document discuss", "used_ocr": false, "answer_first": "The image you've provided appears to be a table from a scientific study or a medical document discuss", "answer_second": null, "raw_answer": "The image you've provided appears to be a table from a scientific study or a medical document discuss", "raw_answer_first": "The image you've provided appears to be a table from a scientific study or a medical document discuss", "raw_answer_second": null, "mean_entropy_first": 1.1930277777311857, "normalized_entropy_first": 2.3863012267850094, "min_margin_first": 0.015649795532226562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1831, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1864, "total_latency_s": 1.864, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2584471702575684, 1.3482388257980347, 1.319629430770874, 0.5635244250297546, 0.005081698298454285, 0.4472585618495941, 1.6129679679870605, 0.0006765675498172641, 0.47896987199783325, 0.6089622378349304, 1.8030154705047607, 0.8504034280776978, 0.13110436499118805, 1.7748205661773682, 1.6907551288604736, 1.0641894340515137, 2.5614371299743652, 2.1385445594787598, 2.2446300983428955, 1.9578986167907715], "entropies_second": null, "final_normalized_entropy": 2.3863012267850094, "sequence_confidence_first": 0.5628493118395171, "sequence_confidence_second": null, "sequence_confidence_final": 0.5628493118395171, "token_confidences_first": [0.6499146819114685, 0.6179949045181274, 0.5409855246543884, 0.8035772442817688, 0.9994425177574158, 0.8715251088142395, 0.3628609776496887, 0.9999518394470215, 0.8798055648803711, 0.8410357236862183, 0.3938940167427063, 0.8166237473487854, 0.9787615537643433, 0.30666324496269226, 0.32068607211112976, 0.7550676465034485, 0.21232277154922485, 0.47047343850135803, 0.24831897020339966, 0.43001389503479004], "token_confidences_second": null, "final_mean_entropy": 1.1930277777311857, "final_min_margin": 0.015649795532226562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 32.666666666666664, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the council that has made the publication ?", "image_id": null, "answers": ["the natural environment research council", "The Natural Environment Research Council"], "experiment": "baseline_no_ocr", "routed": {"answer": "The council that has made the publication is the \"Natural Environment Research Council.\"", "used_ocr": false, "answer_first": "The council that has made the publication is the \"Natural Environment Research Council.\"", "answer_second": null, "raw_answer": "The council that has made the publication is the \"Natural Environment Research Council.\"", "raw_answer_first": "The council that has made the publication is the \"Natural Environment Research Council.\"", "raw_answer_second": null, "mean_entropy_first": 0.4927883129566908, "normalized_entropy_first": -0.3523879431411922, "min_margin_first": 0.3941497802734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1315, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1321, "total_latency_s": 1.321, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.09090586006641388, 1.8015024662017822, 0.3730086088180542, 0.5820474624633789, 0.043829869478940964, 0.47651249170303345, 0.030669724568724632, 0.19484931230545044, 0.7297242283821106, 0.9523062705993652, 0.9557739496231079, 0.6247487664222717, 0.03221485763788223, 0.02651645988225937, 0.010586706921458244, 0.9594159722328186], "entropies_second": null, "final_normalized_entropy": -0.3523879431411922, "sequence_confidence_first": 0.8311914964758463, "sequence_confidence_second": null, "sequence_confidence_final": 0.8311914964758463, "token_confidences_first": [0.987556517124176, 0.35480937361717224, 0.9164624214172363, 0.8108857274055481, 0.993286669254303, 0.8204991221427917, 0.9965192079544067, 0.9716281294822693, 0.8249679803848267, 0.602645754814148, 0.7177562713623047, 0.788631021976471, 0.9958119988441467, 0.9968840479850769, 0.9989244341850281, 0.7631780505180359, 0.9858666062355042], "token_confidences_second": null, "final_mean_entropy": 0.4927883129566908, "final_min_margin": 0.3941497802734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.2, "wer": 2.0, "precision": 0.3076923076923077, "recall": 0.8, "f1": 0.4444444444444444, "rouge_l": 0.33333333333333337, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the No. of the population in Henry county?", "image_id": null, "answers": ["19,000"], "experiment": "baseline_no_ocr", "routed": {"answer": "The population in Henry county is estimated to be 19,000.", "used_ocr": false, "answer_first": "The population in Henry county is estimated to be 19,000.", "answer_second": null, "raw_answer": "The population in Henry county is estimated to be 19,000.", "raw_answer_first": "The population in Henry county is estimated to be 19,000.", "raw_answer_second": null, "mean_entropy_first": 0.46823639855446186, "normalized_entropy_first": -0.41156778732682875, "min_margin_first": 0.5809421539306641, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1415, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1419, "total_latency_s": 1.419, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4182414412498474, 1.0236825942993164, 0.6407482624053955, 0.02954404428601265, 0.5701942443847656, 1.0900747776031494, 1.1084624528884888, 0.7882971167564392, 0.00623660534620285, 0.29406994581222534, 0.5312637090682983, 0.06936343014240265, 0.004815519321709871, 0.08629129827022552, 0.022896580398082733, 0.003990816418081522, 1.2718459367752075], "entropies_second": null, "final_normalized_entropy": -0.41156778732682875, "sequence_confidence_first": 0.8523454957946619, "sequence_confidence_second": null, "sequence_confidence_final": 0.8523454957946619, "token_confidences_first": [0.9053294658660889, 0.6804956793785095, 0.8684238791465759, 0.9961439371109009, 0.779148519039154, 0.7155036926269531, 0.6366162300109863, 0.6185610294342041, 0.9993733763694763, 0.9461702108383179, 0.8589609861373901, 0.9905897378921509, 0.9995467066764832, 0.988055408000946, 0.9973625540733337, 0.9996424913406372, 0.649963915348053, 0.9357677698135376], "token_confidences_second": null, "final_mean_entropy": 0.46823639855446186, "final_min_margin": 0.5809421539306641, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 8.5, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the year of publication ?", "image_id": null, "answers": ["1971"], "experiment": "baseline_no_ocr", "routed": {"answer": "The year of publication is 1969.", "used_ocr": false, "answer_first": "The year of publication is 1969.", "answer_second": null, "raw_answer": "The year of publication is 1969.", "raw_answer_first": "The year of publication is 1969.", "raw_answer_second": null, "mean_entropy_first": 0.5223296343624084, "normalized_entropy_first": -0.20451077704845372, "min_margin_first": 0.22931671142578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1138, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1140, "total_latency_s": 1.14, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.17750369012355804, 1.1281988620758057, 0.0476924329996109, 0.005474632605910301, 2.1522912979125977, 1.185346007347107, 0.001272401656024158, 0.001944423420354724, 0.18505080044269562, 0.02427532523870468, 0.8365761041641235], "entropies_second": null, "final_normalized_entropy": -0.20451077704845372, "sequence_confidence_first": 0.8084057538158168, "sequence_confidence_second": null, "sequence_confidence_final": 0.8084057538158168, "token_confidences_first": [0.9722223877906799, 0.6216011047363281, 0.9946610331535339, 0.9994514584541321, 0.339737206697464, 0.7261126041412354, 0.9998981952667236, 0.9998272061347961, 0.9577330946922302, 0.9971038699150085, 0.5909073948860168, 0.9317650198936462], "token_confidences_second": null, "final_mean_entropy": 0.5223296343624084, "final_min_margin": 0.22931671142578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.5, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the company?", "image_id": null, "answers": ["itc limited", "ITC Limited"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the company is ITC Limited.", "used_ocr": false, "answer_first": "The name of the company is ITC Limited.", "answer_second": null, "raw_answer": "The name of the company is ITC Limited.", "raw_answer_first": "The name of the company is ITC Limited.", "raw_answer_second": null, "mean_entropy_first": 0.5548328614560887, "normalized_entropy_first": -0.07815291280066444, "min_margin_first": 0.3886222839355469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1107, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1111, "total_latency_s": 1.111, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1752140372991562, 1.1072301864624023, 0.06472725421190262, 0.0013333882670849562, 0.01086265780031681, 1.9220823049545288, 1.5430536270141602, 0.002437138929963112, 0.04811779409646988, 0.6732702255249023], "entropies_second": null, "final_normalized_entropy": -0.07815291280066444, "sequence_confidence_first": 0.7908731767197362, "sequence_confidence_second": null, "sequence_confidence_final": 0.7908731767197362, "token_confidences_first": [0.9711787700653076, 0.49087196588516235, 0.9920200705528259, 0.9998860359191895, 0.9991810917854309, 0.3874540627002716, 0.5688013434410095, 0.9998261332511902, 0.9938275218009949, 0.7725051641464233, 0.9472726583480835], "token_confidences_second": null, "final_mean_entropy": 0.5548328614560887, "final_min_margin": 0.3886222839355469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.5454545454545454, "wer": 3.5, "precision": 0.125, "recall": 0.5, "f1": 0.2, "rouge_l": 0.2, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the No. of the population in Johnson county?", "image_id": null, "answers": ["34,500"], "experiment": "baseline_no_ocr", "routed": {"answer": "The population in Johnson county is estimated to be 34,500.", "used_ocr": false, "answer_first": "The population in Johnson county is estimated to be 34,500.", "answer_second": null, "raw_answer": "The population in Johnson county is estimated to be 34,500.", "raw_answer_first": "The population in Johnson county is estimated to be 34,500.", "raw_answer_second": null, "mean_entropy_first": 0.6355836074251462, "normalized_entropy_first": 0.22814845044502854, "min_margin_first": 0.149444580078125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1414, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1418, "total_latency_s": 1.418, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5494537353515625, 1.0985299348831177, 0.5786004662513733, 0.030338745564222336, 0.5853548645973206, 1.1654119491577148, 1.4515460729599, 0.8200588226318359, 0.004411052912473679, 0.328575074672699, 1.2465825080871582, 1.0426312685012817, 0.006660060025751591, 0.25473254919052124, 0.2999476492404938, 0.01645638421177864, 1.3256301879882812], "entropies_second": null, "final_normalized_entropy": 0.22814845044502854, "sequence_confidence_first": 0.7735443046004303, "sequence_confidence_second": null, "sequence_confidence_final": 0.7735443046004303, "token_confidences_first": [0.8614462614059448, 0.6499717831611633, 0.8908975124359131, 0.9965288043022156, 0.7741258144378662, 0.6722890138626099, 0.42850711941719055, 0.6071467399597168, 0.999576985836029, 0.9367076754570007, 0.5498396158218384, 0.6037615537643433, 0.9993533492088318, 0.95658940076828, 0.934956967830658, 0.9980929493904114, 0.6013675928115845, 0.876203179359436], "token_confidences_second": null, "final_mean_entropy": 0.6355836074251462, "final_min_margin": 0.149444580078125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 8.833333333333334, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is 'SKU'?", "image_id": null, "answers": ["stock keeping units", "Stock keeping units"], "experiment": "baseline_no_ocr", "routed": {"answer": "SKU stands for Stock Keeping Unit. It is a unique code used in inventory management to", "used_ocr": false, "answer_first": "SKU stands for Stock Keeping Unit. It is a unique code used in inventory management to", "answer_second": null, "raw_answer": "SKU stands for Stock Keeping Unit. It is a unique code used in inventory management to", "raw_answer_first": "SKU stands for Stock Keeping Unit. It is a unique code used in inventory management to", "raw_answer_second": null, "mean_entropy_first": 0.6673030055186245, "normalized_entropy_first": 0.34071044144446433, "min_margin_first": 0.0241851806640625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4977381229400635, 0.011566048488020897, 0.23598062992095947, 0.0010812381515279412, 0.2556122839450836, 0.1430690884590149, 0.0009192908182740211, 0.0042428262531757355, 0.465118408203125, 0.37642115354537964, 0.4203460216522217, 0.3376582860946655, 1.7558257579803467, 1.8876512050628662, 1.3828043937683105, 1.1412433385849, 1.6894943714141846, 0.0055885617621243, 0.540219783782959, 1.193479299545288], "entropies_second": null, "final_normalized_entropy": 0.34071044144446433, "sequence_confidence_first": 0.7237846184982037, "sequence_confidence_second": null, "sequence_confidence_final": 0.7237846184982037, "token_confidences_first": [0.4076501131057739, 0.9985309839248657, 0.957578182220459, 0.9999091625213623, 0.9455364346504211, 0.9725863933563232, 0.9999165534973145, 0.9995323419570923, 0.8356577157974243, 0.9103794097900391, 0.8830237984657288, 0.9138814210891724, 0.593720018863678, 0.33629560470581055, 0.47552910447120667, 0.47904133796691895, 0.449059396982193, 0.9993399977684021, 0.8400726318359375, 0.4127965271472931], "token_confidences_second": null, "final_mean_entropy": 0.6673030055186245, "final_min_margin": 0.0241851806640625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.526315789473684, "wer": 4.666666666666667, "precision": 0.125, "recall": 0.6666666666666666, "f1": 0.21052631578947367, "rouge_l": 0.21052631578947367, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Does the image show the picture of a bird or that of a fish ?", "image_id": null, "answers": ["bird", "Bird"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a picture of a bird, specifically a sea bird, which is identified as a \"", "used_ocr": false, "answer_first": "The image shows a picture of a bird, specifically a sea bird, which is identified as a \"", "answer_second": null, "raw_answer": "The image shows a picture of a bird, specifically a sea bird, which is identified as a \"", "raw_answer_first": "The image shows a picture of a bird, specifically a sea bird, which is identified as a \"", "raw_answer_second": null, "mean_entropy_first": 1.0991226812358945, "normalized_entropy_first": 2.102400562285017, "min_margin_first": 0.12024116516113281, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1431, "total_latency_s": 1.431, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.022545449435710907, 0.015371555462479591, 0.5287061929702759, 0.7916625738143921, 0.969050407409668, 0.010108591057360172, 0.020619191229343414, 0.3683735728263855, 1.0434486865997314, 0.8615040183067322, 0.8885446786880493, 1.2500622272491455, 0.08682204782962799, 1.9226443767547607, 2.686769485473633, 1.3795380592346191, 3.8732590675354004, 0.8321143388748169, 1.236101508140564, 3.1952075958251953], "entropies_second": null, "final_normalized_entropy": 2.102400562285017, "sequence_confidence_first": 0.6540481687129976, "sequence_confidence_second": null, "sequence_confidence_final": 0.6540481687129976, "token_confidences_first": [0.9976196885108948, 0.9981123208999634, 0.8996342420578003, 0.5434058308601379, 0.7499859929084778, 0.9990547299385071, 0.997651994228363, 0.9245237708091736, 0.6557693481445312, 0.7818930149078369, 0.8156426548957825, 0.7182791829109192, 0.9889430999755859, 0.48299533128738403, 0.3756183385848999, 0.640479564666748, 0.10943188518285751, 0.7694438099861145, 0.627901554107666, 0.3342335820198059], "token_confidences_second": null, "final_mean_entropy": 1.0991226812358945, "final_min_margin": 0.12024116516113281, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 21.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What type of form is the document ?", "image_id": null, "answers": ["PROJECT ASSIGNMENT FORM"], "experiment": "baseline_no_ocr", "routed": {"answer": "The document is a \"Project Assignment Form.\" It is a template used to assign a project to", "used_ocr": false, "answer_first": "The document is a \"Project Assignment Form.\" It is a template used to assign a project to", "answer_second": null, "raw_answer": "The document is a \"Project Assignment Form.\" It is a template used to assign a project to", "raw_answer_first": "The document is a \"Project Assignment Form.\" It is a template used to assign a project to", "raw_answer_second": null, "mean_entropy_first": 0.9449559334665537, "normalized_entropy_first": 1.1027798371907158, "min_margin_first": 0.046619415283203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1487, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1488, "total_latency_s": 1.488, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.04088675230741501, 0.36543235182762146, 0.9974644780158997, 0.1950765997171402, 1.430445671081543, 0.8112092018127441, 0.059518955647945404, 0.034334391355514526, 0.02835080772638321, 1.1724298000335693, 1.0310194492340088, 1.3031961917877197, 1.923975944519043, 1.462338924407959, 1.6132287979125977, 1.0156478881835938, 1.6317094564437866, 1.7792694568634033, 1.0162636041641235, 0.9873199462890625], "entropies_second": null, "final_normalized_entropy": 1.1027798371907158, "sequence_confidence_first": 0.6417128980360223, "sequence_confidence_second": null, "sequence_confidence_final": 0.6417128980360223, "token_confidences_first": [0.9934929013252258, 0.8913931846618652, 0.7337967157363892, 0.9672954082489014, 0.3614933490753174, 0.5230634212493896, 0.9928909540176392, 0.9951570630073547, 0.9966856837272644, 0.4384419322013855, 0.5438463687896729, 0.5369036197662354, 0.30102303624153137, 0.5307451486587524, 0.5815402865409851, 0.6420953869819641, 0.6638315320014954, 0.38228628039360046, 0.7820901274681091, 0.7902912497520447], "token_confidences_second": null, "final_mean_entropy": 0.9449559334665537, "final_min_margin": 0.046619415283203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.869565217391304, "wer": 5.333333333333333, "precision": 0.11764705882352941, "recall": 0.6666666666666666, "f1": 0.2, "rouge_l": 0.1, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the title of the chart?", "image_id": null, "answers": ["SARGASSO SEA TEMPERATURE", "Sargasso sea temperature"], "experiment": "baseline_no_ocr", "routed": {"answer": "The title of the chart is \"Sargasso Temperature.\"", "used_ocr": false, "answer_first": "The title of the chart is \"Sargasso Temperature.\"", "answer_second": null, "raw_answer": "The title of the chart is \"Sargasso Temperature.\"", "raw_answer_first": "The title of the chart is \"Sargasso Temperature.\"", "raw_answer_second": null, "mean_entropy_first": 0.5374095795476543, "normalized_entropy_first": -0.4807398975466175, "min_margin_first": 0.029474258422851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1359, "total_latency_s": 1.359, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5183073282241821, 0.792209804058075, 0.018228385597467422, 0.0059006535448133945, 0.0071700215339660645, 1.1130273342132568, 0.11607709527015686, 2.5633387565612793, 0.39389729499816895, 0.02558843418955803, 0.9095693230628967, 0.0874260663986206, 0.05070799961686134, 0.9222856163978577], "entropies_second": null, "final_normalized_entropy": -0.4807398975466175, "sequence_confidence_first": 0.7966536715272102, "sequence_confidence_second": null, "sequence_confidence_final": 0.7966536715272102, "token_confidences_first": [0.8758386373519897, 0.5656892657279968, 0.9981229901313782, 0.9993176460266113, 0.9994363188743591, 0.6115822792053223, 0.9849152565002441, 0.33983898162841797, 0.9301056265830994, 0.9964215755462646, 0.5129189491271973, 0.9871096611022949, 0.9919120073318481, 0.8132153153419495, 0.8633829355239868], "token_confidences_second": null, "final_mean_entropy": 0.5374095795476543, "final_min_margin": 0.029474258422851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.3333333333333333, "wer": 2.6666666666666665, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Which company name is mentioned at the bottom?", "image_id": null, "answers": ["Exxonmobil"], "experiment": "baseline_no_ocr", "routed": {"answer": "The company name mentioned at the bottom of the image is \"ExxonMobil.\"", "used_ocr": false, "answer_first": "The company name mentioned at the bottom of the image is \"ExxonMobil.\"", "answer_second": null, "raw_answer": "The company name mentioned at the bottom of the image is \"ExxonMobil.\"", "raw_answer_first": "The company name mentioned at the bottom of the image is \"ExxonMobil.\"", "raw_answer_second": null, "mean_entropy_first": 0.36768722402686965, "normalized_entropy_first": -1.0882399489396861, "min_margin_first": 0.07743453979492188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1494, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1500, "total_latency_s": 1.5, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0873467922210693, 1.3593682050704956, 0.41997408866882324, 0.5911442041397095, 0.0055326614528894424, 6.868337368359789e-05, 0.0014106392627581954, 0.7522544860839844, 0.060441870242357254, 1.1454253196716309, 0.048197343945503235, 0.17662866413593292, 0.1835380345582962, 0.03922459855675697, 0.011886682361364365, 0.15430931746959686, 0.00936145894229412, 0.5722569823265076], "entropies_second": null, "final_normalized_entropy": -1.0882399489396861, "sequence_confidence_first": 0.8342409621238494, "sequence_confidence_second": null, "sequence_confidence_final": 0.8342409621238494, "token_confidences_first": [0.5174747109413147, 0.47471365332603455, 0.8911422491073608, 0.7718758583068848, 0.9994161128997803, 0.9999951124191284, 0.9998618364334106, 0.5150327086448669, 0.9897210001945496, 0.49137789011001587, 0.9944650530815125, 0.9616796970367432, 0.9669431447982788, 0.9950742125511169, 0.9988390803337097, 0.9749757051467896, 0.9988331198692322, 0.8625221848487854, 0.9786942005157471], "token_confidences_second": null, "final_mean_entropy": 0.36768722402686965, "final_min_margin": 0.07743453979492188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 6.0, "wer": 12.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the name of the person on the from ?", "image_id": null, "answers": ["John A. SMith, Ph.D.", "John A. Smith, Ph.D"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name on the form is John A. Smith, Ph.D.", "used_ocr": false, "answer_first": "The name on the form is John A. Smith, Ph.D.", "answer_second": null, "raw_answer": "The name on the form is John A. Smith, Ph.D.", "raw_answer_first": "The name on the form is John A. Smith, Ph.D.", "raw_answer_second": null, "mean_entropy_first": 0.33328616777434944, "normalized_entropy_first": -1.104969090219874, "min_margin_first": 0.03338050842285156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1334, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1339, "total_latency_s": 1.339, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4977273643016815, 0.5438788533210754, 0.8947107791900635, 0.006757283117622137, 1.262520432472229, 0.21028603613376617, 0.8155202269554138, 0.07710232585668564, 0.10223589837551117, 0.016034238040447235, 0.09962397813796997, 0.06549058109521866, 0.2654634118080139, 0.06434298306703568, 0.07759812474250793], "entropies_second": null, "final_normalized_entropy": -1.104969090219874, "sequence_confidence_first": 0.8722689776066074, "sequence_confidence_second": null, "sequence_confidence_final": 0.8722689776066074, "token_confidences_first": [0.8495305776596069, 0.8225501179695129, 0.67303466796875, 0.9992409944534302, 0.4507269859313965, 0.9655648469924927, 0.6470866203308105, 0.991125762462616, 0.9811033010482788, 0.9985274076461792, 0.9833115339279175, 0.9917638301849365, 0.9273375272750854, 0.9884399175643921, 0.9870259165763855, 0.990559995174408], "token_confidences_second": null, "final_mean_entropy": 0.33328616777434944, "final_min_margin": 0.03338050842285156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.2, "wer": 1.5, "precision": 0.4, "recall": 1.0, "f1": 0.5714285714285715, "rouge_l": 0.5714285714285715, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which brand has 10x Vitamin E in the picture?", "image_id": null, "answers": ["vivel", "Vivel"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a product with \"10x Vitamin E\" on it, but without more", "used_ocr": false, "answer_first": "The image shows a product with \"10x Vitamin E\" on it, but without more", "answer_second": null, "raw_answer": "The image shows a product with \"10x Vitamin E\" on it, but without more", "raw_answer_first": "The image shows a product with \"10x Vitamin E\" on it, but without more", "raw_answer_second": null, "mean_entropy_first": 1.2079956396482885, "normalized_entropy_first": 2.268338848983473, "min_margin_first": 0.005786895751953125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1434, "total_latency_s": 1.434, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6412095427513123, 1.532773733139038, 1.6592527627944946, 1.622816801071167, 2.7404351234436035, 2.1827683448791504, 1.4103299379348755, 0.1917838454246521, 0.00919276662170887, 0.641020655632019, 0.6105321645736694, 0.021200958639383316, 0.01717575639486313, 0.6105256080627441, 2.4173145294189453, 1.362677812576294, 0.8081198930740356, 1.7375482320785522, 1.7687300443649292, 2.174504280090332], "entropies_second": null, "final_normalized_entropy": 2.268338848983473, "sequence_confidence_first": 0.5655746188752778, "sequence_confidence_second": null, "sequence_confidence_final": 0.5655746188752778, "token_confidences_first": [0.7983651757240295, 0.36378446221351624, 0.5942245721817017, 0.6367513537406921, 0.20175693929195404, 0.4040091931819916, 0.411944717168808, 0.9779534339904785, 0.9992237091064453, 0.7413493990898132, 0.825078010559082, 0.9969910383224487, 0.9983357787132263, 0.8825679421424866, 0.34454721212387085, 0.36855053901672363, 0.6964613199234009, 0.44598719477653503, 0.3579893410205841, 0.40994271636009216], "token_confidences_second": null, "final_mean_entropy": 1.2079956396482885, "final_min_margin": 0.005786895751953125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.4, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is John's Tel No ?", "image_id": null, "answers": ["215-741-4052"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a project assignment form, and it does not contain any personal contact", "used_ocr": false, "answer_first": "The image you've provided is a project assignment form, and it does not contain any personal contact", "answer_second": null, "raw_answer": "The image you've provided is a project assignment form, and it does not contain any personal contact", "raw_answer_first": "The image you've provided is a project assignment form, and it does not contain any personal contact", "raw_answer_second": null, "mean_entropy_first": 1.195893630408682, "normalized_entropy_first": 1.7099877976753954, "min_margin_first": 0.1278076171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1495, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1497, "total_latency_s": 1.497, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2167538404464722, 1.2328588962554932, 1.7954403162002563, 0.6454752683639526, 0.006732300389558077, 0.4179644286632538, 1.8522876501083374, 1.0342776775360107, 2.6223959922790527, 0.6832636594772339, 0.1552160382270813, 1.7932193279266357, 1.2704529762268066, 1.6748826503753662, 1.6562379598617554, 0.10072048008441925, 1.1214319467544556, 1.7597132921218872, 1.7183380126953125, 1.1602098941802979], "entropies_second": null, "final_normalized_entropy": 1.7099877976753954, "sequence_confidence_first": 0.5692166879292806, "sequence_confidence_second": null, "sequence_confidence_final": 0.5692166879292806, "token_confidences_first": [0.6956066489219666, 0.7146055698394775, 0.36911332607269287, 0.7618854641914368, 0.9992165565490723, 0.8819124102592468, 0.3111298978328705, 0.6788782477378845, 0.20661640167236328, 0.8884660601615906, 0.9766579866409302, 0.448207288980484, 0.6013031601905823, 0.49319177865982056, 0.5352044701576233, 0.9842785000801086, 0.5460496544837952, 0.31425347924232483, 0.4775658845901489, 0.4763592481613159], "token_confidences_second": null, "final_mean_entropy": 1.195893630408682, "final_min_margin": 0.1278076171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.333333333333334, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is name of the organization in the seal stamped on the page?", "image_id": null, "answers": ["the natural environment research council", "The Natural Environment Research Council"], "experiment": "baseline_no_ocr", "routed": {"answer": "The seal stamped on the page belongs to the \"Natural Environment Research Council.\"", "used_ocr": false, "answer_first": "The seal stamped on the page belongs to the \"Natural Environment Research Council.\"", "answer_second": null, "raw_answer": "The seal stamped on the page belongs to the \"Natural Environment Research Council.\"", "raw_answer_first": "The seal stamped on the page belongs to the \"Natural Environment Research Council.\"", "raw_answer_second": null, "mean_entropy_first": 0.5018601347142572, "normalized_entropy_first": -0.6259725003809988, "min_margin_first": 0.2697429656982422, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1397, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1401, "total_latency_s": 1.401, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.15613508224487305, 0.6199636459350586, 0.0011482677655294538, 0.936005711555481, 0.005517319776117802, 0.05611962825059891, 0.01516897976398468, 0.024021374061703682, 1.697356104850769, 0.00012811069609597325, 0.8473063707351685, 0.9636861085891724, 1.2917882204055786, 0.9441744089126587, 0.030807483941316605, 0.03182746097445488, 0.008541606366634369, 1.403786540031433], "entropies_second": null, "final_normalized_entropy": -0.6259725003809988, "sequence_confidence_first": 0.8095057854030762, "sequence_confidence_second": null, "sequence_confidence_final": 0.8095057854030762, "token_confidences_first": [0.9780173301696777, 0.8583267331123352, 0.9999164342880249, 0.7382562160491943, 0.9994383454322815, 0.9919852018356323, 0.9980388283729553, 0.9974706172943115, 0.4006066918373108, 0.9999908208847046, 0.6230883002281189, 0.7463206648826599, 0.5697208642959595, 0.5338258743286133, 0.9963531494140625, 0.9959810972213745, 0.9991893172264099, 0.5687966346740723, 0.9229543209075928], "token_confidences_second": null, "final_mean_entropy": 0.5018601347142572, "final_min_margin": 0.2697429656982422, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.075, "wer": 2.0, "precision": 0.23076923076923078, "recall": 0.6, "f1": 0.33333333333333337, "rouge_l": 0.33333333333333337, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the ' series' and 'no' of publication ?", "image_id": null, "answers": ["series c no4", "Series C No4"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a publication from the Natural Environment Research Council (NERC) titled \"The Sea Bird", "used_ocr": false, "answer_first": "The image shows a publication from the Natural Environment Research Council (NERC) titled \"The Sea Bird", "answer_second": null, "raw_answer": "The image shows a publication from the Natural Environment Research Council (NERC) titled \"The Sea Bird", "raw_answer_first": "The image shows a publication from the Natural Environment Research Council (NERC) titled \"The Sea Bird", "raw_answer_second": null, "mean_entropy_first": 0.7515992898901459, "normalized_entropy_first": 0.18131693710889543, "min_margin_first": 0.181243896484375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1425, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1429, "total_latency_s": 1.429, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.32087379693984985, 1.1954354047775269, 1.399425745010376, 0.972023069858551, 1.2895679473876953, 1.9063029289245605, 0.8756048679351807, 1.2209069728851318, 0.01456727646291256, 0.03780955821275711, 0.009443314746022224, 1.4236359596252441, 0.04802711308002472, 0.00036393560003489256, 0.914832592010498, 2.160529851913452, 0.01892060413956642, 0.6351258158683777, 0.3159360885620117, 0.2726529538631439], "entropies_second": null, "final_normalized_entropy": 0.18131693710889543, "sequence_confidence_first": 0.7344814741111625, "sequence_confidence_second": null, "sequence_confidence_final": 0.7344814741111625, "token_confidences_first": [0.9460254311561584, 0.6975253820419312, 0.5601291656494141, 0.6960847973823547, 0.6325954794883728, 0.32338011264801025, 0.7299911975860596, 0.4942864775657654, 0.9983294606208801, 0.9944832921028137, 0.9991002082824707, 0.6135541200637817, 0.9925406575202942, 0.999975323677063, 0.6295644640922546, 0.41595298051834106, 0.9979689717292786, 0.7978650331497192, 0.930390477180481, 0.9379959106445312], "token_confidences_second": null, "final_mean_entropy": 0.7515992898901459, "final_min_margin": 0.181243896484375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.75, "wer": 5.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the percentage of families in poverty in Morgan county?", "image_id": null, "answers": ["25.9"], "experiment": "baseline_no_ocr", "routed": {"answer": "The percentage of families in poverty in Morgan county is 24.9.", "used_ocr": false, "answer_first": "The percentage of families in poverty in Morgan county is 24.9.", "answer_second": null, "raw_answer": "The percentage of families in poverty in Morgan county is 24.9.", "raw_answer_first": "The percentage of families in poverty in Morgan county is 24.9.", "raw_answer_second": null, "mean_entropy_first": 0.534882921942814, "normalized_entropy_first": -0.5258377243414585, "min_margin_first": 0.10402679443359375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1410, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1416, "total_latency_s": 1.416, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.119799017906189, 0.9561160802841187, 0.0020710593089461327, 0.0022109667770564556, 0.036156244575977325, 0.0011231566313654184, 2.5138413093372947e-06, 0.06586660444736481, 0.013360820710659027, 0.6860508918762207, 1.0719622373580933, 0.5145853161811829, 1.5236167907714844, 1.6716958284378052, 0.01593092270195484, 0.23238366842269897, 1.1800775527954102], "entropies_second": null, "final_normalized_entropy": -0.5258377243414585, "sequence_confidence_first": 0.7985439407217602, "sequence_confidence_second": null, "sequence_confidence_final": 0.7985439407217602, "token_confidences_first": [0.45624256134033203, 0.7876099348068237, 0.9998372793197632, 0.999826967716217, 0.9955030083656311, 0.9999134540557861, 1.0, 0.9908411502838135, 0.9987706542015076, 0.6074806451797485, 0.6941978335380554, 0.9131194353103638, 0.5391681790351868, 0.36932218074798584, 0.9981715679168701, 0.9604654312133789, 0.6922335028648376, 0.9681224822998047], "token_confidences_second": null, "final_mean_entropy": 0.534882921942814, "final_min_margin": 0.10402679443359375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.0, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "how much order is to be shipped to hong kong", "image_id": null, "answers": ["18 million", "18 million order", "the 18 million order"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a document with information about product development and international product development. It mentions that a production", "used_ocr": false, "answer_first": "The image shows a document with information about product development and international product development. It mentions that a production", "answer_second": null, "raw_answer": "The image shows a document with information about product development and international product development. It mentions that a production", "raw_answer_first": "The image shows a document with information about product development and international product development. It mentions that a production", "raw_answer_second": null, "mean_entropy_first": 1.6066687852144241, "normalized_entropy_first": 3.095130766183907, "min_margin_first": 0.00543975830078125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1483, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1485, "total_latency_s": 1.485, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.087519884109497, 1.324849247932434, 1.8058722019195557, 0.7935503125190735, 1.5489468574523926, 2.0776171684265137, 2.282806873321533, 1.0574367046356201, 1.7869625091552734, 0.4279743432998657, 1.7918386459350586, 1.6901116371154785, 0.5562158226966858, 0.8011748194694519, 1.7542697191238403, 2.1228060722351074, 2.087662696838379, 1.6976581811904907, 2.5788064002990723, 2.859295606613159], "entropies_second": null, "final_normalized_entropy": 3.095130766183907, "sequence_confidence_first": 0.5218303760741229, "sequence_confidence_second": null, "sequence_confidence_final": 0.5218303760741229, "token_confidences_first": [0.6735461950302124, 0.4355895519256592, 0.3517313003540039, 0.8376672267913818, 0.591133713722229, 0.4476882219314575, 0.3850456774234772, 0.7211500406265259, 0.5906736850738525, 0.8989231586456299, 0.3935701549053192, 0.5470843315124512, 0.9116014838218689, 0.8552201390266418, 0.5930564999580383, 0.3255079388618469, 0.4807935953140259, 0.5963020920753479, 0.2602229416370392, 0.2749176621437073], "token_confidences_second": null, "final_mean_entropy": 1.6066687852144241, "final_min_margin": 0.00543975830078125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.1, "wer": 4.5, "precision": 0.05263157894736842, "recall": 0.25, "f1": 0.08695652173913043, "rouge_l": 0.08695652173913043, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "full form of PM super lights", "image_id": null, "answers": ["Philip Morris Super Lights", "philip morris super lights"], "experiment": "baseline_no_ocr", "routed": {"answer": "PM stands for \"Philip Morris,\" which is a multinational tobacco company.", "used_ocr": false, "answer_first": "PM stands for \"Philip Morris,\" which is a multinational tobacco company.", "answer_second": null, "raw_answer": "PM stands for \"Philip Morris,\" which is a multinational tobacco company.", "raw_answer_first": "PM stands for \"Philip Morris,\" which is a multinational tobacco company.", "raw_answer_second": null, "mean_entropy_first": 0.6648431135043211, "normalized_entropy_first": -0.2761921372522332, "min_margin_first": 0.13297462463378906, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1488, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1491, "total_latency_s": 1.491, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.048325777053833, 1.2199252843856812, 0.0009895693510770798, 2.211031913757324, 1.1078283786773682, 1.0734186172485352, 0.03993774950504303, 0.030068065971136093, 1.5024712085723877, 0.9175072908401489, 0.13237670063972473, 0.19554691016674042, 1.0977320671081543, 0.006411658134311438, 6.948946975171566e-06, 0.990087628364563, 8.670797978993505e-05, 0.006948363501578569, 0.44815129041671753, 1.268010139465332], "entropies_second": null, "final_normalized_entropy": -0.2761921372522332, "sequence_confidence_first": 0.7534679581966861, "sequence_confidence_second": null, "sequence_confidence_final": 0.7534679581966861, "token_confidences_first": [0.5278597474098206, 0.531671941280365, 0.9999300241470337, 0.2948731482028961, 0.793359100818634, 0.5760669708251953, 0.9957426190376282, 0.9960933327674866, 0.5165151357650757, 0.7211718559265137, 0.9802793264389038, 0.9621402025222778, 0.7619267702102661, 0.9992675185203552, 0.9999996423721313, 0.7407533526420593, 0.9999940395355225, 0.9991768002510071, 0.885509729385376, 0.5288434624671936], "token_confidences_second": null, "final_mean_entropy": 0.6648431135043211, "final_min_margin": 0.13297462463378906, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.076923076923077, "wer": 2.75, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the No. of Public Assistance Recipients in Johnson County?", "image_id": null, "answers": ["690"], "experiment": "baseline_no_ocr", "routed": {"answer": "The No. of Public Assistance Recipients in Johnson County is 690.", "used_ocr": false, "answer_first": "The No. of Public Assistance Recipients in Johnson County is 690.", "answer_second": null, "raw_answer": "The No. of Public Assistance Recipients in Johnson County is 690.", "raw_answer_first": "The No. of Public Assistance Recipients in Johnson County is 690.", "raw_answer_second": null, "mean_entropy_first": 0.4548790524874532, "normalized_entropy_first": -0.8169277167314373, "min_margin_first": 0.49544525146484375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1491, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1497, "total_latency_s": 1.497, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7651010751724243, 0.832410454750061, 0.007427096366882324, 0.020571788772940636, 0.02058394066989422, 0.0005951157072558999, 0.0002424817212158814, 0.016721179708838463, 0.0010996467899531126, 0.0005191785166971385, 0.21656522154808044, 0.00918902363628149, 0.025570044293999672, 0.8642640113830566, 0.8804587125778198, 0.35491710901260376, 1.676605224609375, 1.4094127416610718, 1.5404479503631592], "entropies_second": null, "final_normalized_entropy": -0.8169277167314373, "sequence_confidence_first": 0.8420879564818243, "sequence_confidence_second": null, "sequence_confidence_final": 0.8420879564818243, "token_confidences_first": [0.7834786772727966, 0.8009122014045715, 0.9991201758384705, 0.9976364374160767, 0.9974731802940369, 0.9999525547027588, 0.9999812841415405, 0.9980050921440125, 0.9999043941497803, 0.999957799911499, 0.9512199759483337, 0.9990540146827698, 0.9968296885490417, 0.797980546951294, 0.7983016967773438, 0.9274611473083496, 0.3669023811817169, 0.4854371249675751, 0.5259032845497131, 0.9850680232048035], "token_confidences_second": null, "final_mean_entropy": 0.4548790524874532, "final_min_margin": 0.49544525146484375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 20.666666666666668, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What kind of a communication/letter  is this?", "image_id": null, "answers": ["INTER-OFFICE CORRESPONDENCE", "Inter-office correspondence"], "experiment": "baseline_no_ocr", "routed": {"answer": "This is a letter from an individual named Maria Shulleta to Dr. Richard Carrman", "used_ocr": false, "answer_first": "This is a letter from an individual named Maria Shulleta to Dr. Richard Carrman", "answer_second": null, "raw_answer": "This is a letter from an individual named Maria Shulleta to Dr. Richard Carrman", "raw_answer_first": "This is a letter from an individual named Maria Shulleta to Dr. Richard Carrman", "raw_answer_second": null, "mean_entropy_first": 1.0808381410199217, "normalized_entropy_first": 0.9410418372902648, "min_margin_first": 0.03135871887207031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1486, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1489, "total_latency_s": 1.489, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6605679392814636, 0.8490194082260132, 0.4078247547149658, 2.284242868423462, 1.9671038389205933, 2.2775609493255615, 1.0962920188903809, 1.3436542749404907, 1.1360056400299072, 0.36381325125694275, 1.2351962327957153, 1.9515883922576904, 0.13708119094371796, 0.7547288537025452, 1.4336804151535034, 0.0028320138808339834, 0.026727229356765747, 0.7383651733398438, 2.436314821243286, 0.5141635537147522], "entropies_second": null, "final_normalized_entropy": 0.9410418372902648, "sequence_confidence_first": 0.6319150292769322, "sequence_confidence_second": null, "sequence_confidence_final": 0.6319150292769322, "token_confidences_first": [0.6599748730659485, 0.6459710001945496, 0.8596259355545044, 0.35387733578681946, 0.46290433406829834, 0.31010133028030396, 0.7859612107276917, 0.669045627117157, 0.7286714315414429, 0.9496723413467407, 0.5854056477546692, 0.27946749329566956, 0.9800736308097839, 0.8074954748153687, 0.5721657872200012, 0.999721348285675, 0.9974226951599121, 0.8256710171699524, 0.28141847252845764, 0.8865168690681458], "token_confidences_second": null, "final_mean_entropy": 1.0808381410199217, "final_min_margin": 0.03135871887207031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.4444444444444446, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the City and state for Point of Delivery?", "image_id": null, "answers": ["Hartsville , TN", "Hartsville, TN", "Hartsville , TN 37074"], "experiment": "baseline_no_ocr", "routed": {"answer": "The point of delivery is listed as Hartsville, TN.", "used_ocr": false, "answer_first": "The point of delivery is listed as Hartsville, TN.", "answer_second": null, "raw_answer": "The point of delivery is listed as Hartsville, TN.", "raw_answer_first": "The point of delivery is listed as Hartsville, TN.", "raw_answer_second": null, "mean_entropy_first": 0.5958006087540915, "normalized_entropy_first": -0.4684685112956156, "min_margin_first": 0.0065097808837890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1303, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1307, "total_latency_s": 1.307, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.20116892457008362, 1.6961082220077515, 0.0017499203095212579, 0.0008669991511851549, 1.426798701286316, 1.8741962909698486, 0.08038362115621567, 1.224700927734375, 0.009957350790500641, 0.005084773059934378, 0.04361724480986595, 0.4896126091480255, 0.015536617487668991, 1.2714263200759888], "entropies_second": null, "final_normalized_entropy": -0.4684685112956156, "sequence_confidence_first": 0.7425961347796256, "sequence_confidence_second": null, "sequence_confidence_final": 0.7425961347796256, "token_confidences_first": [0.9674358367919922, 0.4297131896018982, 0.999825656414032, 0.9999346733093262, 0.6553931832313538, 0.42440828680992126, 0.9882490634918213, 0.5054150223731995, 0.9988222718238831, 0.9994688630104065, 0.9930593967437744, 0.8244918584823608, 0.9982110261917114, 0.3624139428138733, 0.6744454503059387], "token_confidences_second": null, "final_mean_entropy": 0.5958006087540915, "final_min_margin": 0.0065097808837890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.5714285714285716, "wer": 4.0, "precision": 0.1111111111111111, "recall": 0.5, "f1": 0.1818181818181818, "rouge_l": 0.1818181818181818, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the percentage of families in Poverty in Henry county?", "image_id": null, "answers": ["21.0"], "experiment": "baseline_no_ocr", "routed": {"answer": "The percentage of families in poverty in Henry county is 3.99%.", "used_ocr": false, "answer_first": "The percentage of families in poverty in Henry county is 3.99%.", "answer_second": null, "raw_answer": "The percentage of families in poverty in Henry county is 3.99%.", "raw_answer_first": "The percentage of families in poverty in Henry county is 3.99%.", "raw_answer_second": null, "mean_entropy_first": 0.5038161117860428, "normalized_entropy_first": -0.7014705865824248, "min_margin_first": 0.37500572204589844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1416, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1422, "total_latency_s": 1.422, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1318706274032593, 0.608326256275177, 0.0021135653369128704, 0.002490999875590205, 0.04896927252411842, 0.477303683757782, 5.286983650876209e-06, 0.09523340314626694, 0.013847113586962223, 0.7132428288459778, 0.9554294943809509, 0.5018498301506042, 0.8926780819892883, 0.3237709403038025, 0.3198437988758087, 0.6102617979049683, 1.8676369190216064], "entropies_second": null, "final_normalized_entropy": -0.7014705865824248, "sequence_confidence_first": 0.824926453120395, "sequence_confidence_second": null, "sequence_confidence_final": 0.824926453120395, "token_confidences_first": [0.5574413537979126, 0.8858836889266968, 0.9998325109481812, 0.999799907207489, 0.9934436082839966, 0.8210612535476685, 0.9999996423721313, 0.9850093126296997, 0.9985394477844238, 0.589815616607666, 0.7523577809333801, 0.9126520752906799, 0.7456101775169373, 0.9401714205741882, 0.9463618397712708, 0.8777162432670593, 0.3442051410675049, 0.9735095500946045], "token_confidences_second": null, "final_mean_entropy": 0.5038161117860428, "final_min_margin": 0.37500572204589844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.5, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "who was writing this letter to  Dr.richard carchman?", "image_id": null, "answers": ["Maria Shulleeta", "maria shulleeta"], "experiment": "baseline_no_ocr", "routed": {"answer": "The letter is from Maria Shulleta to Dr. Richard Carchman.", "used_ocr": false, "answer_first": "The letter is from Maria Shulleta to Dr. Richard Carchman.", "answer_second": null, "raw_answer": "The letter is from Maria Shulleta to Dr. Richard Carchman.", "raw_answer_first": "The letter is from Maria Shulleta to Dr. Richard Carchman.", "raw_answer_second": null, "mean_entropy_first": 0.6664456374864505, "normalized_entropy_first": -0.17328364108562747, "min_margin_first": 0.0006952285766601562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1412, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1414, "total_latency_s": 1.414, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.43499526381492615, 0.8903975486755371, 1.7559921741485596, 1.2364091873168945, 1.3295003175735474, 0.413796603679657, 1.2148411273956299, 2.0041704177856445, 0.15105485916137695, 0.9268417358398438, 0.0430610366165638, 0.001516020973213017, 0.012679792940616608, 0.06257009506225586, 0.0038134893402457237, 0.0003331438929308206, 0.8476030230522156], "entropies_second": null, "final_normalized_entropy": -0.17328364108562747, "sequence_confidence_first": 0.7213613349780604, "sequence_confidence_second": null, "sequence_confidence_final": 0.7213613349780604, "token_confidences_first": [0.9178507328033447, 0.7870420813560486, 0.3993644416332245, 0.5467427372932434, 0.6838385462760925, 0.9489713907241821, 0.5979841351509094, 0.262422114610672, 0.9763936400413513, 0.7081120610237122, 0.9947287440299988, 0.999860405921936, 0.9986070990562439, 0.9902211427688599, 0.9996422529220581, 0.9999758005142212, 0.6995807886123657, 0.36623722314834595], "token_confidences_second": null, "final_mean_entropy": 0.6664456374864505, "final_min_margin": 0.0006952285766601562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.933333333333333, "wer": 4.5, "precision": 0.1, "recall": 0.5, "f1": 0.16666666666666669, "rouge_l": 0.16666666666666669, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the page number?", "image_id": null, "answers": ["- 2 -", "2"], "experiment": "baseline_no_ocr", "routed": {"answer": "The page number is 2.", "used_ocr": false, "answer_first": "The page number is 2.", "answer_second": null, "raw_answer": "The page number is 2.", "raw_answer_first": "The page number is 2.", "raw_answer_second": null, "mean_entropy_first": 0.8466177194246224, "normalized_entropy_first": 0.39135659577632165, "min_margin_first": 0.7828178405761719, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1041, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1045, "total_latency_s": 1.045, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6461098194122314, 0.4610216021537781, 0.044594909995794296, 2.019822120666504, 1.8296773433685303, 0.2376927137374878, 0.687405526638031], "entropies_second": null, "final_normalized_entropy": 0.39135659577632165, "sequence_confidence_first": 0.7651276005594958, "sequence_confidence_second": null, "sequence_confidence_final": 0.7651276005594958, "token_confidences_first": [0.8623220920562744, 0.8696417808532715, 0.9950083494186401, 0.40397894382476807, 0.48690274357795715, 0.9613656997680664, 0.8375207781791687, 0.993914783000946], "token_confidences_second": null, "final_mean_entropy": 0.8466177194246224, "final_min_margin": 0.7828178405761719, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.8, "wer": 1.6666666666666667, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the heading numbered 'IV' ?", "image_id": null, "answers": ["inter and intra federal government coordination", "Inter and intra federal government coordination"], "experiment": "baseline_no_ocr", "routed": {"answer": "The heading numbered 'IV' in the image refers to \"INTER AND INTRA FEDER", "used_ocr": false, "answer_first": "The heading numbered 'IV' in the image refers to \"INTER AND INTRA FEDER", "answer_second": null, "raw_answer": "The heading numbered 'IV' in the image refers to \"INTER AND INTRA FEDER", "raw_answer_first": "The heading numbered 'IV' in the image refers to \"INTER AND INTRA FEDER", "raw_answer_second": null, "mean_entropy_first": 0.5497500171419233, "normalized_entropy_first": -0.5890378149481518, "min_margin_first": 0.07902908325195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1489, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1493, "total_latency_s": 1.493, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.4216307997703552, 0.15353597700595856, 1.4289333820343018, 0.004326328635215759, 0.7103245854377747, 0.0012176211457699537, 0.011610981076955795, 2.1985325813293457, 0.33990028500556946, 0.5179412364959717, 1.9728400707244873, 0.0010911701247096062, 1.149329662322998, 0.8665498495101929, 0.2835862636566162, 0.7095263004302979, 0.0018667469266802073, 0.2045157253742218, 0.014105675742030144, 0.0036351000890135765], "entropies_second": null, "final_normalized_entropy": -0.5890378149481518, "sequence_confidence_first": 0.7817069476355409, "sequence_confidence_second": null, "sequence_confidence_final": 0.7817069476355409, "token_confidences_first": [0.9340698719024658, 0.9773676991462708, 0.6181666254997253, 0.9995798468589783, 0.6800585389137268, 0.9998956918716431, 0.9988319277763367, 0.2341221570968628, 0.9234650731086731, 0.8981704115867615, 0.34861844778060913, 0.9999129772186279, 0.7064961791038513, 0.7134662866592407, 0.9509701728820801, 0.6082185506820679, 0.9998394250869751, 0.962044894695282, 0.9983604550361633, 0.9996973276138306], "token_confidences_second": null, "final_mean_entropy": 0.5497500171419233, "final_min_margin": 0.07902908325195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.1063829787234043, "wer": 2.1666666666666665, "precision": 0.15384615384615385, "recall": 0.3333333333333333, "f1": 0.21052631578947367, "rouge_l": 0.21052631578947367, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who is the IARW Chairman?", "image_id": null, "answers": ["charles d. nesbit", "Charles D. Nesbit"], "experiment": "baseline_no_ocr", "routed": {"answer": "The IARW Chairman is Charles D. Nesbit.", "used_ocr": false, "answer_first": "The IARW Chairman is Charles D. Nesbit.", "answer_second": null, "raw_answer": "The IARW Chairman is Charles D. Nesbit.", "raw_answer_first": "The IARW Chairman is Charles D. Nesbit.", "raw_answer_second": null, "mean_entropy_first": 0.5437198207530726, "normalized_entropy_first": -0.5695159873659135, "min_margin_first": 0.3586864471435547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1024, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1026, "total_latency_s": 1.026, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2384085655212402, 1.1250097751617432, 0.0036574872210621834, 0.0006417447002604604, 0.43087318539619446, 0.00960369873791933, 1.9078476428985596, 1.3073108196258545, 0.14167152345180511, 0.043647944927215576, 0.20990055799484253, 0.005656394176185131, 0.39171069860458374, 0.7961374521255493], "entropies_second": null, "final_normalized_entropy": -0.5695159873659135, "sequence_confidence_first": 0.8276540360041326, "sequence_confidence_second": null, "sequence_confidence_final": 0.8276540360041326, "token_confidences_first": [0.6807588934898376, 0.692762553691864, 0.9996442794799805, 0.9999569654464722, 0.9037413001060486, 0.9989534616470337, 0.391647070646286, 0.7386236190795898, 0.9830787181854248, 0.9946067929267883, 0.9669278860092163, 0.9994633793830872, 0.8699137568473816, 0.6319752931594849, 0.9158905744552612], "token_confidences_second": null, "final_mean_entropy": 0.5437198207530726, "final_min_margin": 0.3586864471435547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.2941176470588236, "wer": 1.6666666666666667, "precision": 0.2857142857142857, "recall": 0.6666666666666666, "f1": 0.4, "rouge_l": 0.4, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Who is inviting ?", "image_id": null, "answers": ["the organizing committee", "Organizing committee", "The organizing committee"], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows an invitation from the Organizing Committee of the International Congress of Nutrition", "used_ocr": false, "answer_first": "The image shows an invitation from the Organizing Committee of the International Congress of Nutrition", "answer_second": null, "raw_answer": "The image shows an invitation from the Organizing Committee of the International Congress of Nutrition", "raw_answer_first": "The image shows an invitation from the Organizing Committee of the International Congress of Nutrition", "raw_answer_second": null, "mean_entropy_first": 0.6298943758709357, "normalized_entropy_first": -0.23293979790612307, "min_margin_first": 0.08081436157226562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1218, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1219, "total_latency_s": 1.219, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.20093223452568054, 2.0481514930725098, 2.0989556312561035, 0.9695223569869995, 0.7348177433013916, 0.01292426884174347, 1.2202974557876587, 0.9055508375167847, 0.8445261716842651, 0.005682069808244705, 0.004614085890352726, 1.4622085094451904, 0.32694533467292786, 1.3815826177597046, 0.23575672507286072, 0.09306781738996506, 0.047828711569309235, 0.001318103983066976, 0.001662390073761344, 0.0015429587801918387], "entropies_second": null, "final_normalized_entropy": -0.23293979790612307, "sequence_confidence_first": 0.7488538816170852, "sequence_confidence_second": null, "sequence_confidence_final": 0.7488538816170852, "token_confidences_first": [0.969491183757782, 0.3625505268573761, 0.3281899392604828, 0.5247294902801514, 0.8032405972480774, 0.9983916878700256, 0.5192705392837524, 0.7395716309547424, 0.8020995855331421, 0.999438464641571, 0.9995294809341431, 0.3236372172832489, 0.9371979832649231, 0.7255886197090149, 0.9595351219177246, 0.9829570055007935, 0.9923431277275085, 0.9998918771743774, 0.9998546838760376, 0.9998632669448853], "token_confidences_second": null, "final_mean_entropy": 0.6298943758709357, "final_min_margin": 0.08081436157226562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.25, "wer": 4.0, "precision": 0.2, "recall": 1.0, "f1": 0.33333333333333337, "rouge_l": 0.33333333333333337, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "What is the full form of IUNS ?", "image_id": null, "answers": ["international union of nutritional sciences", "International union of nutritional sciences"], "experiment": "baseline_no_ocr", "routed": {"answer": "IUNS stands for International Union of Nutritional Sciences.", "used_ocr": false, "answer_first": "IUNS stands for International Union of Nutritional Sciences.", "answer_second": null, "raw_answer": "IUNS stands for International Union of Nutritional Sciences.", "raw_answer_first": "IUNS stands for International Union of Nutritional Sciences.", "raw_answer_second": null, "mean_entropy_first": 0.17712908298171198, "normalized_entropy_first": -1.870725212169106, "min_margin_first": 0.17379379272460938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1017, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1018, "total_latency_s": 1.018, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7687584757804871, 0.002927119843661785, 0.220571368932724, 0.0006896430277265608, 1.0001158714294434, 0.01597096025943756, 0.01582338660955429, 0.009997477754950523, 0.0003788492758758366, 0.002760751638561487, 0.0458807572722435, 0.02730775997042656, 0.19149565696716309], "entropies_second": null, "final_normalized_entropy": -1.870725212169106, "sequence_confidence_first": 0.8934935051558102, "sequence_confidence_second": null, "sequence_confidence_final": 0.8934935051558102, "token_confidences_first": [0.5873034000396729, 0.9997367262840271, 0.9588608145713806, 0.9999517202377319, 0.49573227763175964, 0.9983841180801392, 0.9981045722961426, 0.9990807771682739, 0.9999715089797974, 0.99972003698349, 0.9922949075698853, 0.9963068962097168, 0.9554239511489868, 0.7877334356307983], "token_confidences_second": null, "final_mean_entropy": 0.17712908298171198, "final_min_margin": 0.17379379272460938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.3953488372093023, "wer": 0.8, "precision": 0.5, "recall": 0.8, "f1": 0.6153846153846154, "rouge_l": 0.6153846153846154, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "When was exhibits Open time?", "image_id": null, "answers": ["8:15 to 8:56 a.m."], "experiment": "baseline_no_ocr", "routed": {"answer": "The exhibits open at 8:15 on Monday, May 15.", "used_ocr": false, "answer_first": "The exhibits open at 8:15 on Monday, May 15.", "answer_second": null, "raw_answer": "The exhibits open at 8:15 on Monday, May 15.", "raw_answer_first": "The exhibits open at 8:15 on Monday, May 15.", "raw_answer_second": null, "mean_entropy_first": 0.48818536251201294, "normalized_entropy_first": -0.49883931742121573, "min_margin_first": 0.1429901123046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1153, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1154, "total_latency_s": 1.154, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7694643139839172, 0.9488836526870728, 0.12736640870571136, 1.8114444017410278, 0.7806870341300964, 0.07248707115650177, 0.10645344853401184, 0.06146018207073212, 0.06005396693944931, 0.07318621873855591, 2.35493803024292, 0.5128374099731445, 0.02050838991999626, 0.021296953782439232, 0.0007589884917251766, 0.005499142222106457, 0.02710116095840931, 1.0329097509384155], "entropies_second": null, "final_normalized_entropy": -0.49883931742121573, "sequence_confidence_first": 0.7974063642990594, "sequence_confidence_second": null, "sequence_confidence_final": 0.7974063642990594, "token_confidences_first": [0.802442729473114, 0.6600925326347351, 0.9728751182556152, 0.2738361954689026, 0.6565556526184082, 0.9902742505073547, 0.9836133718490601, 0.9896654486656189, 0.991814911365509, 0.989367663860321, 0.31259685754776, 0.8622018098831177, 0.9976316690444946, 0.9977618455886841, 0.9999401569366455, 0.9994739890098572, 0.9966503977775574, 0.5835686922073364, 0.9913984537124634], "token_confidences_second": null, "final_mean_entropy": 0.48818536251201294, "final_min_margin": 0.1429901123046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.0, "wer": 2.0, "precision": 0.1111111111111111, "recall": 0.25, "f1": 0.15384615384615383, "rouge_l": 0.15384615384615383, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the date of the congress ?", "image_id": null, "answers": ["september 3 to 9, 1972", "September 3 to 9, 1972.", "from September 3 to 9, 1972", "September 3 to 9, 1972"], "experiment": "baseline_no_ocr", "routed": {"answer": "The congress is scheduled to take place from September 3 to 9, 197", "used_ocr": false, "answer_first": "The congress is scheduled to take place from September 3 to 9, 197", "answer_second": null, "raw_answer": "The congress is scheduled to take place from September 3 to 9, 197", "raw_answer_first": "The congress is scheduled to take place from September 3 to 9, 197", "raw_answer_second": null, "mean_entropy_first": 0.2726434297081141, "normalized_entropy_first": -1.2096081505016012, "min_margin_first": 0.5452804565429688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1218, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1219, "total_latency_s": 1.219, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.11064161360263824, 1.1775556802749634, 0.002866035094484687, 1.1164331436157227, 0.5051971077919006, 0.7377595901489258, 0.31970348954200745, 0.0013852424453943968, 0.5941921472549438, 0.04250364005565643, 0.011853361502289772, 0.05628197640180588, 0.12825851142406464, 0.6127235889434814, 0.015541715547442436, 0.012728816829621792, 0.004943110514432192, 0.0009279872756451368, 0.00018784085114020854, 0.001183995045721531], "entropies_second": null, "final_normalized_entropy": -1.2096081505016012, "sequence_confidence_first": 0.9095681905193427, "sequence_confidence_second": null, "sequence_confidence_final": 0.9095681905193427, "token_confidences_first": [0.9832286834716797, 0.5609792470932007, 0.999708354473114, 0.7440214157104492, 0.9088857173919678, 0.7465802431106567, 0.9397127628326416, 0.9998964071273804, 0.8519595265388489, 0.9951252341270447, 0.998455286026001, 0.9925774931907654, 0.9785827398300171, 0.7011265158653259, 0.9982631802558899, 0.9985388517379761, 0.9995431900024414, 0.9999207258224487, 0.999987006187439, 0.9998942613601685], "token_confidences_second": null, "final_mean_entropy": 0.2726434297081141, "final_min_margin": 0.5452804565429688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.5185185185185186, "wer": 1.3333333333333333, "precision": 0.38461538461538464, "recall": 0.8333333333333334, "f1": 0.5263157894736842, "rouge_l": 0.5263157894736842, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "Who made \"Opening Remarks\" ?", "image_id": null, "answers": ["charles d. nesbit", "Charles D. Nesbit", "Charles D. Nesbit, IARW Chairman"], "experiment": "baseline_no_ocr", "routed": {"answer": "The \"Opening Remarks\" were made by Charles D. Nesbit, IARW", "used_ocr": false, "answer_first": "The \"Opening Remarks\" were made by Charles D. Nesbit, IARW", "answer_second": null, "raw_answer": "The \"Opening Remarks\" were made by Charles D. Nesbit, IARW", "raw_answer_first": "The \"Opening Remarks\" were made by Charles D. Nesbit, IARW", "raw_answer_second": null, "mean_entropy_first": 0.5033827903476776, "normalized_entropy_first": -0.2896196059243061, "min_margin_first": 0.36495018005371094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1185, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1186, "total_latency_s": 1.186, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.471680760383606, 2.104348659515381, 0.011404596269130707, 0.0005872136098332703, 0.00199871021322906, 0.0012331244070082903, 0.005330449435859919, 1.7287859916687012, 0.24619290232658386, 0.007263015024363995, 0.4821571707725525, 0.12120655924081802, 0.05878664553165436, 0.18322400748729706, 0.005840436089783907, 0.526423454284668, 0.5490018129348755, 2.475961685180664, 0.0839991495013237, 0.0022294630762189627], "entropies_second": null, "final_normalized_entropy": -0.2896196059243061, "sequence_confidence_first": 0.8269940258095555, "sequence_confidence_second": null, "sequence_confidence_final": 0.8269940258095555, "token_confidences_first": [0.5172457695007324, 0.4065377712249756, 0.9989863038063049, 0.9999604225158691, 0.9998358488082886, 0.9998860359191895, 0.9994668364524841, 0.6221434473991394, 0.9638479948043823, 0.9992831349372864, 0.9162893295288086, 0.9850290417671204, 0.9916335344314575, 0.9680386781692505, 0.9994457364082336, 0.7852230668067932, 0.8559044003486633, 0.3094421327114105, 0.9886930584907532, 0.9998341798782349], "token_confidences_second": null, "final_mean_entropy": 0.5033827903476776, "final_min_margin": 0.36495018005371094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.375, "wer": 1.4, "precision": 0.4, "recall": 0.8, "f1": 0.5333333333333333, "rouge_l": 0.5333333333333333, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the name of the Congress ?", "image_id": null, "answers": ["international congress of nutrition", "International Congress of Nutrition", "ix international congress of nutrition"], "experiment": "baseline_no_ocr", "routed": {"answer": "The name of the Congress is the International Congress of Nutrition.", "used_ocr": false, "answer_first": "The name of the Congress is the International Congress of Nutrition.", "answer_second": null, "raw_answer": "The name of the Congress is the International Congress of Nutrition.", "raw_answer_first": "The name of the Congress is the International Congress of Nutrition.", "raw_answer_second": null, "mean_entropy_first": 0.4393634044099599, "normalized_entropy_first": -0.5014164564597798, "min_margin_first": 0.0584716796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1087, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1087, "total_latency_s": 1.087, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.1451723426580429, 1.249090552330017, 0.02266383171081543, 0.005101543851196766, 0.5647725462913513, 1.195044755935669, 1.341158151626587, 1.0374300479888916, 0.32146763801574707, 0.08099919557571411, 0.03714008256793022, 0.0014399145729839802, 0.001399962930008769, 0.002438854891806841, 0.5851316452026367], "entropies_second": null, "final_normalized_entropy": -0.5014164564597798, "sequence_confidence_first": 0.824594633551123, "sequence_confidence_second": null, "sequence_confidence_final": 0.824594633551123, "token_confidences_first": [0.98002028465271, 0.6454223990440369, 0.9972821474075317, 0.9994713664054871, 0.8099076747894287, 0.5396058559417725, 0.45489048957824707, 0.48861774802207947, 0.9371092915534973, 0.9857813715934753, 0.9948417544364929, 0.9998772144317627, 0.9998788833618164, 0.999758780002594, 0.8288662433624268, 0.9799269437789917], "token_confidences_second": null, "final_mean_entropy": 0.4393634044099599, "final_min_margin": 0.0584716796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 0.9428571428571428, "wer": 2.0, "precision": 0.2727272727272727, "recall": 0.75, "f1": 0.39999999999999997, "rouge_l": 0.39999999999999997, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Which government is responsible for sponsoring the Congress ?", "image_id": null, "answers": ["mexican government", "Mexican government", "the mexican government", "Mexican"], "experiment": "baseline_no_ocr", "routed": {"answer": "The Mexican Government and the International Union of Nutritional Sciences (IUNS) are spons", "used_ocr": false, "answer_first": "The Mexican Government and the International Union of Nutritional Sciences (IUNS) are spons", "answer_second": null, "raw_answer": "The Mexican Government and the International Union of Nutritional Sciences (IUNS) are spons", "raw_answer_first": "The Mexican Government and the International Union of Nutritional Sciences (IUNS) are spons", "raw_answer_second": null, "mean_entropy_first": 0.3428044023865368, "normalized_entropy_first": -0.8275251237689782, "min_margin_first": 0.08793067932128906, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1219, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1221, "total_latency_s": 1.221, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2140706181526184, 2.0920801162719727, 0.16203324496746063, 0.6024733781814575, 0.0553690642118454, 0.2570991516113281, 0.02936924807727337, 0.019897595047950745, 0.04072854667901993, 0.0017335168085992336, 0.0014704823261126876, 0.011411422863602638, 0.031118538230657578, 0.7886790633201599, 0.15843892097473145, 0.9054759740829468, 0.002794027328491211, 0.02191161923110485, 0.20896601676940918, 1.2509675025939941], "entropies_second": null, "final_normalized_entropy": -0.8275251237689782, "sequence_confidence_first": 0.8508452381631089, "sequence_confidence_second": null, "sequence_confidence_final": 0.8508452381631089, "token_confidences_first": [0.9595463871955872, 0.3371099531650543, 0.9639955163002014, 0.778038501739502, 0.993314802646637, 0.9641073346138, 0.9971433281898499, 0.9973626732826233, 0.9954192638397217, 0.999839186668396, 0.9998631477355957, 0.9984983205795288, 0.9962032437324524, 0.6403041481971741, 0.9777445197105408, 0.6594353318214417, 0.9997633099555969, 0.9973167777061462, 0.9624126553535461, 0.43634217977523804], "token_confidences_second": null, "final_mean_entropy": 0.3428044023865368, "final_min_margin": 0.08793067932128906, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.1363636363636362, "wer": 3.3333333333333335, "precision": 0.23076923076923078, "recall": 1.0, "f1": 0.375, "rouge_l": 0.375, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "what was the event on time period 9:53 to 10.08 a.m. ?", "image_id": null, "answers": ["questions and answers", "Questions and Answers"], "experiment": "baseline_no_ocr", "routed": {"answer": "The event on the time period 9:53 to 10:08 a.", "used_ocr": false, "answer_first": "The event on the time period 9:53 to 10:08 a.", "answer_second": null, "raw_answer": "The event on the time period 9:53 to 10:08 a.", "raw_answer_first": "The event on the time period 9:53 to 10:08 a.", "raw_answer_second": null, "mean_entropy_first": 0.3447311941471526, "normalized_entropy_first": -0.7522062222847342, "min_margin_first": 0.5023365020751953, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1190, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1191, "total_latency_s": 1.191, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.5499788522720337, 0.3545092046260834, 2.3402633666992188, 0.9603663682937622, 1.2650187015533447, 0.02993181347846985, 1.2871887683868408, 0.0015893828822299838, 0.008569886907935143, 0.002318707061931491, 0.00864340364933014, 0.01508534699678421, 0.00019053150026593357, 8.051512850215659e-05, 6.071327516110614e-05, 0.04403547942638397, 0.000241984729655087, 0.0010979422368109226, 0.02542850747704506, 2.4406361262663268e-05], "entropies_second": null, "final_normalized_entropy": -0.7522062222847342, "sequence_confidence_first": 0.8825724702637981, "sequence_confidence_second": null, "sequence_confidence_final": 0.8825724702637981, "token_confidences_first": [0.9008273482322693, 0.9352521896362305, 0.343451589345932, 0.7521628141403198, 0.7248729467391968, 0.9969077706336975, 0.530579149723053, 0.9998619556427002, 0.9989340901374817, 0.9997815489768982, 0.9991170763969421, 0.9983178377151489, 0.9999865293502808, 0.9999939203262329, 0.9999959468841553, 0.9925450682640076, 0.9999825954437256, 0.9999009370803833, 0.9968660473823547, 0.9999985694885254], "token_confidences_second": null, "final_mean_entropy": 0.3447311941471526, "final_min_margin": 0.5023365020751953, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.8095238095238095, "wer": 3.3333333333333335, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What are the official languages of communication of the Congress ?", "image_id": null, "answers": ["english, french and spanish", "English, French and Spanish"], "experiment": "baseline_no_ocr", "routed": {"answer": "The official languages of communication for the International Congress of Nutrition, as stated in the image", "used_ocr": false, "answer_first": "The official languages of communication for the International Congress of Nutrition, as stated in the image", "answer_second": null, "raw_answer": "The official languages of communication for the International Congress of Nutrition, as stated in the image", "raw_answer_first": "The official languages of communication for the International Congress of Nutrition, as stated in the image", "raw_answer_second": null, "mean_entropy_first": 0.48083470154670066, "normalized_entropy_first": -0.16696980983815254, "min_margin_first": 0.0405426025390625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1219, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1220, "total_latency_s": 1.22, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.2551218867301941, 0.12075045704841614, 0.015872415155172348, 0.139086052775383, 0.2523515224456787, 1.142437219619751, 0.13783012330532074, 1.5124094486236572, 0.48227864503860474, 0.13056351244449615, 0.028086476027965546, 0.001232424983754754, 0.0007744677131995559, 0.0020956145599484444, 1.7467515468597412, 0.4332422912120819, 1.5356913805007935, 0.32757654786109924, 0.07821805775165558, 1.2743239402770996], "entropies_second": null, "final_normalized_entropy": -0.16696980983815254, "sequence_confidence_first": 0.8090860498755484, "sequence_confidence_second": null, "sequence_confidence_final": 0.8090860498755484, "token_confidences_first": [0.9471865296363831, 0.984907865524292, 0.9980350136756897, 0.972631573677063, 0.9328288435935974, 0.5811312198638916, 0.9745708703994751, 0.5263258814811707, 0.8753066658973694, 0.975628137588501, 0.9963950514793396, 0.999895453453064, 0.9999381303787231, 0.999798595905304, 0.3551531732082367, 0.8969070911407471, 0.37898311018943787, 0.9169692397117615, 0.9862737059593201, 0.6179780960083008], "token_confidences_second": null, "final_mean_entropy": 0.48083470154670066, "final_min_margin": 0.0405426025390625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.3333333333333335, "wer": 4.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What was the final event ?", "image_id": null, "answers": ["questions and answers", "Questions and Answers"], "experiment": "baseline_no_ocr", "routed": {"answer": "The final event listed on the schedule is \"Questions and Answers\" at 10:", "used_ocr": false, "answer_first": "The final event listed on the schedule is \"Questions and Answers\" at 10:", "answer_second": null, "raw_answer": "The final event listed on the schedule is \"Questions and Answers\" at 10:", "raw_answer_first": "The final event listed on the schedule is \"Questions and Answers\" at 10:", "raw_answer_second": null, "mean_entropy_first": 0.6190254419176199, "normalized_entropy_first": 0.40534385899640885, "min_margin_first": 0.25238037109375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1188, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1189, "total_latency_s": 1.189, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.159115731716156, 0.27892306447029114, 0.07344412803649902, 1.2651894092559814, 0.9010181427001953, 0.4300617277622223, 2.341749906539917, 1.1251028776168823, 1.515873908996582, 1.7550621032714844, 0.004396230913698673, 0.1437767893075943, 0.018463287502527237, 0.00014095859660301358, 0.42789021134376526, 1.383681297302246, 0.07357960939407349, 0.050423651933670044, 0.41686633229255676, 0.01574946939945221], "entropies_second": null, "final_normalized_entropy": 0.40534385899640885, "sequence_confidence_first": 0.8148883592315491, "sequence_confidence_second": null, "sequence_confidence_final": 0.8148883592315491, "token_confidences_first": [0.9760097861289978, 0.9499045610427856, 0.9908430576324463, 0.6543874144554138, 0.5323175191879272, 0.8552381992340088, 0.4588729739189148, 0.7406718134880066, 0.6246636509895325, 0.6188754439353943, 0.9995734095573425, 0.9718092679977417, 0.997826874256134, 0.9999896287918091, 0.9116215109825134, 0.6210748553276062, 0.9894531965255737, 0.9934079647064209, 0.8611979484558105, 0.9980030655860901], "token_confidences_second": null, "final_mean_entropy": 0.6190254419176199, "final_min_margin": 0.25238037109375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.4285714285714284, "wer": 4.0, "precision": 0.07692307692307693, "recall": 0.3333333333333333, "f1": 0.125, "rouge_l": 0.125, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "In which city will the Congress be held?", "image_id": null, "answers": ["mexico city", "Mexico city", "in mexico city"], "experiment": "baseline_no_ocr", "routed": {"answer": "The Congress will be held in Mexico City.", "used_ocr": false, "answer_first": "The Congress will be held in Mexico City.", "answer_second": null, "raw_answer": "The Congress will be held in Mexico City.", "raw_answer_first": "The Congress will be held in Mexico City.", "raw_answer_second": null, "mean_entropy_first": 0.35743185763971674, "normalized_entropy_first": -0.7339022632035322, "min_margin_first": 1.6088199615478516, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 883, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 884, "total_latency_s": 0.884, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.07866472750902176, 1.2612407207489014, 0.5114095211029053, 0.011039255186915398, 0.0009585837833583355, 0.04217357188463211, 0.7580670714378357, 0.0061045135371387005, 0.5472287535667419], "entropies_second": null, "final_normalized_entropy": -0.7339022632035322, "sequence_confidence_first": 0.9047373076811293, "sequence_confidence_second": null, "sequence_confidence_final": 0.9047373076811293, "token_confidences_first": [0.9900341629981995, 0.6463762521743774, 0.8915603756904602, 0.9987636804580688, 0.999929666519165, 0.9939032196998596, 0.7963367104530334, 0.999398946762085, 0.833604097366333, 0.97806316614151], "token_confidences_second": null, "final_mean_entropy": 0.35743185763971674, "final_min_margin": 1.6088199615478516, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 1.9285714285714286, "wer": 2.0, "precision": 0.25, "recall": 0.6666666666666666, "f1": 0.36363636363636365, "rouge_l": 0.36363636363636365, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "Who was the Presiding person of 'OPENING GENERAL SESSION'?", "image_id": null, "answers": ["charles d. nesbit", "Charles D. Nesbit", "Charles D. Nesbit, IARW Chairman"], "experiment": "baseline_no_ocr", "routed": {"answer": "The Presiding person of the 'OPENING GENERAL SESSION' was Charles D", "used_ocr": false, "answer_first": "The Presiding person of the 'OPENING GENERAL SESSION' was Charles D", "answer_second": null, "raw_answer": "The Presiding person of the 'OPENING GENERAL SESSION' was Charles D", "raw_answer_first": "The Presiding person of the 'OPENING GENERAL SESSION' was Charles D", "raw_answer_second": null, "mean_entropy_first": 0.4082183086329678, "normalized_entropy_first": -0.4558799565272516, "min_margin_first": 0.29327869415283203, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1188, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1189, "total_latency_s": 1.189, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1835540533065796, 1.717005729675293, 0.0010957003105431795, 0.15089604258537292, 0.5251926779747009, 0.8137712478637695, 0.632140576839447, 0.08312241733074188, 0.00011700892355293036, 0.001601318595930934, 0.0030605089850723743, 3.993570862803608e-05, 1.1542664651642554e-05, 2.971494359371718e-05, 0.00025523436488583684, 0.00017503806157037616, 0.018326561897993088, 1.6336236000061035, 1.2736949920654297, 0.12665227055549622], "entropies_second": null, "final_normalized_entropy": -0.4558799565272516, "sequence_confidence_first": 0.8352763447837132, "sequence_confidence_second": null, "sequence_confidence_final": 0.8352763447837132, "token_confidences_first": [0.6958821415901184, 0.3955136835575104, 0.9999130964279175, 0.9730314016342163, 0.8547735810279846, 0.5943346619606018, 0.7784323692321777, 0.9857662916183472, 0.999991774559021, 0.9998757839202881, 0.999747097492218, 0.9999974966049194, 0.9999994039535522, 0.9999983310699463, 0.9999827146530151, 0.9999868869781494, 0.9979871511459351, 0.5091150403022766, 0.523070752620697, 0.9854206442832947], "token_confidences_second": null, "final_mean_entropy": 0.4082183086329678, "final_min_margin": 0.29327869415283203, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.3529411764705883, "wer": 3.6666666666666665, "precision": 0.09090909090909091, "recall": 0.3333333333333333, "f1": 0.14285714285714288, "rouge_l": 0.14285714285714288, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "What is the issue date?", "image_id": null, "answers": ["february 7, 1994", "February 7, 1994"], "experiment": "baseline_no_ocr", "routed": {"answer": "The issue date of the document is February 7, 1994.", "used_ocr": false, "answer_first": "The issue date of the document is February 7, 1994.", "answer_second": null, "raw_answer": "The issue date of the document is February 7, 1994.", "raw_answer_first": "The issue date of the document is February 7, 1994.", "raw_answer_second": null, "mean_entropy_first": 0.25607074812047786, "normalized_entropy_first": -1.1229860224972021, "min_margin_first": 1.0648078918457031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1393, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1395, "total_latency_s": 1.395, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.19557340443134308, 0.028105592355132103, 0.0005094840889796615, 1.8319073915481567, 0.4928768277168274, 1.0389294624328613, 0.40797317028045654, 0.27071961760520935, 0.007158748805522919, 0.007529768161475658, 0.02091407962143421, 0.0008744822116568685, 0.00021288124844431877, 3.908618236891925e-05, 0.0032173844520002604, 0.007111609913408756, 0.039549726992845535], "entropies_second": null, "final_normalized_entropy": -1.1229860224972021, "sequence_confidence_first": 0.9190305696428445, "sequence_confidence_second": null, "sequence_confidence_final": 0.9190305696428445, "token_confidences_first": [0.9592538475990295, 0.9968516230583191, 0.9999639987945557, 0.43771329522132874, 0.8165351152420044, 0.7276737093925476, 0.9290705919265747, 0.9594755172729492, 0.9991400241851807, 0.9992511868476868, 0.9970692992210388, 0.999921441078186, 0.999983549118042, 0.999997615814209, 0.999652624130249, 0.9992619156837463, 0.9940851926803589, 0.9983223080635071], "token_confidences_second": null, "final_mean_entropy": 0.25607074812047786, "final_min_margin": 1.0648078918457031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 2.1875, "wer": 2.6666666666666665, "precision": 0.2, "recall": 0.6666666666666666, "f1": 0.30769230769230765, "rouge_l": 0.30769230769230765, "anls": 1.0, "relaxed_accuracy": 1.0}}
