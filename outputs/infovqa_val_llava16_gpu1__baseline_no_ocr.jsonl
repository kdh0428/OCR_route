{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic that outlines various statistics", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic that outlines various statistics", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic that outlines various statistics", "raw_answer_first": "The image you've provided appears to be a graphic or infographic that outlines various statistics", "raw_answer_second": null, "mean_entropy_first": 1.0040595139209472, "normalized_entropy_first": 0.0, "min_margin_first": 0.06568527221679688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1289, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1291, "total_latency_s": 1.291, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2473254203796387, 0.036340489983558655, 1.340126633644104, 0.2962206304073334, 0.0034103321377187967, 0.6897839307785034, 0.988472580909729, 0.00012625701492652297, 0.04358694329857826, 0.36846283078193665, 3.3244495391845703, 0.3405956029891968, 1.6133930683135986, 2.5177111625671387, 0.015084482729434967, 2.568286895751953, 2.6263785362243652, 0.00019867900118697435, 1.0478428602218628, 1.0133934020996094], "entropies_second": null, "final_normalized_entropy": 0.0, "sequence_confidence_first": 0.6442665617641415, "sequence_confidence_second": null, "sequence_confidence_final": 0.6442665617641415, "token_confidences_first": [0.5045342445373535, 0.996206521987915, 0.6079142689704895, 0.9322028160095215, 0.9996668100357056, 0.6695553660392761, 0.623001217842102, 0.9999922513961792, 0.9939298629760742, 0.8982290625572205, 0.1541137993335724, 0.9010429382324219, 0.6158211827278137, 0.3577730357646942, 0.9981745481491089, 0.25643858313560486, 0.2781918942928314, 0.9999856948852539, 0.7941819429397583, 0.8274686336517334], "token_confidences_second": null, "final_mean_entropy": 1.0040595139209472, "final_min_margin": 0.06568527221679688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a promotional or informational graphic related to", "used_ocr": false, "answer_first": "The image you've provided appears to be a promotional or informational graphic related to", "answer_second": null, "raw_answer": "The image you've provided appears to be a promotional or informational graphic related to", "raw_answer_first": "The image you've provided appears to be a promotional or informational graphic related to", "raw_answer_second": null, "mean_entropy_first": 0.6535175091117708, "normalized_entropy_first": -350.5420048091764, "min_margin_first": 0.07353019714355469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1183, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1184, "total_latency_s": 1.184, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8125138282775879, 0.06363604217767715, 1.3698633909225464, 0.26444438099861145, 0.003217986086383462, 0.7056074142456055, 0.82651686668396, 5.454615893540904e-05, 0.021220900118350983, 0.540946364402771, 3.2395081520080566, 0.0020030452869832516, 0.0004949412541463971, 1.743751049041748, 0.46486711502075195, 0.4473450481891632, 0.8046303391456604, 0.004252550192177296, 1.7552127838134766, 0.00026343821082264185], "entropies_second": null, "final_normalized_entropy": -350.5420048091764, "sequence_confidence_first": 0.7366097481462781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7366097481462781, "token_confidences_first": [0.6658463478088379, 0.992685079574585, 0.5677998661994934, 0.9424121975898743, 0.9996770620346069, 0.6536072492599487, 0.5930841565132141, 0.9999967813491821, 0.997244119644165, 0.8063528537750244, 0.19020451605319977, 0.9998229146003723, 0.999964714050293, 0.365521639585495, 0.8738417625427246, 0.8381020426750183, 0.7793771028518677, 0.9995582699775696, 0.506045937538147, 0.9999823570251465], "token_confidences_second": null, "final_mean_entropy": 0.6535175091117708, "final_min_margin": 0.07353019714355469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "ANSWER", "used_ocr": false, "answer_first": "ANSWER", "answer_second": null, "raw_answer": "ANSWER: [ANSWER]", "raw_answer_first": "ANSWER: [ANSWER]", "raw_answer_second": null, "mean_entropy_first": 0.6201666116481647, "normalized_entropy_first": -3.3171364820388027, "min_margin_first": 0.12775039672851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 846, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 849, "total_latency_s": 0.849, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4491167068481445, 0.03513430058956146, 0.004054988734424114, 0.002924592001363635, 0.024687882512807846, 1.5238752365112305, 3.119272232055664, 0.006623213645070791, 0.0038453475572168827, 0.0321316160261631], "entropies_second": null, "final_normalized_entropy": -3.3171364820388027, "sequence_confidence_first": 0.8207633556730987, "sequence_confidence_second": null, "sequence_confidence_final": 0.8207633556730987, "token_confidences_first": [0.43985074758529663, 0.9968482851982117, 0.9996544122695923, 0.9997356534004211, 0.9972004890441895, 0.5036923885345459, 0.5202680230140686, 0.9994010925292969, 0.9996252059936523, 0.9972442388534546, 0.9981010556221008], "token_confidences_second": null, "final_mean_entropy": 0.6201666116481647, "final_min_margin": 0.12775039672851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to the impact of", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to the impact of", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to the impact of", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to the impact of", "raw_answer_second": null, "mean_entropy_first": 0.8197903697551737, "normalized_entropy_first": -0.7907459231576766, "min_margin_first": 0.10391616821289062, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1234, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1238, "total_latency_s": 1.238, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8592228293418884, 0.07828137278556824, 1.4901095628738403, 0.26763901114463806, 0.005656343884766102, 0.700332522392273, 0.8666965961456299, 4.779584924108349e-05, 0.021548671647906303, 0.7376152873039246, 2.9128551483154297, 0.5401484370231628, 1.4607481956481934, 1.9143235683441162, 0.013736098073422909, 1.9926751852035522, 0.0001539699442218989, 1.0352541208267212, 1.4484095573425293, 0.050353121012449265], "entropies_second": null, "final_normalized_entropy": -0.7907459231576766, "sequence_confidence_first": 0.7131811916641617, "sequence_confidence_second": null, "sequence_confidence_final": 0.7131811916641617, "token_confidences_first": [0.5347285866737366, 0.9894814491271973, 0.5419051051139832, 0.941577672958374, 0.999428927898407, 0.6595543622970581, 0.5101087093353271, 0.9999971389770508, 0.997514009475708, 0.5832636952400208, 0.35236868262290955, 0.7836803793907166, 0.7226975560188293, 0.5022388696670532, 0.9983262419700623, 0.4902784824371338, 0.9999901056289673, 0.7219867706298828, 0.623668372631073, 0.9931893348693848], "token_confidences_second": null, "final_mean_entropy": 0.8197903697551737, "final_min_margin": 0.10391616821289062, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a brief history of social media platforms, presented in a timeline format.", "used_ocr": false, "answer_first": "This image appears to be a brief history of social media platforms, presented in a timeline format.", "answer_second": null, "raw_answer": "This image appears to be a brief history of social media platforms, presented in a timeline format.", "raw_answer_first": "This image appears to be a brief history of social media platforms, presented in a timeline format.", "raw_answer_second": null, "mean_entropy_first": 0.9765182551476755, "normalized_entropy_first": 0.3807193760625947, "min_margin_first": 0.025058746337890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1337, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1339, "total_latency_s": 1.339, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0467885732650757, 0.9102826118469238, 1.6138906478881836, 0.0003240264195483178, 0.044431865215301514, 0.7049456834793091, 2.4501752853393555, 1.014910101890564, 0.6710249781608582, 0.18948954343795776, 0.0006904772599227726, 0.04265013337135315, 1.0345113277435303, 3.6040823459625244, 1.0494316816329956, 0.7598092555999756, 2.9225568771362305, 0.009809484705328941, 0.5847327709197998, 0.875827431678772], "entropies_second": null, "final_normalized_entropy": 0.3807193760625947, "sequence_confidence_first": 0.6330561011204244, "sequence_confidence_second": null, "sequence_confidence_final": 0.6330561011204244, "token_confidences_first": [0.46482914686203003, 0.573550820350647, 0.3636910915374756, 0.9999737739562988, 0.9949038028717041, 0.6274253129959106, 0.33025291562080383, 0.7503173351287842, 0.8420045971870422, 0.9710896015167236, 0.9999467134475708, 0.9955877065658569, 0.7346594333648682, 0.116425059735775, 0.6348451972007751, 0.7998454570770264, 0.28101813793182373, 0.9990045428276062, 0.8950810432434082, 0.8020877838134766], "token_confidences_second": null, "final_mean_entropy": 0.9765182551476755, "final_min_margin": 0.025058746337890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic that appears to be from a research project or report", "used_ocr": false, "answer_first": "The image you've provided is a graphic that appears to be from a research project or report", "answer_second": null, "raw_answer": "The image you've provided is a graphic that appears to be from a research project or report", "raw_answer_first": "The image you've provided is a graphic that appears to be from a research project or report", "raw_answer_second": null, "mean_entropy_first": 1.2522259527584538, "normalized_entropy_first": 2.39931529751356, "min_margin_first": 0.009664535522460938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1336, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1337, "total_latency_s": 1.337, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7951995134353638, 0.13127654790878296, 1.69066321849823, 0.22010651230812073, 0.0023070506285876036, 0.7523622512817383, 0.8991255164146423, 0.9079734086990356, 2.708627223968506, 0.5099163055419922, 2.844661235809326, 2.409085273742676, 0.002504433272406459, 1.1183898448944092, 2.2673630714416504, 0.8296520709991455, 1.983137607574463, 0.9179693460464478, 1.959720492362976, 2.094478130340576], "entropies_second": null, "final_normalized_entropy": 2.39931529751356, "sequence_confidence_first": 0.5306361048130829, "sequence_confidence_second": null, "sequence_confidence_final": 0.5306361048130829, "token_confidences_first": [0.5804260969161987, 0.9786359667778015, 0.3984537124633789, 0.9549697637557983, 0.9997639060020447, 0.563174307346344, 0.5256689190864563, 0.5655511617660522, 0.3836781680583954, 0.8027178049087524, 0.24615950882434845, 0.2614765763282776, 0.9997969269752502, 0.6029559373855591, 0.20314809679985046, 0.782258152961731, 0.33942297101020813, 0.733630895614624, 0.5365561842918396, 0.3412731885910034], "token_confidences_second": null, "final_mean_entropy": 1.2522259527584538, "final_min_margin": 0.009664535522460938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to healthcare and", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to healthcare and", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to healthcare and", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to healthcare and", "raw_answer_second": null, "mean_entropy_first": 0.8818273892436992, "normalized_entropy_first": -0.4889130804353524, "min_margin_first": 0.03394126892089844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1291, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1295, "total_latency_s": 1.295, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.025993824005127, 0.05004606395959854, 1.3682788610458374, 0.28025466203689575, 0.0033334344625473022, 0.7023671865463257, 0.9168649911880493, 9.65478247962892e-05, 0.02424847148358822, 0.6936192512512207, 3.369722366333008, 0.26108312606811523, 1.3044987916946411, 2.4341962337493896, 0.011822152882814407, 2.2646448612213135, 0.00013660467811860144, 1.5037157535552979, 0.12709790468215942, 1.2945266962051392], "entropies_second": null, "final_normalized_entropy": -0.4889130804353524, "sequence_confidence_first": 0.66607707682983, "sequence_confidence_second": null, "sequence_confidence_final": 0.66607707682983, "token_confidences_first": [0.5464200377464294, 0.9942317605018616, 0.600750744342804, 0.9380779266357422, 0.9996588230133057, 0.6443992853164673, 0.5479298233985901, 0.9999940395355225, 0.9968767166137695, 0.6273706555366516, 0.19726309180259705, 0.9304287433624268, 0.7290988564491272, 0.23413844406604767, 0.9985981583595276, 0.4320785105228424, 0.9999914169311523, 0.4371226727962494, 0.9768970608711243, 0.7572817206382751], "token_confidences_second": null, "final_mean_entropy": 0.8818273892436992, "final_min_margin": 0.03394126892089844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a salary guide for social media jobs in the United States.", "used_ocr": false, "answer_first": "The image you've provided is a salary guide for social media jobs in the United States.", "answer_second": null, "raw_answer": "The image you've provided is a salary guide for social media jobs in the United States.", "raw_answer_first": "The image you've provided is a salary guide for social media jobs in the United States.", "raw_answer_second": null, "mean_entropy_first": 0.6778413798567271, "normalized_entropy_first": -1.7791579830067212, "min_margin_first": 0.18651390075683594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1180, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1181, "total_latency_s": 1.181, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9744505882263184, 0.07110393047332764, 1.5734057426452637, 0.26262935996055603, 0.0026725383941084146, 0.689913272857666, 0.8021138310432434, 0.8176734447479248, 2.975107431411743, 4.810495374840684e-05, 0.06543498486280441, 1.1441307067871094, 0.7170368432998657, 0.0004578124498948455, 0.2997265160083771, 1.6091020107269287, 0.4595857858657837, 0.25196772813796997, 0.0007804845226928592, 0.8394864797592163], "entropies_second": null, "final_normalized_entropy": -1.7791579830067212, "sequence_confidence_first": 0.7391192330400403, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391192330400403, "token_confidences_first": [0.517148494720459, 0.9918403625488281, 0.5357896685600281, 0.9414313435554504, 0.9997349381446838, 0.6687652468681335, 0.5416426658630371, 0.7382591962814331, 0.2011493593454361, 0.9999969005584717, 0.9922388792037964, 0.7838777899742126, 0.8281567096710205, 0.9999655485153198, 0.9383668303489685, 0.48004093766212463, 0.9051236510276794, 0.9547756314277649, 0.9999345541000366, 0.678669810295105], "token_confidences_second": null, "final_mean_entropy": 0.6778413798567271, "final_min_margin": 0.18651390075683594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This is a colorful calendar for the year 2018, specifically designed for social media", "used_ocr": false, "answer_first": "This is a colorful calendar for the year 2018, specifically designed for social media", "answer_second": null, "raw_answer": "This is a colorful calendar for the year 2018, specifically designed for social media", "raw_answer_first": "This is a colorful calendar for the year 2018, specifically designed for social media", "raw_answer_second": null, "mean_entropy_first": 0.9383604382876001, "normalized_entropy_first": 0.078621688831557, "min_margin_first": 0.031009674072265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1336, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1338, "total_latency_s": 1.338, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1120718717575073, 0.7699912786483765, 0.5608746409416199, 2.067239999771118, 0.1934879571199417, 2.0432028770446777, 2.0132689476013184, 0.9819005131721497, 0.5902202129364014, 0.005812718998640776, 5.192045500734821e-05, 4.382605038699694e-05, 0.005287627689540386, 0.02023523859679699, 2.0078413486480713, 2.519803047180176, 2.0946080684661865, 0.5418830513954163, 1.2298405170440674, 0.009543102234601974], "entropies_second": null, "final_normalized_entropy": 0.078621688831557, "sequence_confidence_first": 0.6834546245468484, "sequence_confidence_second": null, "sequence_confidence_final": 0.6834546245468484, "token_confidences_first": [0.4631752669811249, 0.5328591465950012, 0.8132816553115845, 0.345740407705307, 0.9571990370750427, 0.4194180965423584, 0.4884262681007385, 0.633376955986023, 0.8823779225349426, 0.9993917942047119, 0.9999964237213135, 0.9999971389770508, 0.9993770718574524, 0.9974276423454285, 0.4516693949699402, 0.46355751156806946, 0.4909646809101105, 0.840269148349762, 0.7563849091529846, 0.9990792274475098], "token_confidences_second": null, "final_mean_entropy": 0.9383604382876001, "final_min_margin": 0.031009674072265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a poster or infographic related to social media impact on", "used_ocr": false, "answer_first": "The image you've provided appears to be a poster or infographic related to social media impact on", "answer_second": null, "raw_answer": "The image you've provided appears to be a poster or infographic related to social media impact on", "raw_answer_first": "The image you've provided appears to be a poster or infographic related to social media impact on", "raw_answer_second": null, "mean_entropy_first": 0.9024706868243811, "normalized_entropy_first": -0.15039678932139183, "min_margin_first": 0.1622905731201172, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1060, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1066, "total_latency_s": 1.066, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1418606042861938, 0.0868847668170929, 1.7143127918243408, 0.2781217694282532, 0.0018767069559544325, 0.7269100546836853, 1.0121374130249023, 0.00020332243002485484, 0.040099188685417175, 0.6838460564613342, 2.6653261184692383, 0.4574604630470276, 2.0785136222839355, 0.016217030584812164, 2.342714786529541, 0.000202239491045475, 1.1125435829162598, 0.017866667360067368, 1.9586524963378906, 1.7136640548706055], "entropies_second": null, "final_normalized_entropy": -0.15039678932139183, "sequence_confidence_first": 0.6570937120320218, "sequence_confidence_second": null, "sequence_confidence_final": 0.6570937120320218, "token_confidences_first": [0.6313332915306091, 0.9890692830085754, 0.42144790291786194, 0.9383177757263184, 0.9998226761817932, 0.6630375385284424, 0.6165451407432556, 0.9999872446060181, 0.9947645664215088, 0.7018365263938904, 0.2765548825263977, 0.9269630908966064, 0.2959694266319275, 0.9979411959648132, 0.3468559980392456, 0.9999873638153076, 0.7129108309745789, 0.9982807636260986, 0.41506874561309814, 0.4119485914707184], "token_confidences_second": null, "final_mean_entropy": 0.9024706868243811, "final_min_margin": 0.1622905731201172, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a visual representation of the \"State of Social Media 20", "used_ocr": false, "answer_first": "The image you've provided is a visual representation of the \"State of Social Media 20", "answer_second": null, "raw_answer": "The image you've provided is a visual representation of the \"State of Social Media 20", "raw_answer_first": "The image you've provided is a visual representation of the \"State of Social Media 20", "raw_answer_second": null, "mean_entropy_first": 0.9212502389258589, "normalized_entropy_first": -0.01857830940275576, "min_margin_first": 0.15719985961914062, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1237, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1242, "total_latency_s": 1.242, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.178861141204834, 0.08296379446983337, 1.5098551511764526, 0.26087895035743713, 0.005759468302130699, 0.7359864711761475, 0.9163696765899658, 0.5354756712913513, 3.2562713623046875, 2.205258846282959, 0.5744473934173584, 1.9535731077194214, 2.617952346801758, 1.2515170574188232, 0.05904729664325714, 0.06983407586812973, 0.012914812192320824, 1.1955838203430176, 0.0019743323791772127, 0.00048000257811509073], "entropies_second": null, "final_normalized_entropy": -0.01857830940275576, "sequence_confidence_first": 0.6616837170864607, "sequence_confidence_second": null, "sequence_confidence_final": 0.6616837170864607, "token_confidences_first": [0.4909537732601166, 0.9905845522880554, 0.5796287655830383, 0.9438018202781677, 0.9993963241577148, 0.6428612470626831, 0.5174102783203125, 0.8655679225921631, 0.18341156840324402, 0.36107757687568665, 0.8982999324798584, 0.397665411233902, 0.4518532156944275, 0.7077656388282776, 0.9901161789894104, 0.991300642490387, 0.9987198114395142, 0.45599043369293213, 0.9998290538787842, 0.9999651908874512], "token_confidences_second": null, "final_mean_entropy": 0.9212502389258589, "final_min_margin": 0.15719985961914062, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a publication or a report, specifically foc", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a publication or a report, specifically foc", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a publication or a report, specifically foc", "raw_answer_first": "The image you've provided appears to be a page from a publication or a report, specifically foc", "raw_answer_second": null, "mean_entropy_first": 1.1236845869530954, "normalized_entropy_first": 1.3906262807064438, "min_margin_first": 0.26200103759765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1347, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7897266149520874, 0.019103532657027245, 1.6051576137542725, 0.2370195835828781, 0.0027378404047340155, 0.6829816699028015, 0.8131086826324463, 3.624536475399509e-05, 0.01157880388200283, 0.6827639937400818, 2.32125186920166, 0.2013123482465744, 0.34563347697257996, 2.061387062072754, 1.4201794862747192, 1.7871904373168945, 2.688717842102051, 2.4621896743774414, 2.064066171646118, 2.2775487899780273], "entropies_second": null, "final_normalized_entropy": 1.3906262807064438, "sequence_confidence_first": 0.6049367143196217, "sequence_confidence_second": null, "sequence_confidence_final": 0.6049367143196217, "token_confidences_first": [0.571570634841919, 0.9981673955917358, 0.39322030544281006, 0.9497188329696655, 0.9997215867042542, 0.6727036237716675, 0.5521339178085327, 0.9999978542327881, 0.9986238479614258, 0.8053450584411621, 0.4403688311576843, 0.9630887508392334, 0.9280796051025391, 0.3993639647960663, 0.5541916489601135, 0.48347386717796326, 0.30396488308906555, 0.25629109144210815, 0.42753735184669495, 0.48257339000701904], "token_confidences_second": null, "final_mean_entropy": 1.1236845869530954, "final_min_margin": 0.26200103759765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to the future of", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to the future of", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to the future of", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to the future of", "raw_answer_second": null, "mean_entropy_first": 0.7729824744877988, "normalized_entropy_first": -1.1464374446102132, "min_margin_first": 0.45868873596191406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1189, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1190, "total_latency_s": 1.19, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9379072189331055, 0.0527571365237236, 1.4055585861206055, 0.24844247102737427, 0.004581007175147533, 0.7169687747955322, 0.8405572175979614, 6.749553722329438e-05, 0.01897415891289711, 0.6795810461044312, 3.0914509296417236, 0.4394744634628296, 1.2785248756408691, 1.9557974338531494, 0.011871588416397572, 1.9892723560333252, 0.00014884467236697674, 0.7448869943618774, 0.9010536670684814, 0.14177322387695312], "entropies_second": null, "final_normalized_entropy": -1.1464374446102132, "sequence_confidence_first": 0.7435550767615531, "sequence_confidence_second": null, "sequence_confidence_final": 0.7435550767615531, "token_confidences_first": [0.6492300033569336, 0.9939596652984619, 0.5828936696052551, 0.946852445602417, 0.9995229244232178, 0.6558442115783691, 0.612156331539154, 0.9999959468841553, 0.9978156089782715, 0.6570581793785095, 0.3286130130290985, 0.8497571349143982, 0.7388449907302856, 0.42708832025527954, 0.9985694885253906, 0.5177043080329895, 0.9999902248382568, 0.8020686507225037, 0.7950145602226257, 0.9804437160491943], "token_confidences_second": null, "final_mean_entropy": 0.7729824744877988, "final_min_margin": 0.45868873596191406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a chart that lists the types of", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a chart that lists the types of", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a chart that lists the types of", "raw_answer_first": "The image you've provided appears to be an infographic or a chart that lists the types of", "raw_answer_second": null, "mean_entropy_first": 1.1379097249882761, "normalized_entropy_first": 1.4049918020717522, "min_margin_first": 0.028690338134765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1290, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1291, "total_latency_s": 1.291, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8755019307136536, 0.07202501595020294, 1.8261778354644775, 0.2737160921096802, 0.004047741182148457, 0.7247317433357239, 1.0109888315200806, 0.0002117204712703824, 0.06893078982830048, 0.744162917137146, 0.4837140440940857, 0.012912214733660221, 2.552593469619751, 2.2233448028564453, 2.793229579925537, 2.85176420211792, 2.6499922275543213, 2.301103353500366, 1.2837724685668945, 0.005273519083857536], "entropies_second": null, "final_normalized_entropy": 1.4049918020717522, "sequence_confidence_first": 0.5827771656830808, "sequence_confidence_second": null, "sequence_confidence_final": 0.5827771656830808, "token_confidences_first": [0.5972214937210083, 0.9910197854042053, 0.36033234000205994, 0.9390000700950623, 0.9995629191398621, 0.6545714735984802, 0.4782876670360565, 0.9999849796295166, 0.990918755531311, 0.5040327310562134, 0.8990408182144165, 0.9984766840934753, 0.3483683466911316, 0.3529978096485138, 0.3882068693637848, 0.27093735337257385, 0.21584609150886536, 0.35952651500701904, 0.7244806289672852, 0.9994365572929382], "token_confidences_second": null, "final_mean_entropy": 1.1379097249882761, "final_min_margin": 0.028690338134765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to the topic of G", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to the topic of G", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to the topic of G", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to the topic of G", "raw_answer_second": null, "mean_entropy_first": 1.2711725343586295, "normalized_entropy_first": 2.0719985014196074, "min_margin_first": 0.08714103698730469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1027, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1029, "total_latency_s": 1.029, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9989275932312012, 0.03235765919089317, 1.1463310718536377, 0.3585955798625946, 0.0073576136492192745, 0.6636326313018799, 0.8770767450332642, 0.00010823810589499772, 0.04555749148130417, 0.8530430197715759, 3.790914297103882, 0.40925878286361694, 1.624571681022644, 2.5375823974609375, 2.28922963142395, 0.00038801872869953513, 3.2210073471069336, 3.2929155826568604, 0.07635574042797089, 3.19823956489563], "entropies_second": null, "final_normalized_entropy": 2.0719985014196074, "sequence_confidence_first": 0.5588554141185671, "sequence_confidence_second": null, "sequence_confidence_final": 0.5588554141185671, "token_confidences_first": [0.6023445725440979, 0.9968326687812805, 0.6632325649261475, 0.9142433404922485, 0.9993228912353516, 0.7152685523033142, 0.7020746469497681, 0.9999935626983643, 0.9939990043640137, 0.508134126663208, 0.11617777496576309, 0.8635566830635071, 0.688291072845459, 0.31445959210395813, 0.316733717918396, 0.9999740123748779, 0.18410469591617584, 0.323629766702652, 0.9863212704658508, 0.23676033318042755], "token_confidences_second": null, "final_mean_entropy": 1.2711725343586295, "final_min_margin": 0.08714103698730469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a promotional graphic or advertisement for", "used_ocr": false, "answer_first": "The image you've provided appears to be a promotional graphic or advertisement for", "answer_second": null, "raw_answer": "The image you've provided appears to be a promotional graphic or advertisement for", "raw_answer_first": "The image you've provided appears to be a promotional graphic or advertisement for", "raw_answer_second": null, "mean_entropy_first": 0.7296528455051885, "normalized_entropy_first": -1.415234491713201, "min_margin_first": 0.019189834594726562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1244, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1251, "total_latency_s": 1.251, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0551087856292725, 0.1376313418149948, 1.629453420639038, 0.33663269877433777, 0.0034549315460026264, 0.6583424210548401, 0.8928977847099304, 0.00020141062850598246, 0.040837518870830536, 0.7702603340148926, 3.254925489425659, 0.0020240508019924164, 0.000559744075872004, 1.9321831464767456, 0.007771408185362816, 1.1142687797546387, 1.6898810863494873, 0.004565867595374584, 0.0023536973167210817, 1.05970299243927], "entropies_second": null, "final_normalized_entropy": -1.415234491713201, "sequence_confidence_first": 0.6929240367957837, "sequence_confidence_second": null, "sequence_confidence_final": 0.6929240367957837, "token_confidences_first": [0.610156774520874, 0.9826547503471375, 0.3716824948787689, 0.9205245971679688, 0.9996416568756104, 0.7265149354934692, 0.6545701622962952, 0.9999873638153076, 0.9945810437202454, 0.513169527053833, 0.20982961356639862, 0.9998236298561096, 0.9999603033065796, 0.2925069034099579, 0.9991126656532288, 0.4807376265525818, 0.5663217902183533, 0.9995467066764832, 0.9997671246528625, 0.7843775153160095], "token_confidences_second": null, "final_mean_entropy": 0.7296528455051885, "final_min_margin": 0.019189834594726562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic related to the popular ride-sharing", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic related to the popular ride-sharing", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic related to the popular ride-sharing", "raw_answer_first": "The image you've provided appears to be an infographic related to the popular ride-sharing", "raw_answer_second": null, "mean_entropy_first": 0.7369440056070744, "normalized_entropy_first": -1.1858487165593274, "min_margin_first": 0.021392822265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1222, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1224, "total_latency_s": 1.224, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8306269645690918, 0.18588435649871826, 2.0314228534698486, 0.2938770055770874, 0.00282111088745296, 0.740925133228302, 1.09506356716156, 0.00015316410281229764, 0.06662372499704361, 0.7377848625183105, 0.47618964314460754, 0.013702446594834328, 2.6608543395996094, 0.00026759126922115684, 1.0313878059387207, 2.906614065170288, 1.078940510749817, 0.024523092433810234, 0.5610886812210083, 0.00012919300934299827], "entropies_second": null, "final_normalized_entropy": -1.1858487165593274, "sequence_confidence_first": 0.65001142305101, "sequence_confidence_second": null, "sequence_confidence_final": 0.65001142305101, "token_confidences_first": [0.5405671000480652, 0.9681189060211182, 0.25593113899230957, 0.9322484135627747, 0.9997157454490662, 0.6436454653739929, 0.46828222274780273, 0.9999903440475464, 0.9917405843734741, 0.5879102349281311, 0.883605420589447, 0.9983647465705872, 0.21753567457199097, 0.9999816417694092, 0.5774295926094055, 0.14838525652885437, 0.6562489867210388, 0.9971108436584473, 0.7682146430015564, 0.9999914169311523], "token_confidences_second": null, "final_mean_entropy": 0.7369440056070744, "final_min_margin": 0.021392822265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic representation of a speech by President Obama, summarizing key terms from", "used_ocr": false, "answer_first": "The image is a graphic representation of a speech by President Obama, summarizing key terms from", "answer_second": null, "raw_answer": "The image is a graphic representation of a speech by President Obama, summarizing key terms from", "raw_answer_first": "The image is a graphic representation of a speech by President Obama, summarizing key terms from", "raw_answer_second": null, "mean_entropy_first": 1.3815297801414999, "normalized_entropy_first": 2.4048054782230834, "min_margin_first": 0.024053573608398438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1340, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1341, "total_latency_s": 1.341, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9607316851615906, 0.09610985219478607, 1.991161823272705, 0.6663722395896912, 1.8071107864379883, 0.6031472086906433, 2.356271505355835, 2.2472052574157715, 1.9313044548034668, 2.0554354190826416, 2.802992343902588, 0.5136030912399292, 0.36927464604377747, 7.899924094090238e-05, 1.9876503944396973, 3.3165576457977295, 0.7017604112625122, 1.404404878616333, 0.4903601408004761, 1.329062819480896], "entropies_second": null, "final_normalized_entropy": 2.4048054782230834, "sequence_confidence_first": 0.5303472012205395, "sequence_confidence_second": null, "sequence_confidence_final": 0.5303472012205395, "token_confidences_first": [0.7533354163169861, 0.9880784153938293, 0.21290059387683868, 0.8504207134246826, 0.5320273637771606, 0.7200223803520203, 0.3374769985675812, 0.4516837000846863, 0.32693976163864136, 0.5578778982162476, 0.3024047613143921, 0.9000746011734009, 0.8792380690574646, 0.9999943971633911, 0.29913315176963806, 0.20833200216293335, 0.5166162252426147, 0.43486568331718445, 0.8916105031967163, 0.7227025628089905], "token_confidences_second": null, "final_mean_entropy": 1.3815297801414999, "final_min_margin": 0.024053573608398438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a colorful infographic titled \"Government Support for the", "used_ocr": false, "answer_first": "The image you've provided is a colorful infographic titled \"Government Support for the", "answer_second": null, "raw_answer": "The image you've provided is a colorful infographic titled \"Government Support for the", "raw_answer_first": "The image you've provided is a colorful infographic titled \"Government Support for the", "raw_answer_second": null, "mean_entropy_first": 0.5441875745367725, "normalized_entropy_first": -1.9532649849717147, "min_margin_first": 0.016819000244140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7694011926651001, 0.06929410994052887, 1.6519417762756348, 0.2468266487121582, 0.0027866759337484837, 0.6643735766410828, 0.849817156791687, 0.7330406308174133, 2.5550122261047363, 0.033853113651275635, 0.967083215713501, 0.014533255249261856, 1.802969217300415, 0.012311909347772598, 0.22257055342197418, 0.0023601404391229153, 0.000862681888975203, 0.15815132856369019, 0.025450170040130615, 0.10111191123723984], "entropies_second": null, "final_normalized_entropy": -1.9532649849717147, "sequence_confidence_first": 0.7754268552733758, "sequence_confidence_second": null, "sequence_confidence_final": 0.7754268552733758, "token_confidences_first": [0.49911150336265564, 0.990439772605896, 0.45138445496559143, 0.9467222690582275, 0.9997161030769348, 0.7077839970588684, 0.561635434627533, 0.6193642616271973, 0.37575194239616394, 0.9950693845748901, 0.7859963178634644, 0.998242974281311, 0.44558584690093994, 0.9987323880195618, 0.9612559676170349, 0.9997742772102356, 0.9999295473098755, 0.9651520848274231, 0.9973247051239014, 0.9838054180145264], "token_confidences_second": null, "final_mean_entropy": 0.5441875745367725, "final_min_margin": 0.016819000244140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various statistics and data points related to", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various statistics and data points related to", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various statistics and data points related to", "raw_answer_first": "The image you've provided appears to be a collage of various statistics and data points related to", "raw_answer_second": null, "mean_entropy_first": 0.9634888624786981, "normalized_entropy_first": 0.11614423107312162, "min_margin_first": 0.14855575561523438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1050, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1053, "total_latency_s": 1.053, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1023128032684326, 0.044175781309604645, 1.24643075466156, 0.3397102653980255, 0.0014794457238167524, 0.6005931496620178, 0.9281830191612244, 0.0003641823132056743, 0.051100291311740875, 0.4728739261627197, 3.558896541595459, 0.002307418268173933, 0.804377555847168, 0.9455569386482239, 2.50201416015625, 1.0108702182769775, 2.819460391998291, 1.1455764770507812, 1.6797423362731934, 0.013751592487096786], "entropies_second": null, "final_normalized_entropy": 0.11614423107312162, "sequence_confidence_first": 0.6676081379567383, "sequence_confidence_second": null, "sequence_confidence_final": 0.6676081379567383, "token_confidences_first": [0.5739341974258423, 0.9951320886611938, 0.6332454681396484, 0.9181810021400452, 0.9998724460601807, 0.7534253001213074, 0.6838316321372986, 0.9999767541885376, 0.9927865862846375, 0.8450731635093689, 0.2335486263036728, 0.9997943043708801, 0.6158198714256287, 0.8290721774101257, 0.2797877788543701, 0.6025919914245605, 0.26935186982154846, 0.7585986256599426, 0.5256142616271973, 0.9984580278396606], "token_confidences_second": null, "final_mean_entropy": 0.9634888624786981, "final_min_margin": 0.14855575561523438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to environmental and health issues", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to environmental and health issues", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to environmental and health issues", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to environmental and health issues", "raw_answer_second": null, "mean_entropy_first": 0.8915425437960949, "normalized_entropy_first": -0.1958346295509176, "min_margin_first": 0.17681884765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1341, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1342, "total_latency_s": 1.342, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8524731993675232, 0.02341974899172783, 1.7722892761230469, 0.2802850902080536, 0.0019978913478553295, 0.6459797024726868, 0.8675801157951355, 7.629759784322232e-05, 0.024016767740249634, 0.9124659895896912, 2.693321704864502, 0.45422840118408203, 1.6778002977371216, 2.0465331077575684, 1.8597043752670288, 0.00011386309051886201, 1.449549913406372, 1.0048155784606934, 0.39708656072616577, 0.8671129941940308], "entropies_second": null, "final_normalized_entropy": -0.1958346295509176, "sequence_confidence_first": 0.6719289026809468, "sequence_confidence_second": null, "sequence_confidence_final": 0.6719289026809468, "token_confidences_first": [0.5712921023368835, 0.9974842071533203, 0.32131868600845337, 0.9367141723632812, 0.9998067021369934, 0.7301216721534729, 0.6014341711997986, 0.9999953508377075, 0.9970312118530273, 0.55143803358078, 0.36156901717185974, 0.8368420004844666, 0.6037968397140503, 0.46974995732307434, 0.5496646165847778, 0.999993085861206, 0.48230576515197754, 0.5191284418106079, 0.8754416704177856, 0.8221380710601807], "token_confidences_second": null, "final_mean_entropy": 0.8915425437960949, "final_min_margin": 0.17681884765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to healthcare trends", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to healthcare trends", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to healthcare trends", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to healthcare trends", "raw_answer_second": null, "mean_entropy_first": 0.9993006998924102, "normalized_entropy_first": 0.29666421563685547, "min_margin_first": 0.19261550903320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1157, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1161, "total_latency_s": 1.161, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1778345108032227, 0.05399210751056671, 1.6706832647323608, 0.29361388087272644, 0.0058284481056034565, 0.6828184723854065, 0.8865731954574585, 9.486130147706717e-05, 0.03806179016828537, 0.7843794822692871, 2.7592849731445312, 0.3725675940513611, 1.2815072536468506, 1.9422024488449097, 2.3019115924835205, 0.00012109986710129306, 2.31265926361084, 1.4592007398605347, 1.9601690769195557, 0.0025099418126046658], "entropies_second": null, "final_normalized_entropy": 0.29666421563685547, "sequence_confidence_first": 0.6373240730253452, "sequence_confidence_second": null, "sequence_confidence_final": 0.6373240730253452, "token_confidences_first": [0.5942129492759705, 0.9939040541648865, 0.43163520097732544, 0.9332970976829529, 0.9993694424629211, 0.7020887732505798, 0.6529701352119446, 0.9999942779541016, 0.9947673082351685, 0.5948847532272339, 0.2477036863565445, 0.8836647868156433, 0.7370012998580933, 0.5133023262023926, 0.3844816982746124, 0.9999924898147583, 0.26211050152778625, 0.5659298896789551, 0.4013727605342865, 0.9997490048408508], "token_confidences_second": null, "final_mean_entropy": 0.9993006998924102, "final_min_margin": 0.19261550903320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a poster or infographic that compares the United Kingdom", "used_ocr": false, "answer_first": "The image you've provided appears to be a poster or infographic that compares the United Kingdom", "answer_second": null, "raw_answer": "The image you've provided appears to be a poster or infographic that compares the United Kingdom", "raw_answer_first": "The image you've provided appears to be a poster or infographic that compares the United Kingdom", "raw_answer_second": null, "mean_entropy_first": 0.9459702790525626, "normalized_entropy_first": 0.029811017181118848, "min_margin_first": 0.0502777099609375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1123, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1125, "total_latency_s": 1.125, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8893977999687195, 0.06397822499275208, 1.5792505741119385, 0.27243638038635254, 0.0031682983972132206, 0.6808744072914124, 0.9383114576339722, 8.733735012356192e-05, 0.0367768332362175, 0.786929726600647, 3.4128165245056152, 0.4911714792251587, 2.087411880493164, 0.015396124683320522, 2.4233031272888184, 2.4907703399658203, 0.00018094883125741035, 1.0123696327209473, 1.7218728065490723, 0.012901676818728447], "entropies_second": null, "final_normalized_entropy": 0.029811017181118848, "sequence_confidence_first": 0.6709058812368521, "sequence_confidence_second": null, "sequence_confidence_final": 0.6709058812368521, "token_confidences_first": [0.5254231691360474, 0.9927809834480286, 0.47875210642814636, 0.941007137298584, 0.9996851682662964, 0.7063456773757935, 0.6130038499832153, 0.9999948740005493, 0.9954745173454285, 0.5052139759063721, 0.27484261989593506, 0.9215070009231567, 0.3284379541873932, 0.9980814456939697, 0.38129377365112305, 0.4223613440990448, 0.9999854564666748, 0.7715644240379333, 0.6477379202842712, 0.998636782169342], "token_confidences_second": null, "final_mean_entropy": 0.9459702790525626, "final_min_margin": 0.0502777099609375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic titled \"Equity Report: The Latest Statistics.\" It provides information", "used_ocr": false, "answer_first": "The image is an infographic titled \"Equity Report: The Latest Statistics.\" It provides information", "answer_second": null, "raw_answer": "The image is an infographic titled \"Equity Report: The Latest Statistics.\" It provides information", "raw_answer_first": "The image is an infographic titled \"Equity Report: The Latest Statistics.\" It provides information", "raw_answer_second": null, "mean_entropy_first": 0.7186026796567603, "normalized_entropy_first": -1.096961237477915, "min_margin_first": 0.18853759765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1401, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1402, "total_latency_s": 1.402, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7494274973869324, 0.055151280015707016, 1.8436400890350342, 0.9437320828437805, 0.5860645771026611, 0.016726963222026825, 1.827518105506897, 0.015199394896626472, 1.0969327688217163, 0.003607799531891942, 0.11259472370147705, 1.5428649187088013, 0.12942934036254883, 0.011259537190198898, 0.0003092591359745711, 0.03589404374361038, 1.1364011764526367, 0.5507220029830933, 1.6762439012527466, 2.0383341312408447], "entropies_second": null, "final_normalized_entropy": -1.096961237477915, "sequence_confidence_first": 0.7130248295459632, "sequence_confidence_second": null, "sequence_confidence_final": 0.7130248295459632, "token_confidences_first": [0.5984680652618408, 0.993724524974823, 0.2890361249446869, 0.5285791158676147, 0.877345085144043, 0.9979404807090759, 0.6014781594276428, 0.9983665347099304, 0.48786893486976624, 0.9996654987335205, 0.9786081910133362, 0.5303885340690613, 0.9791673421859741, 0.9987633228302002, 0.9999738931655884, 0.995416522026062, 0.549319326877594, 0.8590890765190125, 0.5868350267410278, 0.35390445590019226], "token_confidences_second": null, "final_mean_entropy": 0.7186026796567603, "final_min_margin": 0.18853759765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that illustrates the global carbon footprint by nation. The", "used_ocr": false, "answer_first": "This image is a colorful infographic that illustrates the global carbon footprint by nation. The", "answer_second": null, "raw_answer": "This image is a colorful infographic that illustrates the global carbon footprint by nation. The", "raw_answer_first": "This image is a colorful infographic that illustrates the global carbon footprint by nation. The", "raw_answer_second": null, "mean_entropy_first": 1.0283237409661523, "normalized_entropy_first": 0.5432932029789503, "min_margin_first": 0.026580810546875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1204, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1205, "total_latency_s": 1.205, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9512369632720947, 0.8041659593582153, 1.8872207403182983, 0.5248076319694519, 2.4227843284606934, 0.17500652372837067, 2.267009735107422, 0.013699909672141075, 2.2731683254241943, 1.784806728363037, 0.00012736732605844736, 0.6544861793518066, 1.135671854019165, 0.6078363060951233, 0.08475571870803833, 0.00021445390302687883, 1.3185639381408691, 1.255144476890564, 0.8238208293914795, 1.581946849822998], "entropies_second": null, "final_normalized_entropy": 0.5432932029789503, "sequence_confidence_first": 0.6246306016126603, "sequence_confidence_second": null, "sequence_confidence_final": 0.6246306016126603, "token_confidences_first": [0.579841673374176, 0.5604121088981628, 0.3745579123497009, 0.8748452067375183, 0.3839007019996643, 0.962825357913971, 0.2623726427555084, 0.998353123664856, 0.22216728329658508, 0.4947395622730255, 0.9999911785125732, 0.8304718136787415, 0.7635982036590576, 0.7827095985412598, 0.9887984395027161, 0.9999862909317017, 0.6833831667900085, 0.6213443279266357, 0.8085988163948059, 0.4281086027622223], "token_confidences_second": null, "final_mean_entropy": 1.0283237409661523, "final_min_margin": 0.026580810546875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a promotional graphic for a report titled \"", "used_ocr": false, "answer_first": "The image you've provided appears to be a promotional graphic for a report titled \"", "answer_second": null, "raw_answer": "The image you've provided appears to be a promotional graphic for a report titled \"", "raw_answer_first": "The image you've provided appears to be a promotional graphic for a report titled \"", "raw_answer_second": null, "mean_entropy_first": 0.7609853850077343, "normalized_entropy_first": -0.8608469518856132, "min_margin_first": 0.09691429138183594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1230, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1232, "total_latency_s": 1.232, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.931304931640625, 0.07362061738967896, 1.7065916061401367, 0.2574446201324463, 0.003346408950164914, 0.7357567548751831, 0.8560948967933655, 6.229030987014994e-05, 0.010523716919124126, 0.8493128418922424, 2.034191131591797, 0.0016860461328178644, 0.00021511618979275227, 1.5045042037963867, 0.003696481930091977, 1.1591613292694092, 1.2464133501052856, 2.4181551933288574, 1.4109359979629517, 0.016690164804458618], "entropies_second": null, "final_normalized_entropy": -0.8608469518856132, "sequence_confidence_first": 0.6795589905199153, "sequence_confidence_second": null, "sequence_confidence_final": 0.6795589905199153, "token_confidences_first": [0.5127274990081787, 0.990860104560852, 0.3289514482021332, 0.9443880319595337, 0.9996548891067505, 0.6260823607444763, 0.5092800855636597, 0.9999961853027344, 0.9988538026809692, 0.5073579549789429, 0.3665616512298584, 0.9998440742492676, 0.999985933303833, 0.5580824613571167, 0.999631404876709, 0.6086991429328918, 0.6420538425445557, 0.4175955653190613, 0.5194942951202393, 0.9981291890144348], "token_confidences_second": null, "final_mean_entropy": 0.7609853850077343, "final_min_margin": 0.09691429138183594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This is a graphic design that appears to be a poster or an advertisement for a cr", "used_ocr": false, "answer_first": "This is a graphic design that appears to be a poster or an advertisement for a cr", "answer_second": null, "raw_answer": "This is a graphic design that appears to be a poster or an advertisement for a cr", "raw_answer_first": "This is a graphic design that appears to be a poster or an advertisement for a cr", "raw_answer_second": null, "mean_entropy_first": 1.277954246714944, "normalized_entropy_first": 1.9041839737849444, "min_margin_first": 0.08574676513671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1318, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1318, "total_latency_s": 1.318, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.937311589717865, 0.9413816928863525, 0.7097421884536743, 2.7905993461608887, 0.2307664155960083, 1.3235664367675781, 3.027761936187744, 1.4384413957595825, 0.001590483239851892, 0.1808038353919983, 1.674614429473877, 3.164182662963867, 0.3739958107471466, 2.2369208335876465, 1.651229739189148, 0.010923406109213829, 0.002130149630829692, 1.0617059469223022, 1.9441146850585938, 1.857301950454712], "entropies_second": null, "final_normalized_entropy": 1.9041839737849444, "sequence_confidence_first": 0.5681492347070792, "sequence_confidence_second": null, "sequence_confidence_final": 0.5681492347070792, "token_confidences_first": [0.6100077629089355, 0.5568665862083435, 0.5201081037521362, 0.2710319757461548, 0.9389434456825256, 0.7519423961639404, 0.23834484815597534, 0.671326756477356, 0.9998779296875, 0.9752041697502136, 0.4830872416496277, 0.22833232581615448, 0.9212990999221802, 0.2604493796825409, 0.40744441747665405, 0.9988036155700684, 0.9997976422309875, 0.7644409537315369, 0.4566936790943146, 0.6192914843559265], "token_confidences_second": null, "final_mean_entropy": 1.277954246714944, "final_min_margin": 0.08574676513671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic representation of the demographics of LinkedIn users, established", "used_ocr": false, "answer_first": "This image appears to be a graphic representation of the demographics of LinkedIn users, established", "answer_second": null, "raw_answer": "This image appears to be a graphic representation of the demographics of LinkedIn users, established", "raw_answer_first": "This image appears to be a graphic representation of the demographics of LinkedIn users, established", "raw_answer_second": null, "mean_entropy_first": 1.188888588349073, "normalized_entropy_first": 1.1287215864100701, "min_margin_first": 0.049816131591796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1354, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9740837216377258, 0.9081591963768005, 1.4361822605133057, 0.00010912999277934432, 0.06685087829828262, 0.6832490563392639, 3.1677024364471436, 0.6706861853599548, 2.0882811546325684, 0.9796733856201172, 2.1408660411834717, 1.8480850458145142, 0.6123910546302795, 0.0001883677177829668, 0.7156966924667358, 1.43003511428833, 0.016642160713672638, 1.3855605125427246, 2.0107674598693848, 2.642561912536621], "entropies_second": null, "final_normalized_entropy": 1.1287215864100701, "sequence_confidence_first": 0.5478409519925858, "sequence_confidence_second": null, "sequence_confidence_final": 0.5478409519925858, "token_confidences_first": [0.4933897852897644, 0.6211072206497192, 0.48397311568260193, 0.999992847442627, 0.9908778071403503, 0.7193254828453064, 0.36457502841949463, 0.7191320061683655, 0.3595164120197296, 0.7720673084259033, 0.35543254017829895, 0.40908142924308777, 0.717204749584198, 0.9999876022338867, 0.7493579387664795, 0.3863928020000458, 0.9977614879608154, 0.4123350977897644, 0.24783317744731903, 0.2504035532474518], "token_confidences_second": null, "final_mean_entropy": 1.188888588349073, "final_min_margin": 0.049816131591796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a comparison chart between the Indian cricket team and the", "used_ocr": false, "answer_first": "The image you've provided appears to be a comparison chart between the Indian cricket team and the", "answer_second": null, "raw_answer": "The image you've provided appears to be a comparison chart between the Indian cricket team and the", "raw_answer_first": "The image you've provided appears to be a comparison chart between the Indian cricket team and the", "raw_answer_second": null, "mean_entropy_first": 0.7718672906084976, "normalized_entropy_first": -0.938369664320012, "min_margin_first": 0.01557159423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1137, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1140, "total_latency_s": 1.14, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1588480472564697, 0.05786997079849243, 1.6302567720413208, 0.30743250250816345, 0.0038513329345732927, 0.6782688498497009, 0.9976011514663696, 0.00011436764907557517, 0.05206409469246864, 0.36604174971580505, 3.0468220710754395, 1.796250581741333, 1.7476389408111572, 1.3346803188323975, 1.2311969995498657, 0.8620246648788452, 0.00537507189437747, 0.013658881187438965, 0.10890036821365356, 0.03844907507300377], "entropies_second": null, "final_normalized_entropy": -0.938369664320012, "sequence_confidence_first": 0.6898285276622197, "sequence_confidence_second": null, "sequence_confidence_final": 0.6898285276622197, "token_confidences_first": [0.4555966556072235, 0.9930800795555115, 0.4461822509765625, 0.9303678870201111, 0.9995957016944885, 0.6861672401428223, 0.483602911233902, 0.999993085861206, 0.9925946593284607, 0.9041336178779602, 0.24332508444786072, 0.3829626441001892, 0.4450069069862366, 0.6070900559425354, 0.6849404573440552, 0.6319136619567871, 0.9993613362312317, 0.9985973238945007, 0.9845731854438782, 0.9947448968887329], "token_confidences_second": null, "final_mean_entropy": 0.7718672906084976, "final_min_margin": 0.01557159423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a poster that outlines the impact", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a poster that outlines the impact", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a poster that outlines the impact", "raw_answer_first": "The image you've provided appears to be an infographic or a poster that outlines the impact", "raw_answer_second": null, "mean_entropy_first": 1.1000270996904873, "normalized_entropy_first": 0.6947190218976991, "min_margin_first": 0.018795013427734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1300, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1311, "total_latency_s": 1.311, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8040822148323059, 0.06837146729230881, 2.0064988136291504, 0.32485517859458923, 0.0030946198385208845, 0.7223082780838013, 1.0264828205108643, 0.00018216155876871198, 0.05489557236433029, 0.779826283454895, 0.657236635684967, 0.010161802172660828, 2.1954593658447266, 2.353269577026367, 3.016547679901123, 2.3747386932373047, 2.34407377243042, 0.0001813100534491241, 1.0600781440734863, 2.198197603225708], "entropies_second": null, "final_normalized_entropy": 0.6947190218976991, "sequence_confidence_first": 0.596354444194675, "sequence_confidence_second": null, "sequence_confidence_final": 0.596354444194675, "token_confidences_first": [0.691789984703064, 0.9917995929718018, 0.2825295031070709, 0.9221145510673523, 0.9996720552444458, 0.675208568572998, 0.6036069989204407, 0.9999874830245972, 0.9930035471916199, 0.49817219376564026, 0.8258531093597412, 0.9988464117050171, 0.4507744610309601, 0.27550527453422546, 0.2091473639011383, 0.3395595848560333, 0.3395605683326721, 0.9999865293502808, 0.6842029094696045, 0.5315110087394714], "token_confidences_second": null, "final_mean_entropy": 1.1000270996904873, "final_min_margin": 0.018795013427734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a promotional poster for the \"UAE's First T2", "used_ocr": false, "answer_first": "The image appears to be a promotional poster for the \"UAE's First T2", "answer_second": null, "raw_answer": "The image appears to be a promotional poster for the \"UAE's First T2", "raw_answer_first": "The image appears to be a promotional poster for the \"UAE's First T2", "raw_answer_second": null, "mean_entropy_first": 0.48070533756399525, "normalized_entropy_first": -2.3643559130472256, "min_margin_first": 0.05377197265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1357, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1358, "total_latency_s": 1.358, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8619088530540466, 0.11753323674201965, 1.809885859489441, 0.00011804095993284136, 0.0179925374686718, 0.6853686571121216, 1.0534507036209106, 0.0009185374365188181, 0.00011018830991815776, 1.075671911239624, 0.7170721292495728, 1.044421672821045, 1.45768404006958, 0.3028492331504822, 0.008108381181955338, 0.06954661756753922, 0.012064467184245586, 0.04487617313861847, 0.333175390958786, 0.0013501205248758197], "entropies_second": null, "final_normalized_entropy": -2.3643559130472256, "sequence_confidence_first": 0.7703812929241997, "sequence_confidence_second": null, "sequence_confidence_final": 0.7703812929241997, "token_confidences_first": [0.6409093141555786, 0.9819660782814026, 0.25839564204216003, 0.9999920129776001, 0.9979652166366577, 0.6780222058296204, 0.5567919611930847, 0.9999222755432129, 0.9999929666519165, 0.6479422450065613, 0.5652055144309998, 0.7053366899490356, 0.39754483103752136, 0.9553366899490356, 0.9990097284317017, 0.9894009232521057, 0.9983817338943481, 0.9939292669296265, 0.9198100566864014, 0.9998855590820312], "token_confidences_second": null, "final_mean_entropy": 0.48070533756399525, "final_min_margin": 0.05377197265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a poster or a page from a publication related to the 201", "used_ocr": false, "answer_first": "This image appears to be a poster or a page from a publication related to the 201", "answer_second": null, "raw_answer": "This image appears to be a poster or a page from a publication related to the 201", "raw_answer_first": "This image appears to be a poster or a page from a publication related to the 201", "raw_answer_second": null, "mean_entropy_first": 1.0855334571602726, "normalized_entropy_first": 0.6836089501642388, "min_margin_first": 0.4461402893066406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1467, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1471, "total_latency_s": 1.471, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8219480514526367, 0.7758502960205078, 1.0778939723968506, 9.686606790637597e-05, 0.047632213681936264, 0.6899822950363159, 3.5928170680999756, 0.6589770913124084, 2.599853992462158, 3.1290714740753174, 0.21779987215995789, 0.2690982222557068, 2.24414324760437, 2.1066107749938965, 0.00024076265981420875, 1.1643612384796143, 2.2798476219177246, 0.010034186765551567, 0.006708966568112373, 0.017700929194688797], "entropies_second": null, "final_normalized_entropy": 0.6836089501642388, "sequence_confidence_first": 0.6377560313332186, "sequence_confidence_second": null, "sequence_confidence_final": 0.6377560313332186, "token_confidences_first": [0.6505918502807617, 0.6313043236732483, 0.6688744425773621, 0.9999939203262329, 0.9937430024147034, 0.7974805235862732, 0.1622651219367981, 0.8732091188430786, 0.35942891240119934, 0.25171273946762085, 0.9721408486366272, 0.9330134391784668, 0.3987642824649811, 0.5000752210617065, 0.9999830722808838, 0.5835915803909302, 0.4223119914531708, 0.9988527297973633, 0.9993201494216919, 0.9977838397026062], "token_confidences_second": null, "final_mean_entropy": 1.0855334571602726, "final_min_margin": 0.4461402893066406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of MS Dhoni's milestone runs in OD", "used_ocr": false, "answer_first": "This image is a graphic representation of MS Dhoni's milestone runs in OD", "answer_second": null, "raw_answer": "This image is a graphic representation of MS Dhoni's milestone runs in OD", "raw_answer_first": "This image is a graphic representation of MS Dhoni's milestone runs in OD", "raw_answer_second": null, "mean_entropy_first": 0.8503978848177212, "normalized_entropy_first": -0.359469120009182, "min_margin_first": 0.016262054443359375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1268, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1269, "total_latency_s": 1.269, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0019152164459229, 0.8412851095199585, 1.424398422241211, 0.33509188890457153, 3.1741814613342285, 0.6971480250358582, 3.3030734062194824, 1.0083980560302734, 2.572512149810791, 0.0016009529354050756, 0.008867669850587845, 8.859955414664e-05, 0.04534471780061722, 2.522415161365643e-05, 0.5097307562828064, 0.20739567279815674, 0.0642532929778099, 0.23190659284591675, 1.3786015510559082, 0.20213893055915833], "entropies_second": null, "final_normalized_entropy": -0.359469120009182, "sequence_confidence_first": 0.6700240013076647, "sequence_confidence_second": null, "sequence_confidence_final": 0.6700240013076647, "token_confidences_first": [0.477102667093277, 0.5674414038658142, 0.42011287808418274, 0.9264553189277649, 0.3151934742927551, 0.5505571365356445, 0.19693711400032043, 0.8211026191711426, 0.3279537856578827, 0.9998571872711182, 0.998992383480072, 0.999994158744812, 0.9929715991020203, 0.9999985694885254, 0.9221025109291077, 0.947404146194458, 0.9930325746536255, 0.9666809439659119, 0.4299311339855194, 0.9588987231254578], "token_confidences_second": null, "final_mean_entropy": 0.8503978848177212, "final_min_margin": 0.016262054443359375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a promotional poster for the \"Road to Final Champions Trophy", "used_ocr": false, "answer_first": "The image appears to be a promotional poster for the \"Road to Final Champions Trophy", "answer_second": null, "raw_answer": "The image appears to be a promotional poster for the \"Road to Final Champions Trophy", "raw_answer_first": "The image appears to be a promotional poster for the \"Road to Final Champions Trophy", "raw_answer_second": null, "mean_entropy_first": 0.6604873893425974, "normalized_entropy_first": -1.1791239592957727, "min_margin_first": 0.13614463806152344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1127, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1128, "total_latency_s": 1.128, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1821069717407227, 0.2160566747188568, 1.6033935546875, 0.00019657102529890835, 0.023395221680402756, 0.7225409150123596, 1.0978538990020752, 0.0011772711295634508, 0.00013555803161580116, 1.2158315181732178, 0.7334019541740417, 0.9687836170196533, 2.675448417663574, 0.3780154287815094, 0.0008108915062621236, 0.11068832129240036, 0.2408807873725891, 1.9847495555877686, 0.0537295825779438, 0.0005510756745934486], "entropies_second": null, "final_normalized_entropy": -1.1791239592957727, "sequence_confidence_first": 0.7051905277645169, "sequence_confidence_second": null, "sequence_confidence_final": 0.7051905277645169, "token_confidences_first": [0.5449097752571106, 0.9605342745780945, 0.3539922833442688, 0.9999864101409912, 0.9973880648612976, 0.6531459093093872, 0.498930424451828, 0.9998980760574341, 0.9999911785125732, 0.6363496780395508, 0.6057474613189697, 0.5154014825820923, 0.22971700131893158, 0.9415194988250732, 0.9999349117279053, 0.9768730401992798, 0.9524369835853577, 0.3866204023361206, 0.9939637780189514, 0.9999557733535767], "token_confidences_second": null, "final_mean_entropy": 0.6604873893425974, "final_min_margin": 0.13614463806152344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a promotional or informational graphic for a", "used_ocr": false, "answer_first": "The image you've provided appears to be a promotional or informational graphic for a", "answer_second": null, "raw_answer": "The image you've provided appears to be a promotional or informational graphic for a", "raw_answer_first": "The image you've provided appears to be a promotional or informational graphic for a", "raw_answer_second": null, "mean_entropy_first": 0.5844850247405702, "normalized_entropy_first": -1.3802586148150187, "min_margin_first": 0.08770179748535156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1243, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1244, "total_latency_s": 1.244, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8403223752975464, 0.012233288027346134, 1.4145541191101074, 0.23647548258304596, 0.0019258027896285057, 0.6419774889945984, 0.7740586996078491, 3.8519036024808884e-05, 0.02161656692624092, 0.6103468537330627, 2.5507469177246094, 0.00047091420856304467, 0.0001931740262079984, 1.1111509799957275, 0.32290366291999817, 0.26988857984542847, 0.6659384369850159, 0.005051535088568926, 1.2989047765731812, 0.9109023213386536], "entropies_second": null, "final_normalized_entropy": -1.3802586148150187, "sequence_confidence_first": 0.7667993325467157, "sequence_confidence_second": null, "sequence_confidence_final": 0.7667993325467157, "token_confidences_first": [0.5780907869338989, 0.9988312125205994, 0.5237092971801758, 0.9497085213661194, 0.9998096823692322, 0.7172563076019287, 0.6718795895576477, 0.9999977350234985, 0.997260570526123, 0.7821648716926575, 0.3091319501399994, 0.9999638795852661, 0.9999873638153076, 0.6921322345733643, 0.9260367751121521, 0.9245983362197876, 0.8699070811271667, 0.9994328618049622, 0.41822561621665955, 0.686983585357666], "token_confidences_second": null, "final_mean_entropy": 0.5844850247405702, "final_min_margin": 0.08770179748535156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various cricket players, each representing", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various cricket players, each representing", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various cricket players, each representing", "raw_answer_first": "The image you've provided appears to be a collage of various cricket players, each representing", "raw_answer_second": null, "mean_entropy_first": 1.2570863430781174, "normalized_entropy_first": 1.639507314934596, "min_margin_first": 0.12296295166015625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1136, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1139, "total_latency_s": 1.139, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3910199403762817, 0.07202987372875214, 1.6426022052764893, 0.309006005525589, 0.0048635974526405334, 0.7498747706413269, 1.113955020904541, 0.00036050923517905176, 0.08251062780618668, 0.4984220862388611, 2.7245373725891113, 0.0022591548040509224, 1.328728437423706, 3.0975611209869385, 2.6990365982055664, 0.2817429304122925, 1.8456313610076904, 1.9353625774383545, 2.3941969871520996, 2.9680256843566895], "entropies_second": null, "final_normalized_entropy": 1.639507314934596, "sequence_confidence_first": 0.5700910261495993, "sequence_confidence_second": null, "sequence_confidence_final": 0.5700910261495993, "token_confidences_first": [0.4934437572956085, 0.9918491840362549, 0.40785109996795654, 0.9310837388038635, 0.9995392560958862, 0.635069727897644, 0.5835862755775452, 0.9999760389328003, 0.9872819781303406, 0.8618133664131165, 0.27787625789642334, 0.9997996687889099, 0.4758754074573517, 0.3347218632698059, 0.317543089389801, 0.9203686714172363, 0.5034206509590149, 0.3661109507083893, 0.3064330816268921, 0.30729421973228455], "token_confidences_second": null, "final_mean_entropy": 1.2570863430781174, "final_min_margin": 0.12296295166015625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows two bar charts, each representing the percentage of people who follow five different sports. The", "used_ocr": false, "answer_first": "The image shows two bar charts, each representing the percentage of people who follow five different sports. The", "answer_second": null, "raw_answer": "The image shows two bar charts, each representing the percentage of people who follow five different sports. The", "raw_answer_first": "The image shows two bar charts, each representing the percentage of people who follow five different sports. The", "raw_answer_second": null, "mean_entropy_first": 1.4425458371639253, "normalized_entropy_first": 2.113532055240824, "min_margin_first": 0.21609878540039062, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1279, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1280, "total_latency_s": 1.28, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8474916219711304, 0.08520388603210449, 1.8427095413208008, 0.8101556301116943, 2.3480851650238037, 0.7779484391212463, 2.324918270111084, 0.7144593000411987, 1.7108709812164307, 1.727565884590149, 2.9279098510742188, 0.3919845223426819, 3.186461925506592, 1.482008695602417, 0.7898755073547363, 2.1988558769226074, 0.9033653736114502, 0.12326610088348389, 2.5300376415252686, 1.1277425289154053], "entropies_second": null, "final_normalized_entropy": 2.113532055240824, "sequence_confidence_first": 0.519882847356213, "sequence_confidence_second": null, "sequence_confidence_final": 0.519882847356213, "token_confidences_first": [0.6727489233016968, 0.989116370677948, 0.27729666233062744, 0.6037305593490601, 0.4478001594543457, 0.63324373960495, 0.3008204400539398, 0.8550348877906799, 0.5966914892196655, 0.4374087154865265, 0.28198814392089844, 0.9430409669876099, 0.18773165345191956, 0.6145079731941223, 0.8854975700378418, 0.2710500657558441, 0.6835039854049683, 0.9854234457015991, 0.2617223858833313, 0.7557355165481567], "token_confidences_second": null, "final_mean_entropy": 1.4425458371639253, "final_min_margin": 0.21609878540039062, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This is an advertisement for the 2015 edition of the Wisden Crick", "used_ocr": false, "answer_first": "This is an advertisement for the 2015 edition of the Wisden Crick", "answer_second": null, "raw_answer": "This is an advertisement for the 2015 edition of the Wisden Crick", "raw_answer_first": "This is an advertisement for the 2015 edition of the Wisden Crick", "raw_answer_second": null, "mean_entropy_first": 0.6643111772427801, "normalized_entropy_first": -1.027502998768496, "min_margin_first": 0.07108306884765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1217, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1217, "total_latency_s": 1.217, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8840620517730713, 0.9050589799880981, 0.7195506691932678, 0.7897473573684692, 0.006380184553563595, 0.001825563726015389, 1.384756326675415, 1.0462262630462646, 1.7650651931762695, 0.5836752653121948, 0.04937785491347313, 0.016713237389922142, 0.02933354675769806, 1.2394672632217407, 0.013060025870800018, 1.042593002319336, 2.052302598953247, 0.008014647290110588, 0.4410630166530609, 0.307950496673584], "entropies_second": null, "final_normalized_entropy": -1.027502998768496, "sequence_confidence_first": 0.7495221302065276, "sequence_confidence_second": null, "sequence_confidence_final": 0.7495221302065276, "token_confidences_first": [0.5015984177589417, 0.5065242052078247, 0.5637067556381226, 0.7927020192146301, 0.9992881417274475, 0.9998243451118469, 0.6004640460014343, 0.7703229188919067, 0.4739515483379364, 0.7558462023735046, 0.9927453398704529, 0.9978832602500916, 0.9962908029556274, 0.7399997711181641, 0.9987226128578186, 0.730570912361145, 0.3657057583332062, 0.9991466999053955, 0.9412304162979126, 0.9089257717132568], "token_confidences_second": null, "final_mean_entropy": 0.6643111772427801, "final_min_margin": 0.07108306884765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphical representation of demographic estimates for the census metropolitan areas (CMA", "used_ocr": false, "answer_first": "This image is a graphical representation of demographic estimates for the census metropolitan areas (CMA", "answer_second": null, "raw_answer": "This image is a graphical representation of demographic estimates for the census metropolitan areas (CMA", "raw_answer_first": "This image is a graphical representation of demographic estimates for the census metropolitan areas (CMA", "raw_answer_second": null, "mean_entropy_first": 0.8134199848398567, "normalized_entropy_first": -0.4095067857184504, "min_margin_first": 0.039371490478515625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1417, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1418, "total_latency_s": 1.418, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7881197929382324, 0.8621994256973267, 1.2961912155151367, 0.5218984484672546, 3.1923718452453613, 0.711105227470398, 0.6236380934715271, 0.3287031352519989, 1.2003719806671143, 0.0671033263206482, 1.1372830867767334, 0.7444412708282471, 1.8957006931304932, 1.7221295833587646, 0.03506278246641159, 0.0023918149527162313, 0.08375555276870728, 1.0393089056015015, 0.015697410330176353, 0.0009261055383831263], "entropies_second": null, "final_normalized_entropy": -0.4095067857184504, "sequence_confidence_first": 0.6610081433673096, "sequence_confidence_second": null, "sequence_confidence_final": 0.6610081433673096, "token_confidences_first": [0.5071970224380493, 0.6270313262939453, 0.5936214923858643, 0.8791757822036743, 0.16401250660419464, 0.5370656847953796, 0.8775992393493652, 0.9445536136627197, 0.7095033526420593, 0.9879512190818787, 0.6026887893676758, 0.8588252067565918, 0.2960434556007385, 0.40880700945854187, 0.9956883788108826, 0.9997653365135193, 0.9845356345176697, 0.48709461092948914, 0.9983022212982178, 0.9999257326126099], "token_confidences_second": null, "final_mean_entropy": 0.8134199848398567, "final_min_margin": 0.039371490478515625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a poster or infographic related to the ICC Cr", "used_ocr": false, "answer_first": "The image you've provided appears to be a poster or infographic related to the ICC Cr", "answer_second": null, "raw_answer": "The image you've provided appears to be a poster or infographic related to the ICC Cr", "raw_answer_first": "The image you've provided appears to be a poster or infographic related to the ICC Cr", "raw_answer_second": null, "mean_entropy_first": 0.6600416308181594, "normalized_entropy_first": -0.9418033686632945, "min_margin_first": 0.07561111450195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1135, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1138, "total_latency_s": 1.138, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0547516345977783, 0.036918770521879196, 1.4661051034927368, 0.29649630188941956, 0.0021258238703012466, 0.6597391366958618, 0.8627341985702515, 6.692980241496116e-05, 0.02388366311788559, 0.564218282699585, 2.627333641052246, 0.24058309197425842, 2.3425674438476562, 0.024474555626511574, 1.6888574361801147, 0.00015694925969000906, 0.21097859740257263, 0.8671131134033203, 0.0015072966925799847, 0.23022064566612244], "entropies_second": null, "final_normalized_entropy": -0.9418033686632945, "sequence_confidence_first": 0.7255719544765111, "sequence_confidence_second": null, "sequence_confidence_final": 0.7255719544765111, "token_confidences_first": [0.4802348017692566, 0.9958646297454834, 0.5325955152511597, 0.9333095550537109, 0.9998088479042053, 0.7049670219421387, 0.608156144618988, 0.9999961853027344, 0.9970510005950928, 0.7928636074066162, 0.2261659950017929, 0.9612001180648804, 0.29892513155937195, 0.996595561504364, 0.4628352224826813, 0.99998939037323, 0.9646754264831543, 0.7377484440803528, 0.9998615980148315, 0.9517958760261536], "token_confidences_second": null, "final_mean_entropy": 0.6600416308181594, "final_min_margin": 0.07561111450195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a colorful infographic that provides information about healthcare jobs and", "used_ocr": false, "answer_first": "The image you've provided is a colorful infographic that provides information about healthcare jobs and", "answer_second": null, "raw_answer": "The image you've provided is a colorful infographic that provides information about healthcare jobs and", "raw_answer_first": "The image you've provided is a colorful infographic that provides information about healthcare jobs and", "raw_answer_second": null, "mean_entropy_first": 0.9116902415174991, "normalized_entropy_first": 0.06614589312962002, "min_margin_first": 0.07690811157226562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1253, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1256, "total_latency_s": 1.256, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9151940941810608, 0.09942060708999634, 1.5669599771499634, 0.27899444103240967, 0.0027957335114479065, 0.7002187967300415, 0.8623234033584595, 0.7453742623329163, 1.8258076906204224, 0.007213170640170574, 0.7270767688751221, 0.007854122668504715, 2.3742241859436035, 2.467761993408203, 1.9627184867858887, 0.7818142771720886, 1.0399771928787231, 0.011622326448559761, 0.4087725877761841, 1.4476807117462158], "entropies_second": null, "final_normalized_entropy": 0.06614589312962002, "sequence_confidence_first": 0.6295065296853382, "sequence_confidence_second": null, "sequence_confidence_final": 0.6295065296853382, "token_confidences_first": [0.5297808051109314, 0.9861722588539124, 0.47473111748695374, 0.9377537965774536, 0.9997488856315613, 0.6552152037620544, 0.5043584108352661, 0.6014837622642517, 0.4658946394920349, 0.9991734623908997, 0.807470440864563, 0.9991255402565002, 0.3915245234966278, 0.3049828112125397, 0.32959675788879395, 0.7183911800384521, 0.52687007188797, 0.9985935091972351, 0.9155208468437195, 0.40398675203323364], "token_confidences_second": null, "final_mean_entropy": 0.9116902415174991, "final_min_margin": 0.07690811157226562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a promotional graphic for a cricket match or event. It", "used_ocr": false, "answer_first": "The image appears to be a promotional graphic for a cricket match or event. It", "answer_second": null, "raw_answer": "The image appears to be a promotional graphic for a cricket match or event. It", "raw_answer_first": "The image appears to be a promotional graphic for a cricket match or event. It", "raw_answer_second": null, "mean_entropy_first": 1.0892966968443942, "normalized_entropy_first": 0.7488423691247419, "min_margin_first": 0.12819480895996094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1339, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1340, "total_latency_s": 1.34, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.001951813697815, 0.08943117409944534, 1.72477388381958, 0.0003148504183627665, 0.05332638695836067, 0.659058153629303, 3.459420680999756, 0.0024372534826397896, 0.0007148374570533633, 1.4766712188720703, 0.008868762291967869, 1.4271448850631714, 1.520468831062317, 1.4592673778533936, 0.008924974128603935, 2.4929404258728027, 1.9345213174819946, 2.1948747634887695, 1.4363691806793213, 0.8344531655311584], "entropies_second": null, "final_normalized_entropy": 0.7488423691247419, "sequence_confidence_first": 0.5921022063497152, "sequence_confidence_second": null, "sequence_confidence_final": 0.5921022063497152, "token_confidences_first": [0.5057618021965027, 0.9892372488975525, 0.3219951093196869, 0.999976396560669, 0.9926322102546692, 0.7468427419662476, 0.17833875119686127, 0.9997797608375549, 0.99994957447052, 0.5593758821487427, 0.999053418636322, 0.5208714604377747, 0.5241838097572327, 0.7135167717933655, 0.9988835453987122, 0.23836608231067657, 0.29680871963500977, 0.3746471703052521, 0.5824009776115417, 0.7847133874893188], "token_confidences_second": null, "final_mean_entropy": 1.0892966968443942, "final_min_margin": 0.12819480895996094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a promotional graphic for a cricket-related event or article", "used_ocr": false, "answer_first": "The image appears to be a promotional graphic for a cricket-related event or article", "answer_second": null, "raw_answer": "The image appears to be a promotional graphic for a cricket-related event or article", "raw_answer_first": "The image appears to be a promotional graphic for a cricket-related event or article", "raw_answer_second": null, "mean_entropy_first": 0.9850705653108889, "normalized_entropy_first": 0.2783061602683364, "min_margin_first": 0.08351325988769531, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1330, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1331, "total_latency_s": 1.331, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8642092347145081, 0.038754113018512726, 1.4301531314849854, 0.000268679519649595, 0.028138747438788414, 0.7340455055236816, 1.4861083030700684, 0.0011466712458059192, 0.00034128129482269287, 1.431349754333496, 0.0017088046297430992, 1.5711941719055176, 1.6050310134887695, 2.013827085494995, 0.00871873740106821, 2.0680336952209473, 0.13269254565238953, 1.484510898590088, 1.395801067352295, 3.4053778648376465], "entropies_second": null, "final_normalized_entropy": 0.2783061602683364, "sequence_confidence_first": 0.6491304384812917, "sequence_confidence_second": null, "sequence_confidence_final": 0.6491304384812917, "token_confidences_first": [0.5066058039665222, 0.9953655004501343, 0.46891212463378906, 0.9999806880950928, 0.9967315196990967, 0.6039505004882812, 0.6818056702613831, 0.9999034404754639, 0.999976634979248, 0.5300574898719788, 0.9998378753662109, 0.4892389178276062, 0.49622368812561035, 0.5711011290550232, 0.9989511966705322, 0.36832255125045776, 0.9762544631958008, 0.6905179023742676, 0.5339337587356567, 0.18687602877616882], "token_confidences_second": null, "final_mean_entropy": 0.9850705653108889, "final_min_margin": 0.08351325988769531, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that presents a list of reasons why people might choose", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that presents a list of reasons why people might choose", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that presents a list of reasons why people might choose", "raw_answer_first": "This image appears to be a graphic or poster that presents a list of reasons why people might choose", "raw_answer_second": null, "mean_entropy_first": 1.6839033095602645, "normalized_entropy_first": 3.1706382586128012, "min_margin_first": 0.03495979309082031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1291, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1292, "total_latency_s": 1.292, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8951196670532227, 0.829898476600647, 1.637256383895874, 0.0002945036976598203, 0.08622965216636658, 0.7457317113876343, 3.007737874984741, 0.3463120758533478, 2.223452568054199, 1.79723060131073, 2.445507049560547, 3.088528633117676, 2.3964884281158447, 2.7428746223449707, 0.3638714551925659, 3.2432801723480225, 2.2435836791992188, 2.3625502586364746, 2.1367902755737305, 1.0853281021118164], "entropies_second": null, "final_normalized_entropy": 3.1706382586128012, "sequence_confidence_first": 0.47625006088410404, "sequence_confidence_second": null, "sequence_confidence_final": 0.47625006088410404, "token_confidences_first": [0.5034105181694031, 0.7288331985473633, 0.3892824947834015, 0.999977707862854, 0.9889727234840393, 0.5563401579856873, 0.19889187812805176, 0.8987273573875427, 0.45233163237571716, 0.4335336983203888, 0.39572590589523315, 0.2103395313024521, 0.36004215478897095, 0.3910258114337921, 0.9417364597320557, 0.32874172925949097, 0.29147088527679443, 0.42356592416763306, 0.3573630154132843, 0.8172624111175537], "token_confidences_second": null, "final_mean_entropy": 1.6839033095602645, "final_min_margin": 0.03495979309082031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a fact book or data snapshot from the Tennessee Higher Education Commission", "used_ocr": false, "answer_first": "The image you've provided is a fact book or data snapshot from the Tennessee Higher Education Commission", "answer_second": null, "raw_answer": "The image you've provided is a fact book or data snapshot from the Tennessee Higher Education Commission", "raw_answer_first": "The image you've provided is a fact book or data snapshot from the Tennessee Higher Education Commission", "raw_answer_second": null, "mean_entropy_first": 0.6039409297249222, "normalized_entropy_first": -1.220635359944115, "min_margin_first": 0.20903778076171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1409, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1410, "total_latency_s": 1.41, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.804058313369751, 0.11050963401794434, 1.5105059146881104, 0.2721618115901947, 0.002308431314304471, 0.6158536076545715, 0.7266479730606079, 0.8779593110084534, 1.5704467296600342, 0.5195479393005371, 2.126331090927124, 0.8160252571105957, 0.20934051275253296, 1.6896674633026123, 0.13748905062675476, 0.06360417604446411, 0.007250860333442688, 0.00011484579590614885, 0.0006743254489265382, 0.01832134649157524], "entropies_second": null, "final_normalized_entropy": -1.220635359944115, "sequence_confidence_first": 0.7858189206622251, "sequence_confidence_second": null, "sequence_confidence_final": 0.7858189206622251, "token_confidences_first": [0.5804534554481506, 0.985524594783783, 0.5203579068183899, 0.9377231597900391, 0.9997702240943909, 0.7380020022392273, 0.6631720066070557, 0.7194246053695679, 0.6809961795806885, 0.8728975057601929, 0.4245375990867615, 0.8241745233535767, 0.9674866199493408, 0.42194637656211853, 0.977538526058197, 0.9912817478179932, 0.9992614388465881, 0.9999912977218628, 0.9999449253082275, 0.9980236291885376], "token_confidences_second": null, "final_mean_entropy": 0.6039409297249222, "final_min_margin": 0.20903778076171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various Bible verses and titles,", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various Bible verses and titles,", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various Bible verses and titles,", "raw_answer_first": "The image you've provided appears to be a collage of various Bible verses and titles,", "raw_answer_second": null, "mean_entropy_first": 1.525690223730635, "normalized_entropy_first": 1.7269858729677068, "min_margin_first": 0.02970123291015625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 984, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 985, "total_latency_s": 0.985, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4360630512237549, 0.9455956816673279, 1.9137483835220337, 0.5991489887237549, 0.026118040084838867, 0.6941551566123962, 1.5505602359771729, 0.0014127532485872507, 0.335610032081604, 0.514220654964447, 3.5783462524414062, 0.004820259287953377, 1.148221492767334, 2.89811110496521, 3.4048287868499756, 2.6588568687438965, 0.0007392796687781811, 2.520036458969116, 4.037362098693848, 2.2458488941192627], "entropies_second": null, "final_normalized_entropy": 1.7269858729677068, "sequence_confidence_first": 0.47714465839303943, "sequence_confidence_second": null, "sequence_confidence_final": 0.47714465839303943, "token_confidences_first": [0.4788517951965332, 0.7923725247383118, 0.28451985120773315, 0.8192141056060791, 0.9973679184913635, 0.7335601449012756, 0.43899890780448914, 0.9999014139175415, 0.9301842451095581, 0.8486868739128113, 0.17509834468364716, 0.9995145797729492, 0.5689826011657715, 0.3353835940361023, 0.1681324988603592, 0.22943395376205444, 0.9999450445175171, 0.2654569745063782, 0.10320056974887848, 0.472779780626297], "token_confidences_second": null, "final_mean_entropy": 1.525690223730635, "final_min_margin": 0.02970123291015625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to urban sprawl. It shows a", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to urban sprawl. It shows a", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to urban sprawl. It shows a", "raw_answer_first": "This image appears to be a graphic or infographic related to urban sprawl. It shows a", "raw_answer_second": null, "mean_entropy_first": 1.0941566121960933, "normalized_entropy_first": 0.22203089372077373, "min_margin_first": 0.016424179077148438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1349, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1350, "total_latency_s": 1.35, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8297943472862244, 0.8042734861373901, 1.3905911445617676, 7.653298962395638e-05, 0.038495779037475586, 0.7203389406204224, 3.0092668533325195, 0.653015673160553, 2.0797605514526367, 2.4943785667419434, 0.014811145141720772, 2.2617170810699463, 0.00018170825205743313, 1.336541771888733, 0.672120213508606, 0.006969217211008072, 1.2486844062805176, 1.048531413078308, 2.16494083404541, 1.108642578125], "entropies_second": null, "final_normalized_entropy": 0.22203089372077373, "sequence_confidence_first": 0.5902018614550248, "sequence_confidence_second": null, "sequence_confidence_final": 0.5902018614550248, "token_confidences_first": [0.5825117230415344, 0.49436792731285095, 0.461167573928833, 0.9999951124191284, 0.9950767159461975, 0.686089277267456, 0.2596796154975891, 0.6782026290893555, 0.4447256028652191, 0.20818859338760376, 0.998213529586792, 0.42497172951698303, 0.9999885559082031, 0.6516011357307434, 0.8867054581642151, 0.9992600083351135, 0.6053150296211243, 0.6605694890022278, 0.2725170850753784, 0.666659951210022], "token_confidences_second": null, "final_mean_entropy": 1.0941566121960933, "final_min_margin": 0.016424179077148438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a chart titled \"How Long It Takes to Read Each Book", "used_ocr": false, "answer_first": "The image you've provided is a chart titled \"How Long It Takes to Read Each Book", "answer_second": null, "raw_answer": "The image you've provided is a chart titled \"How Long It Takes to Read Each Book", "raw_answer_first": "The image you've provided is a chart titled \"How Long It Takes to Read Each Book", "raw_answer_second": null, "mean_entropy_first": 0.6724072210025043, "normalized_entropy_first": -1.039392570405173, "min_margin_first": 0.0691986083984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1223, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1224, "total_latency_s": 1.224, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8620719313621521, 0.26356041431427, 2.05958890914917, 0.30801647901535034, 0.006096335127949715, 0.7481865286827087, 1.3754849433898926, 0.8836178779602051, 2.62129545211792, 2.0806169509887695, 0.02069389820098877, 0.7789819836616516, 0.4822247624397278, 0.49636310338974, 0.017918255180120468, 0.008595042861998081, 0.24095425009727478, 0.058839112520217896, 0.07885784655809402, 0.05618034303188324], "entropies_second": null, "final_normalized_entropy": -1.039392570405173, "sequence_confidence_first": 0.7230915503842832, "sequence_confidence_second": null, "sequence_confidence_final": 0.7230915503842832, "token_confidences_first": [0.6706007122993469, 0.9587108492851257, 0.2653495669364929, 0.9303265810012817, 0.9993085861206055, 0.6416219472885132, 0.4224293828010559, 0.6462998390197754, 0.3695419430732727, 0.4041520059108734, 0.9974984526634216, 0.5645966529846191, 0.8272324204444885, 0.8670161962509155, 0.9980952143669128, 0.9989332556724548, 0.9395576119422913, 0.9913188815116882, 0.988046407699585, 0.9930175542831421], "token_confidences_second": null, "final_mean_entropy": 0.6724072210025043, "final_min_margin": 0.0691986083984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This is an infographic poster for the World Cup, specifically for the 2014 edition", "used_ocr": false, "answer_first": "This is an infographic poster for the World Cup, specifically for the 2014 edition", "answer_second": null, "raw_answer": "This is an infographic poster for the World Cup, specifically for the 2014 edition", "raw_answer_first": "This is an infographic poster for the World Cup, specifically for the 2014 edition", "raw_answer_second": null, "mean_entropy_first": 1.261060291947797, "normalized_entropy_first": 0.8096647035793628, "min_margin_first": 0.2115650177001953, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1342, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1343, "total_latency_s": 1.343, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.006779432296753, 0.8304422497749329, 0.6631239652633667, 0.9359527826309204, 0.013168180361390114, 2.243152141571045, 2.4082064628601074, 0.5839530229568481, 1.9829603433609009, 0.006560911424458027, 1.8733422756195068, 2.6453592777252197, 2.714148759841919, 1.494706630706787, 2.8303771018981934, 0.6084147691726685, 0.017189616337418556, 0.2809152603149414, 0.45214807987213135, 1.6303045749664307], "entropies_second": null, "final_normalized_entropy": 0.8096647035793628, "sequence_confidence_first": 0.5883448357523497, "sequence_confidence_second": null, "sequence_confidence_final": 0.5883448357523497, "token_confidences_first": [0.5218935608863831, 0.5615573525428772, 0.656969428062439, 0.7798171639442444, 0.9983046054840088, 0.4337703585624695, 0.30914071202278137, 0.8808494210243225, 0.3353736400604248, 0.9993733763694763, 0.5786182880401611, 0.2647455036640167, 0.2931395471096039, 0.665850818157196, 0.44047123193740845, 0.7219672203063965, 0.9982727766036987, 0.9325810074806213, 0.9026996493339539, 0.5208362340927124], "token_confidences_second": null, "final_mean_entropy": 1.261060291947797, "final_min_margin": 0.2115650177001953, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic illustration that promotes the concept of peace.", "used_ocr": false, "answer_first": "The image you've provided is a graphic illustration that promotes the concept of peace.", "answer_second": null, "raw_answer": "The image you've provided is a graphic illustration that promotes the concept of peace.", "raw_answer_first": "The image you've provided is a graphic illustration that promotes the concept of peace.", "raw_answer_second": null, "mean_entropy_first": 1.1571284430101514, "normalized_entropy_first": 0.429244765623759, "min_margin_first": 0.11029434204101562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1931, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1932, "total_latency_s": 1.932, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7948709726333618, 0.03180110082030296, 1.7611186504364014, 0.20490385591983795, 0.0049322620034217834, 0.7951356768608093, 1.0352541208267212, 0.8232595920562744, 2.7880678176879883, 0.21875986456871033, 2.438001871109009, 0.2530212998390198, 2.298105239868164, 3.707671642303467, 0.7017385959625244, 1.0803923606872559, 1.4740443229675293, 0.11558862030506134, 0.9573698043823242, 1.6585311889648438], "entropies_second": null, "final_normalized_entropy": 0.429244765623759, "sequence_confidence_first": 0.5537548910026323, "sequence_confidence_second": null, "sequence_confidence_final": 0.5537548910026323, "token_confidences_first": [0.5274216532707214, 0.9965190887451172, 0.4698333144187927, 0.9592015743255615, 0.9994500279426575, 0.6235164999961853, 0.504216730594635, 0.6775172352790833, 0.1974351704120636, 0.9434086680412292, 0.25639262795448303, 0.931816041469574, 0.44949254393577576, 0.14754827320575714, 0.5269374847412109, 0.6598281264305115, 0.6501649022102356, 0.9819604158401489, 0.5264523029327393, 0.4226923882961273], "token_confidences_second": null, "final_mean_entropy": 1.1571284430101514, "final_min_margin": 0.11029434204101562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"WHOLE GRAIN MOM", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"WHOLE GRAIN MOM", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"WHOLE GRAIN MOM", "raw_answer_first": "The image you've provided is an infographic titled \"WHOLE GRAIN MOM", "raw_answer_second": null, "mean_entropy_first": 0.46649107994744554, "normalized_entropy_first": -1.7819451337415044, "min_margin_first": 0.09039878845214844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1341, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1348, "total_latency_s": 1.348, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8586635589599609, 0.1083434522151947, 1.8422746658325195, 0.3175409734249115, 0.0025916905142366886, 0.7072687745094299, 0.943077027797699, 0.7619973421096802, 0.46216005086898804, 0.010659714229404926, 1.988124966621399, 0.01370873674750328, 0.968777060508728, 0.2102041393518448, 0.00037653063191100955, 0.02742166444659233, 0.00039645927608944476, 0.00020200959988869727, 0.07621246576309204, 0.029820315539836884], "entropies_second": null, "final_normalized_entropy": -1.7819451337415044, "sequence_confidence_first": 0.7896646168886282, "sequence_confidence_second": null, "sequence_confidence_final": 0.7896646168886282, "token_confidences_first": [0.5518195629119873, 0.9842125773429871, 0.370221346616745, 0.9257773160934448, 0.9997428059577942, 0.6570932865142822, 0.552021861076355, 0.6311177611351013, 0.886648416519165, 0.9987469911575317, 0.49764639139175415, 0.998525083065033, 0.5017955899238586, 0.9594390988349915, 0.9999755620956421, 0.996729850769043, 0.9999690055847168, 0.9999865293502808, 0.9919232726097107, 0.9962330460548401], "token_confidences_second": null, "final_mean_entropy": 0.46649107994744554, "final_min_margin": 0.09039878845214844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous infographic comparing the pros and cons of reading an", "used_ocr": false, "answer_first": "The image you've provided is a humorous infographic comparing the pros and cons of reading an", "answer_second": null, "raw_answer": "The image you've provided is a humorous infographic comparing the pros and cons of reading an", "raw_answer_first": "The image you've provided is a humorous infographic comparing the pros and cons of reading an", "raw_answer_second": null, "mean_entropy_first": 0.9994397524860688, "normalized_entropy_first": 0.07596340860937353, "min_margin_first": 0.1301422119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1827, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1828, "total_latency_s": 1.828, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9734702110290527, 0.19075599312782288, 1.8865585327148438, 0.2526026666164398, 0.005727678537368774, 0.811828076839447, 0.974103569984436, 0.7383575439453125, 2.6226131916046143, 0.0010889319237321615, 1.312441110610962, 0.0122365802526474, 1.3341299295425415, 1.9010931253433228, 3.58201265335083, 0.1339406818151474, 0.005639151204377413, 0.03194037824869156, 1.2636489868164062, 1.954606056213379], "entropies_second": null, "final_normalized_entropy": 0.07596340860937353, "sequence_confidence_first": 0.6300315310204215, "sequence_confidence_second": null, "sequence_confidence_final": 0.6300315310204215, "token_confidences_first": [0.5383738279342651, 0.9687836170196533, 0.3646928668022156, 0.9456973671913147, 0.9993627667427063, 0.5783823132514954, 0.5045230984687805, 0.641046941280365, 0.3430735468864441, 0.9999097585678101, 0.6922644376754761, 0.9985588192939758, 0.5812557339668274, 0.4990658462047577, 0.14492858946323395, 0.971256673336029, 0.9995512366294861, 0.9960476756095886, 0.729515552520752, 0.41060084104537964], "token_confidences_second": null, "final_mean_entropy": 0.9994397524860688, "final_min_margin": 0.1301422119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a visual timeline of the rise of robots and artificial intelligence (AI). It", "used_ocr": false, "answer_first": "This image is a visual timeline of the rise of robots and artificial intelligence (AI). It", "answer_second": null, "raw_answer": "This image is a visual timeline of the rise of robots and artificial intelligence (AI). It", "raw_answer_first": "This image is a visual timeline of the rise of robots and artificial intelligence (AI). It", "raw_answer_second": null, "mean_entropy_first": 0.8673545015277341, "normalized_entropy_first": -0.33243739649596366, "min_margin_first": 0.012380599975585938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1187, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1188, "total_latency_s": 1.188, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7822815179824829, 1.0000250339508057, 1.2961790561676025, 0.7380231022834778, 2.7285752296447754, 1.4368464946746826, 0.003954633604735136, 2.3221237659454346, 0.2109191119670868, 1.2584420442581177, 0.21888327598571777, 1.0664466619491577, 0.0019144173711538315, 0.015353409573435783, 0.6372720003128052, 0.007035323418676853, 1.615318775177002, 0.017272887751460075, 1.0543320178985596, 0.9358912706375122], "entropies_second": null, "final_normalized_entropy": -0.33243739649596366, "sequence_confidence_first": 0.6355928307217313, "sequence_confidence_second": null, "sequence_confidence_final": 0.6355928307217313, "token_confidences_first": [0.49629297852516174, 0.48536914587020874, 0.6090660691261292, 0.5840926766395569, 0.21368806064128876, 0.594956636428833, 0.9996041655540466, 0.3490271270275116, 0.970234215259552, 0.6672523021697998, 0.9439603090286255, 0.44684916734695435, 0.999828577041626, 0.9983793497085571, 0.8057743906974792, 0.9992621541023254, 0.3945416808128357, 0.9981978535652161, 0.4671936631202698, 0.7541689872741699], "token_confidences_second": null, "final_mean_entropy": 0.8673545015277341, "final_min_margin": 0.012380599975585938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic or infographic related to the EVFTA Im", "used_ocr": false, "answer_first": "The image you've provided is a graphic or infographic related to the EVFTA Im", "answer_second": null, "raw_answer": "The image you've provided is a graphic or infographic related to the EVFTA Im", "raw_answer_first": "The image you've provided is a graphic or infographic related to the EVFTA Im", "raw_answer_second": null, "mean_entropy_first": 0.9553763922100188, "normalized_entropy_first": -0.031078483815802426, "min_margin_first": 0.014528274536132812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1145, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1146, "total_latency_s": 1.146, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6803940534591675, 0.03675258532166481, 1.5819346904754639, 0.27581360936164856, 0.0009980142349377275, 0.6053857803344727, 0.8660768866539001, 0.9350152015686035, 2.4083142280578613, 0.7362134456634521, 2.8726143836975098, 2.1199188232421875, 0.023210477083921432, 2.0504069328308105, 0.00019150308798998594, 0.574430525302887, 2.008756160736084, 0.37076881527900696, 0.00018613511929288507, 0.9601455926895142], "entropies_second": null, "final_normalized_entropy": -0.031078483815802426, "sequence_confidence_first": 0.6133264528691286, "sequence_confidence_second": null, "sequence_confidence_final": 0.6133264528691286, "token_confidences_first": [0.7266034483909607, 0.9958327412605286, 0.49893665313720703, 0.9379865527153015, 0.9999101161956787, 0.7475729584693909, 0.5002607107162476, 0.5060122013092041, 0.4497644901275635, 0.5128606557846069, 0.17072774469852448, 0.5011426210403442, 0.996841311454773, 0.41257691383361816, 0.9999871253967285, 0.8875123858451843, 0.2803162634372711, 0.9145398139953613, 0.9999881982803345, 0.4794469475746155], "token_confidences_second": null, "final_mean_entropy": 0.9553763922100188, "final_min_margin": 0.014528274536132812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a colorful infographic that illustrates the various uses of artificial", "used_ocr": false, "answer_first": "The image you've provided is a colorful infographic that illustrates the various uses of artificial", "answer_second": null, "raw_answer": "The image you've provided is a colorful infographic that illustrates the various uses of artificial", "raw_answer_first": "The image you've provided is a colorful infographic that illustrates the various uses of artificial", "raw_answer_second": null, "mean_entropy_first": 1.0374284419565811, "normalized_entropy_first": 0.24815904952096773, "min_margin_first": 0.16308212280273438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1302, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1303, "total_latency_s": 1.303, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9977269172668457, 0.1032872349023819, 1.892035722732544, 0.29415082931518555, 0.005057557485997677, 0.7275340557098389, 1.111413836479187, 0.7021574974060059, 2.339876174926758, 0.019490716978907585, 1.1083505153656006, 0.011653462424874306, 1.9355947971343994, 2.644658088684082, 0.0003012714150827378, 1.0937250852584839, 2.3989391326904297, 2.316812515258789, 0.5372353792190552, 0.5085680484771729], "entropies_second": null, "final_normalized_entropy": 0.24815904952096773, "sequence_confidence_first": 0.6289964414868396, "sequence_confidence_second": null, "sequence_confidence_final": 0.6289964414868396, "token_confidences_first": [0.5479011535644531, 0.9866681098937988, 0.36863693594932556, 0.9318721890449524, 0.9994418025016785, 0.6734219789505005, 0.5237430930137634, 0.7490426301956177, 0.41706058382987976, 0.997535228729248, 0.743025541305542, 0.9986500144004822, 0.42643892765045166, 0.25285688042640686, 0.9999778270721436, 0.6410030126571655, 0.3560081720352173, 0.37275850772857666, 0.8064213991165161, 0.8392465114593506], "token_confidences_second": null, "final_mean_entropy": 1.0374284419565811, "final_min_margin": 0.16308212280273438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various statistics and demographic information related", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various statistics and demographic information related", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various statistics and demographic information related", "raw_answer_first": "The image you've provided appears to be a collage of various statistics and demographic information related", "raw_answer_second": null, "mean_entropy_first": 1.0133465407452604, "normalized_entropy_first": 0.14907178580579244, "min_margin_first": 0.0067348480224609375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1344, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1345, "total_latency_s": 1.345, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9591829776763916, 0.04357435554265976, 1.4081268310546875, 0.3207376003265381, 0.0024386802688241005, 0.646092414855957, 0.9344854354858398, 7.34935310902074e-05, 0.037571266293525696, 0.7518455386161804, 3.454210042953491, 0.0051630157977342606, 0.7946876287460327, 2.139148235321045, 2.5680723190307617, 0.7732789516448975, 1.964639663696289, 0.6902560591697693, 1.3121999502182007, 1.461146354675293], "entropies_second": null, "final_normalized_entropy": 0.14907178580579244, "sequence_confidence_first": 0.6127986759643281, "sequence_confidence_second": null, "sequence_confidence_final": 0.6127986759643281, "token_confidences_first": [0.5109313726425171, 0.9949794411659241, 0.5884336829185486, 0.9244887232780457, 0.9997625946998596, 0.726479172706604, 0.49594441056251526, 0.9999954700469971, 0.9950174689292908, 0.6295983195304871, 0.16114914417266846, 0.9994762539863586, 0.701968252658844, 0.49950340390205383, 0.2104116678237915, 0.8042264580726624, 0.43204379081726074, 0.6325242519378662, 0.520759642124176, 0.6570347547531128], "token_confidences_second": null, "final_mean_entropy": 1.0133465407452604, "final_min_margin": 0.0067348480224609375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a chart comparing the box office grosses of various comic-based films", "used_ocr": false, "answer_first": "This image appears to be a chart comparing the box office grosses of various comic-based films", "answer_second": null, "raw_answer": "This image appears to be a chart comparing the box office grosses of various comic-based films", "raw_answer_first": "This image appears to be a chart comparing the box office grosses of various comic-based films", "raw_answer_second": null, "mean_entropy_first": 0.9899864539557711, "normalized_entropy_first": 0.05380466048896678, "min_margin_first": 0.21240615844726562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1213, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1213, "total_latency_s": 1.213, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8347222805023193, 1.032369613647461, 1.4764094352722168, 3.0208020689315163e-05, 0.0461096465587616, 0.12811528146266937, 2.4941892623901367, 2.198166608810425, 1.186288595199585, 1.687024712562561, 0.33979612588882446, 1.409656286239624, 1.9112839698791504, 0.4382927417755127, 1.5801684856414795, 1.7447254657745361, 0.005344462115317583, 0.5514886379241943, 0.13074858486652374, 0.6047986745834351], "entropies_second": null, "final_normalized_entropy": 0.05380466048896678, "sequence_confidence_first": 0.674541965216748, "sequence_confidence_second": null, "sequence_confidence_final": 0.674541965216748, "token_confidences_first": [0.539034903049469, 0.5317573547363281, 0.47236168384552, 0.9999982118606567, 0.9935216307640076, 0.9767847657203674, 0.36231380701065063, 0.39501991868019104, 0.6760562062263489, 0.6423483490943909, 0.8960722088813782, 0.6356961131095886, 0.4160902798175812, 0.9016746282577515, 0.606290340423584, 0.53766268491745, 0.999447762966156, 0.8122888803482056, 0.9770199060440063, 0.8428438901901245], "token_confidences_second": null, "final_mean_entropy": 0.9899864539557711, "final_min_margin": 0.21240615844726562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a poster or infographic titled \"GENIUSES\" that discusses", "used_ocr": false, "answer_first": "This image appears to be a poster or infographic titled \"GENIUSES\" that discusses", "answer_second": null, "raw_answer": "This image appears to be a poster or infographic titled \"GENIUSES\" that discusses", "raw_answer_first": "This image appears to be a poster or infographic titled \"GENIUSES\" that discusses", "raw_answer_second": null, "mean_entropy_first": 0.9642968832748011, "normalized_entropy_first": -0.050333847858228316, "min_margin_first": 0.10361289978027344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1343, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1344, "total_latency_s": 1.344, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9729850888252258, 0.8935917615890503, 1.0340772867202759, 9.070616215467453e-05, 0.02186110056936741, 0.7154710292816162, 3.5765082836151123, 0.600645124912262, 1.5125497579574585, 0.014523183926939964, 1.870731234550476, 0.01217467337846756, 1.187822699546814, 0.17836250364780426, 0.4815264940261841, 0.005482380278408527, 1.424020528793335, 2.246701240539551, 2.535869598388672, 0.0009429887868463993], "entropies_second": null, "final_normalized_entropy": -0.050333847858228316, "sequence_confidence_first": 0.6396445123832439, "sequence_confidence_second": null, "sequence_confidence_final": 0.6396445123832439, "token_confidences_first": [0.500835657119751, 0.5869502425193787, 0.659577488899231, 0.9999939203262329, 0.9975433349609375, 0.6466691493988037, 0.16829584538936615, 0.8968960046768188, 0.5833218693733215, 0.9981808662414551, 0.5134060382843018, 0.9986870884895325, 0.4874921441078186, 0.9688395261764526, 0.825877845287323, 0.9995385408401489, 0.5028223991394043, 0.3140425682067871, 0.3788520097732544, 0.9999285936355591], "token_confidences_second": null, "final_mean_entropy": 0.9642968832748011, "final_min_margin": 0.10361289978027344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to World Population Day, which is observed on", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to World Population Day, which is observed on", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to World Population Day, which is observed on", "raw_answer_first": "This image appears to be a graphic or poster related to World Population Day, which is observed on", "raw_answer_second": null, "mean_entropy_first": 0.8881694880328723, "normalized_entropy_first": -0.3643496340358, "min_margin_first": 0.0672607421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1240, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1240, "total_latency_s": 1.24, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8289787769317627, 0.8561028242111206, 1.2044150829315186, 7.532180461566895e-05, 0.0421207919716835, 0.6919487118721008, 2.8360002040863037, 0.639741063117981, 2.139526605606079, 2.0450406074523926, 1.755535364151001, 0.00019628401787485927, 1.0355579853057861, 0.0037019760347902775, 0.0016366029158234596, 0.9033468961715698, 0.8630074858665466, 0.3793507218360901, 1.1439741849899292, 0.3931322693824768], "entropies_second": null, "final_normalized_entropy": -0.3643496340358, "sequence_confidence_first": 0.7057837858233035, "sequence_confidence_second": null, "sequence_confidence_final": 0.7057837858233035, "token_confidences_first": [0.5053839087486267, 0.6732179522514343, 0.595841109752655, 0.9999953508377075, 0.9951057434082031, 0.6956598162651062, 0.40046218037605286, 0.681704580783844, 0.4434646666049957, 0.34527161717414856, 0.6397103071212769, 0.9999867677688599, 0.7192695736885071, 0.9996606111526489, 0.9998635053634644, 0.7809394001960754, 0.8433074951171875, 0.9303910136222839, 0.6216170787811279, 0.915440022945404], "token_confidences_second": null, "final_mean_entropy": 0.8881694880328723, "final_min_margin": 0.0672607421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a table from a tennis racket guide or comparison chart. The table is", "used_ocr": false, "answer_first": "This image appears to be a table from a tennis racket guide or comparison chart. The table is", "answer_second": null, "raw_answer": "This image appears to be a table from a tennis racket guide or comparison chart. The table is", "raw_answer_first": "This image appears to be a table from a tennis racket guide or comparison chart. The table is", "raw_answer_second": null, "mean_entropy_first": 1.2435029745515749, "normalized_entropy_first": 1.204108843372363, "min_margin_first": 0.12915420532226562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1401, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1402, "total_latency_s": 1.402, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0028421878814697, 0.8127481341362, 1.006111741065979, 2.1860831111553125e-05, 0.03294765576720238, 0.26502034068107605, 2.852550506591797, 2.473755359649658, 0.30624261498451233, 2.7663626670837402, 2.428276777267456, 0.01928200200200081, 2.2825756072998047, 1.4707225561141968, 2.885568618774414, 0.7363368272781372, 0.816175103187561, 0.9477338790893555, 0.5490505695343018, 1.2157344818115234], "entropies_second": null, "final_normalized_entropy": 1.204108843372363, "sequence_confidence_first": 0.5805177112631497, "sequence_confidence_second": null, "sequence_confidence_final": 0.5805177112631497, "token_confidences_first": [0.5006473660469055, 0.6396921277046204, 0.7278931140899658, 0.9999988079071045, 0.9955288767814636, 0.9443202018737793, 0.2740386128425598, 0.3101784288883209, 0.937006413936615, 0.2712027132511139, 0.3418996334075928, 0.9977228045463562, 0.3961528539657593, 0.479999303817749, 0.26773619651794434, 0.8540726900100708, 0.7397193312644958, 0.5701414346694946, 0.890805721282959, 0.716242253780365], "token_confidences_second": null, "final_mean_entropy": 1.2435029745515749, "final_min_margin": 0.12915420532226562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"All The Whales We've Kil", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"All The Whales We've Kil", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"All The Whales We've Kil", "raw_answer_first": "The image you've provided is an infographic titled \"All The Whales We've Kil", "raw_answer_second": null, "mean_entropy_first": 0.4967528695357032, "normalized_entropy_first": -2.136105073567674, "min_margin_first": 0.0036754608154296875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1953, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1954, "total_latency_s": 1.954, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.788001537322998, 0.12431709468364716, 1.8090198040008545, 0.24402201175689697, 0.0037493002600967884, 0.7347094416618347, 1.0067172050476074, 0.8605914115905762, 0.8400010466575623, 0.011241022497415543, 1.9802547693252563, 0.01295210886746645, 0.5686430931091309, 0.6087691187858582, 0.0059072962030768394, 0.006774552166461945, 0.02752392739057541, 0.29730644822120667, 0.0037675597704946995, 0.000788641395047307], "entropies_second": null, "final_normalized_entropy": -2.136105073567674, "sequence_confidence_first": 0.7705186258771572, "sequence_confidence_second": null, "sequence_confidence_final": 0.7705186258771572, "token_confidences_first": [0.49460095167160034, 0.9802044034004211, 0.3722842037677765, 0.9479891657829285, 0.9996100068092346, 0.6321913003921509, 0.6467764973640442, 0.5092222094535828, 0.7116644978523254, 0.998671293258667, 0.38834553956985474, 0.998598039150238, 0.8275287747383118, 0.7258273959159851, 0.9994572997093201, 0.9992008805274963, 0.9970769882202148, 0.9272153377532959, 0.9996529817581177, 0.9999392032623291], "token_confidences_second": null, "final_mean_entropy": 0.4967528695357032, "final_min_margin": 0.0036754608154296875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a poster that provides information about the", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a poster that provides information about the", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a poster that provides information about the", "raw_answer_first": "The image you've provided appears to be an infographic or a poster that provides information about the", "raw_answer_second": null, "mean_entropy_first": 1.0836994596706062, "normalized_entropy_first": 0.5202281435520062, "min_margin_first": 0.024044036865234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1212, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1214, "total_latency_s": 1.214, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9687761068344116, 0.07409464567899704, 1.4128518104553223, 0.28614988923072815, 0.0024414120707660913, 0.6618210673332214, 0.8814138770103455, 7.696602551732212e-05, 0.01912388578057289, 0.7246701717376709, 0.7840451002120972, 0.011558828875422478, 2.2500295639038086, 2.163224220275879, 3.0194897651672363, 2.491077184677124, 2.2543976306915283, 1.7201467752456665, 1.0306406021118164, 0.9179596900939941], "entropies_second": null, "final_normalized_entropy": 0.5202281435520062, "sequence_confidence_first": 0.617487421964909, "sequence_confidence_second": null, "sequence_confidence_final": 0.617487421964909, "token_confidences_first": [0.5018857717514038, 0.9907152652740479, 0.5428611636161804, 0.9351339340209961, 0.9997623562812805, 0.7053069472312927, 0.5559535622596741, 0.9999953508377075, 0.9978208541870117, 0.5040992498397827, 0.7761148810386658, 0.9986808896064758, 0.454088419675827, 0.30263033509254456, 0.35863062739372253, 0.3617566227912903, 0.4634052515029907, 0.4324449300765991, 0.629854142665863, 0.7481086254119873], "token_confidences_second": null, "final_mean_entropy": 1.0836994596706062, "final_min_margin": 0.024044036865234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various statistics and information related to ag", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various statistics and information related to ag", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various statistics and information related to ag", "raw_answer_first": "The image you've provided appears to be a collage of various statistics and information related to ag", "raw_answer_second": null, "mean_entropy_first": 1.0544291870857705, "normalized_entropy_first": 0.3729003069205193, "min_margin_first": 0.17527389526367188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1065, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1069, "total_latency_s": 1.069, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3482749462127686, 0.07305745780467987, 1.3907841444015503, 0.3318308889865875, 0.004099330864846706, 0.6630115509033203, 1.054154396057129, 0.00019814717234112322, 0.039212070405483246, 0.6480058431625366, 3.4164936542510986, 0.0038600447587668896, 0.9447145462036133, 1.9684003591537476, 2.8081560134887695, 0.8773182034492493, 2.3564021587371826, 1.330169916152954, 0.00487902294844389, 1.8255610466003418], "entropies_second": null, "final_normalized_entropy": 0.3729003069205193, "sequence_confidence_first": 0.6238346405018234, "sequence_confidence_second": null, "sequence_confidence_final": 0.6238346405018234, "token_confidences_first": [0.518202006816864, 0.9917445778846741, 0.6285803914070129, 0.9225032329559326, 0.9996393918991089, 0.724258542060852, 0.5046343803405762, 0.999987006187439, 0.9950392842292786, 0.6869654655456543, 0.212515726685524, 0.9996308088302612, 0.5421013832092285, 0.5659023523330688, 0.30922532081604004, 0.6081459522247314, 0.3323763310909271, 0.6790148019790649, 0.9995272159576416, 0.38729262351989746], "token_confidences_second": null, "final_mean_entropy": 1.0544291870857705, "final_min_margin": 0.17527389526367188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a graphic titled \"Population & Migration Estimates April 20", "used_ocr": false, "answer_first": "The image displays a graphic titled \"Population & Migration Estimates April 20", "answer_second": null, "raw_answer": "The image displays a graphic titled \"Population & Migration Estimates April 20", "raw_answer_first": "The image displays a graphic titled \"Population & Migration Estimates April 20", "raw_answer_second": null, "mean_entropy_first": 0.6467467999376822, "normalized_entropy_first": -1.3121058404317085, "min_margin_first": 0.010990142822265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1208, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1208, "total_latency_s": 1.208, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9345900416374207, 0.15626125037670135, 1.9104417562484741, 1.7352838516235352, 3.2428627014160156, 0.45760196447372437, 2.143756866455078, 0.01523610670119524, 1.0816073417663574, 0.0011089774779975414, 0.13188737630844116, 0.07206566631793976, 0.0005408496945165098, 0.00717728678137064, 0.00028805379406549037, 0.0003696972562465817, 1.039049744606018, 0.0025063734501600266, 0.0014292054111137986, 0.0008708869572728872], "entropies_second": null, "final_normalized_entropy": -1.3121058404317085, "sequence_confidence_first": 0.7204685143916817, "sequence_confidence_second": null, "sequence_confidence_final": 0.7204685143916817, "token_confidences_first": [0.6208602786064148, 0.9769973754882812, 0.289165198802948, 0.3169120252132416, 0.1439308375120163, 0.8421071171760559, 0.45511871576309204, 0.9984500408172607, 0.6170927286148071, 0.9999114274978638, 0.9731695055961609, 0.9890165328979492, 0.9999547004699707, 0.9993952512741089, 0.9999780654907227, 0.9999738931655884, 0.7818658351898193, 0.9997984766960144, 0.999862790107727, 0.9999309778213501], "token_confidences_second": null, "final_mean_entropy": 0.6467467999376822, "final_min_margin": 0.010990142822265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a chart or graph with various statistics and data points.", "used_ocr": false, "answer_first": "The image you've provided appears to be a chart or graph with various statistics and data points.", "answer_second": null, "raw_answer": "The image you've provided appears to be a chart or graph with various statistics and data points.", "raw_answer_first": "The image you've provided appears to be a chart or graph with various statistics and data points.", "raw_answer_second": null, "mean_entropy_first": 1.3705722565995528, "normalized_entropy_first": 1.7256993077124372, "min_margin_first": 0.105804443359375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 959, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 961, "total_latency_s": 0.961, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.7852861881256104, 0.06000717356801033, 1.2689051628112793, 0.4610196053981781, 0.003410754958167672, 0.6884499788284302, 1.3123856782913208, 0.000728134298697114, 0.3007410168647766, 0.24191981554031372, 3.703218460083008, 1.361029863357544, 1.8823494911193848, 2.625246047973633, 1.4605311155319214, 3.095036029815674, 1.929260015487671, 2.586197853088379, 1.0516330003738403, 1.5940897464752197], "entropies_second": null, "final_normalized_entropy": 1.7256993077124372, "sequence_confidence_first": 0.5576468121316958, "sequence_confidence_second": null, "sequence_confidence_final": 0.5576468121316958, "token_confidences_first": [0.44345688819885254, 0.9934629201889038, 0.6318926811218262, 0.8800932765007019, 0.9996689558029175, 0.7105212807655334, 0.5903062224388123, 0.9999525547027588, 0.9386430382728577, 0.9571661353111267, 0.18762409687042236, 0.7099587321281433, 0.3348217010498047, 0.18139296770095825, 0.6170815825462341, 0.22052757441997528, 0.431074321269989, 0.47832170128822327, 0.7963793277740479, 0.5068483948707581], "token_confidences_second": null, "final_mean_entropy": 1.3705722565995528, "final_min_margin": 0.105804443359375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a vibrant and informative infographic about women's", "used_ocr": false, "answer_first": "The image you've provided is a vibrant and informative infographic about women's", "answer_second": null, "raw_answer": "The image you've provided is a vibrant and informative infographic about women's", "raw_answer_first": "The image you've provided is a vibrant and informative infographic about women's", "raw_answer_second": null, "mean_entropy_first": 0.777033875561392, "normalized_entropy_first": -0.7446019336629477, "min_margin_first": 0.048130035400390625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1154, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1155, "total_latency_s": 1.155, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1543171405792236, 0.10106103122234344, 1.5517154932022095, 0.2498287409543991, 0.004738439340144396, 0.750851571559906, 0.8823298215866089, 0.7976870536804199, 2.3395538330078125, 0.039805032312870026, 0.008546686731278896, 1.3299673795700073, 1.5677876472473145, 0.05708979070186615, 1.3848772048950195, 0.006841197609901428, 2.114351749420166, 1.165130853652954, 0.03415652737021446, 4.031658318126574e-05], "entropies_second": null, "final_normalized_entropy": -0.7446019336629477, "sequence_confidence_first": 0.6755103860938323, "sequence_confidence_second": null, "sequence_confidence_final": 0.6755103860938323, "token_confidences_first": [0.4624786674976349, 0.9869664907455444, 0.5001773834228516, 0.9461016058921814, 0.9994874000549316, 0.6146175265312195, 0.5572303533554077, 0.5509225726127625, 0.316406786441803, 0.9939265847206116, 0.9989104270935059, 0.5256823301315308, 0.5520883202552795, 0.9899736642837524, 0.4993915855884552, 0.9992817044258118, 0.31701210141181946, 0.6757990717887878, 0.9958027005195618, 0.999997615814209], "token_confidences_second": null, "final_mean_entropy": 0.777033875561392, "final_min_margin": 0.048130035400390625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphical representation of the \"Misery Index,\" which", "used_ocr": false, "answer_first": "The image you've provided is a graphical representation of the \"Misery Index,\" which", "answer_second": null, "raw_answer": "The image you've provided is a graphical representation of the \"Misery Index,\" which", "raw_answer_first": "The image you've provided is a graphical representation of the \"Misery Index,\" which", "raw_answer_second": null, "mean_entropy_first": 0.7226366597111337, "normalized_entropy_first": -0.8927420898115842, "min_margin_first": 0.19385910034179688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1503, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1503, "total_latency_s": 1.503, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8012186884880066, 0.1124420091509819, 1.4384156465530396, 0.24557766318321228, 0.011428841389715672, 0.6788687705993652, 0.8176432847976685, 1.3881937265396118, 2.5813136100769043, 1.3774641752243042, 0.2522900104522705, 0.9855405688285828, 1.050856590270996, 1.1179622411727905, 0.13762833178043365, 0.039743468165397644, 0.0008218518923968077, 0.0028586797416210175, 0.7471683621406555, 0.6652966737747192], "entropies_second": null, "final_normalized_entropy": -0.8927420898115842, "sequence_confidence_first": 0.743286147010507, "sequence_confidence_second": null, "sequence_confidence_final": 0.743286147010507, "token_confidences_first": [0.5702803134918213, 0.9839721918106079, 0.5705259442329407, 0.9457031488418579, 0.9985350370407104, 0.7325276136398315, 0.5354335308074951, 0.5554799437522888, 0.3669597804546356, 0.5459591746330261, 0.9609392881393433, 0.7561947703361511, 0.7823161482810974, 0.5986925959587097, 0.97735995054245, 0.9937578439712524, 0.9999241828918457, 0.9997472167015076, 0.7436478734016418, 0.8169658780097961], "token_confidences_second": null, "final_mean_entropy": 0.7226366597111337, "final_min_margin": 0.19385910034179688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a collage or a poster that combines various elements related to basketball,", "used_ocr": false, "answer_first": "This image appears to be a collage or a poster that combines various elements related to basketball,", "answer_second": null, "raw_answer": "This image appears to be a collage or a poster that combines various elements related to basketball,", "raw_answer_first": "This image appears to be a collage or a poster that combines various elements related to basketball,", "raw_answer_second": null, "mean_entropy_first": 1.3110887038998045, "normalized_entropy_first": 1.4362781844867283, "min_margin_first": 0.12705516815185547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1410, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1411, "total_latency_s": 1.411, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8328949809074402, 0.786449134349823, 0.9590067267417908, 0.0001129072843468748, 0.01669699139893055, 0.31497830152511597, 3.431872844696045, 0.00461843004450202, 1.7023749351501465, 2.7373416423797607, 3.5849266052246094, 2.271961212158203, 2.3726284503936768, 0.0016036605229601264, 2.2556209564208984, 1.0301954746246338, 0.7016369104385376, 0.0010400385363027453, 1.5861045122146606, 1.6297093629837036], "entropies_second": null, "final_normalized_entropy": 1.4362781844867283, "sequence_confidence_first": 0.5315821794445461, "sequence_confidence_second": null, "sequence_confidence_final": 0.5315821794445461, "token_confidences_first": [0.6464493274688721, 0.6239428520202637, 0.6212039589881897, 0.9999924898147583, 0.9982395172119141, 0.9174820184707642, 0.20604188740253448, 0.9995309114456177, 0.5029565691947937, 0.2762564420700073, 0.16570937633514404, 0.3415590822696686, 0.24847157299518585, 0.9998522996902466, 0.34454986453056335, 0.8192375898361206, 0.8658008575439453, 0.9999194145202637, 0.509802520275116, 0.2822270393371582], "token_confidences_second": null, "final_mean_entropy": 1.3110887038998045, "final_min_margin": 0.12705516815185547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various statistics and information related to women", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various statistics and information related to women", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various statistics and information related to women", "raw_answer_first": "The image you've provided appears to be a collage of various statistics and information related to women", "raw_answer_second": null, "mean_entropy_first": 1.1051404739409918, "normalized_entropy_first": 0.4843954587604073, "min_margin_first": 0.02172565460205078, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1161, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1176, "total_latency_s": 1.176, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1205294132232666, 0.0683446004986763, 1.4970293045043945, 0.38614991307258606, 0.00610657362267375, 0.6381864547729492, 1.0626708269119263, 0.0003781633567996323, 0.07935088872909546, 0.6355277895927429, 3.436880588531494, 0.0031335242092609406, 0.8593714833259583, 2.0041956901550293, 2.421741008758545, 1.1896281242370605, 2.640784502029419, 2.216907024383545, 0.018992874771356583, 1.8169007301330566], "entropies_second": null, "final_normalized_entropy": 0.4843954587604073, "sequence_confidence_first": 0.6393888740371788, "sequence_confidence_second": null, "sequence_confidence_final": 0.6393888740371788, "token_confidences_first": [0.6129752397537231, 0.9925509691238403, 0.5050157904624939, 0.9032033085823059, 0.9993976354598999, 0.7335221171379089, 0.6447811722755432, 0.9999761581420898, 0.9880722761154175, 0.7510595917701721, 0.21356342732906342, 0.9997099041938782, 0.6546486020088196, 0.5557247400283813, 0.4826638996601105, 0.6189228892326355, 0.230464905500412, 0.4435559809207916, 0.9978049397468567, 0.5660431385040283], "token_confidences_second": null, "final_mean_entropy": 1.1051404739409918, "final_min_margin": 0.02172565460205078, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various fitness-related topics,", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various fitness-related topics,", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various fitness-related topics,", "raw_answer_first": "The image you've provided appears to be a collage of various fitness-related topics,", "raw_answer_second": null, "mean_entropy_first": 0.9671156433054421, "normalized_entropy_first": -0.073822002839412, "min_margin_first": 0.17658233642578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1056, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1060, "total_latency_s": 1.06, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.212852954864502, 0.028204526752233505, 1.1863281726837158, 0.3375132381916046, 0.00590519281104207, 0.6515482068061829, 0.9278761744499207, 0.00013116681657265872, 0.025502800941467285, 0.6669965982437134, 2.9989445209503174, 0.003978257533162832, 1.2488161325454712, 1.2637460231781006, 3.1511197090148926, 0.0009513012482784688, 1.1507353782653809, 0.10099979490041733, 2.5244388580322266, 1.8557238578796387], "entropies_second": null, "final_normalized_entropy": -0.073822002839412, "sequence_confidence_first": 0.6699088046204767, "sequence_confidence_second": null, "sequence_confidence_final": 0.6699088046204767, "token_confidences_first": [0.5633822083473206, 0.9971718192100525, 0.6734333038330078, 0.9193960428237915, 0.9994733929634094, 0.7163655161857605, 0.6244301795959473, 0.9999921321868896, 0.9967859983444214, 0.6985122561454773, 0.318217396736145, 0.999622106552124, 0.5194701552391052, 0.7684325575828552, 0.2931012511253357, 0.9999250173568726, 0.5878530740737915, 0.9834232330322266, 0.39914438128471375, 0.35639411211013794], "token_confidences_second": null, "final_mean_entropy": 0.9671156433054421, "final_min_margin": 0.17658233642578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic that appears to be a ranking of the top", "used_ocr": false, "answer_first": "The image you've provided is a graphic that appears to be a ranking of the top", "answer_second": null, "raw_answer": "The image you've provided is a graphic that appears to be a ranking of the top", "raw_answer_first": "The image you've provided is a graphic that appears to be a ranking of the top", "raw_answer_second": null, "mean_entropy_first": 1.2368743370170705, "normalized_entropy_first": 1.0175706229138242, "min_margin_first": 0.017822265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1200, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1201, "total_latency_s": 1.201, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9358630180358887, 0.13840419054031372, 1.7904908657073975, 0.3142429292201996, 0.002620287472382188, 0.6955706477165222, 0.9090398550033569, 0.9021276235580444, 3.0504918098449707, 0.5016101002693176, 3.143004894256592, 2.3966064453125, 0.003877622541040182, 1.1126012802124023, 1.9913330078125, 3.1112403869628906, 1.0964457988739014, 1.4132716655731201, 0.8293745517730713, 0.3992697596549988], "entropies_second": null, "final_normalized_entropy": 1.0175706229138242, "sequence_confidence_first": 0.5467358107328469, "sequence_confidence_second": null, "sequence_confidence_final": 0.5467358107328469, "token_confidences_first": [0.49160483479499817, 0.9782293438911438, 0.3759291172027588, 0.9260894656181335, 0.9997304081916809, 0.6908411979675293, 0.6027740240097046, 0.5662238597869873, 0.29013094305992126, 0.8087557554244995, 0.19504721462726593, 0.2887129485607147, 0.9996706247329712, 0.7588428258895874, 0.3876102566719055, 0.21398679912090302, 0.5920407772064209, 0.4543188214302063, 0.7274834513664246, 0.8871293067932129], "token_confidences_second": null, "final_mean_entropy": 1.2368743370170705, "final_min_margin": 0.017822265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic from the Food Banks Canada organization, which is a national charity", "used_ocr": false, "answer_first": "This image is a graphic from the Food Banks Canada organization, which is a national charity", "answer_second": null, "raw_answer": "This image is a graphic from the Food Banks Canada organization, which is a national charity", "raw_answer_first": "This image is a graphic from the Food Banks Canada organization, which is a national charity", "raw_answer_second": null, "mean_entropy_first": 1.5161132981447736, "normalized_entropy_first": 2.0486072855424466, "min_margin_first": 0.011893272399902344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1401, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1402, "total_latency_s": 1.402, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7655328512191772, 1.1126333475112915, 1.0848596096038818, 0.8228716254234314, 2.539472818374634, 0.41370269656181335, 2.817533016204834, 1.4679316282272339, 1.6390715837478638, 0.0953565239906311, 0.0006510343519039452, 0.08951375633478165, 2.501518726348877, 1.2711639404296875, 2.8954548835754395, 2.425739288330078, 3.1663975715637207, 2.786863088607788, 1.7321710586547852, 0.6938269138336182], "entropies_second": null, "final_normalized_entropy": 2.0486072855424466, "sequence_confidence_first": 0.4884074968499131, "sequence_confidence_second": null, "sequence_confidence_final": 0.4884074968499131, "token_confidences_first": [0.5517432689666748, 0.5288025736808777, 0.5151104927062988, 0.5684738755226135, 0.40574538707733154, 0.8626232147216797, 0.29864028096199036, 0.512898862361908, 0.5158271193504333, 0.9862344264984131, 0.9999463558197021, 0.9842431545257568, 0.3007766902446747, 0.6926371455192566, 0.3187871277332306, 0.42417633533477783, 0.19451765716075897, 0.21505653858184814, 0.4363190531730652, 0.5059103965759277], "token_confidences_second": null, "final_mean_entropy": 1.5161132981447736, "final_min_margin": 0.011893272399902344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic with a focus on the Millennium Development Goals (MD", "used_ocr": false, "answer_first": "The image is a graphic with a focus on the Millennium Development Goals (MD", "answer_second": null, "raw_answer": "The image is a graphic with a focus on the Millennium Development Goals (MD", "raw_answer_first": "The image is a graphic with a focus on the Millennium Development Goals (MD", "raw_answer_second": null, "mean_entropy_first": 0.8993459280305615, "normalized_entropy_first": -0.5762614369911883, "min_margin_first": 0.01688385009765625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1403, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1404, "total_latency_s": 1.404, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7523524761199951, 0.04309726506471634, 1.6341251134872437, 0.6266385912895203, 2.52687668800354, 0.20086811482906342, 2.5357134342193604, 1.9336235523223877, 3.533142566680908, 0.0038419365882873535, 2.437957286834717, 1.1865642070770264, 0.026232223957777023, 1.9341507140779868e-05, 0.00039082521107047796, 0.005193848628550768, 0.0019080431666225195, 0.1331799477338791, 0.37292051315307617, 0.0322725847363472], "entropies_second": null, "final_normalized_entropy": -0.5762614369911883, "sequence_confidence_first": 0.6656942436973583, "sequence_confidence_second": null, "sequence_confidence_final": 0.6656942436973583, "token_confidences_first": [0.5867044925689697, 0.9946885108947754, 0.3243906497955322, 0.7498003840446472, 0.38422924280166626, 0.9497557282447815, 0.29768985509872437, 0.4398457407951355, 0.14433589577674866, 0.999671220779419, 0.42643609642982483, 0.7817525267601013, 0.9964954257011414, 0.9999988079071045, 0.9999703168869019, 0.9994947910308838, 0.9998458623886108, 0.9708837270736694, 0.9301161766052246, 0.9955751895904541], "token_confidences_second": null, "final_mean_entropy": 0.8993459280305615, "final_min_margin": 0.01688385009765625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Working Poor\" by Think Mont", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Working Poor\" by Think Mont", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Working Poor\" by Think Mont", "raw_answer_first": "The image you've provided is an infographic titled \"The Working Poor\" by Think Mont", "raw_answer_second": null, "mean_entropy_first": 0.9037602998054354, "normalized_entropy_first": -0.5214488468366985, "min_margin_first": 0.04082298278808594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1358, "total_latency_s": 1.358, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.767417311668396, 0.08391280472278595, 1.6672554016113281, 0.24844743311405182, 0.003555900417268276, 0.7331569790840149, 0.8454583883285522, 1.0300542116165161, 0.4926467537879944, 0.010149267502129078, 1.2659810781478882, 0.017812103033065796, 1.066453456878662, 0.13394621014595032, 0.087657630443573, 0.0006014215177856386, 2.302868366241455, 1.8431251049041748, 2.543135404586792, 2.9315707683563232], "entropies_second": null, "final_normalized_entropy": -0.5214488468366985, "sequence_confidence_first": 0.6695460583077624, "sequence_confidence_second": null, "sequence_confidence_final": 0.6695460583077624, "token_confidences_first": [0.6555983424186707, 0.9887005686759949, 0.3974742889404297, 0.9465713500976562, 0.9996235370635986, 0.6564112305641174, 0.5822427272796631, 0.5104416608810425, 0.8798603415489197, 0.9988395571708679, 0.7390099167823792, 0.9980680346488953, 0.6690781116485596, 0.9741756319999695, 0.9855871200561523, 0.9999537467956543, 0.1952187567949295, 0.4771656394004822, 0.42474687099456787, 0.41836145520210266], "token_confidences_second": null, "final_mean_entropy": 0.9037602998054354, "final_min_margin": 0.04082298278808594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a promotional graphic for a community called \"", "used_ocr": false, "answer_first": "The image you've provided appears to be a promotional graphic for a community called \"", "answer_second": null, "raw_answer": "The image you've provided appears to be a promotional graphic for a community called \"", "raw_answer_first": "The image you've provided appears to be a promotional graphic for a community called \"", "raw_answer_second": null, "mean_entropy_first": 0.7323034850996919, "normalized_entropy_first": -1.1499308562208395, "min_margin_first": 0.031490325927734375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1463, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1465, "total_latency_s": 1.465, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9546111822128296, 0.024903636425733566, 1.5517199039459229, 0.29121631383895874, 0.0020836645271629095, 0.7142730951309204, 0.794681966304779, 6.24830718152225e-05, 0.017165090888738632, 0.3209834694862366, 2.182962417602539, 0.0014824379468336701, 0.00033079314744099975, 1.4449595212936401, 0.0023941078688949347, 0.8660064339637756, 1.282759428024292, 1.5993690490722656, 1.9182312488555908, 0.6758734583854675], "entropies_second": null, "final_normalized_entropy": -1.1499308562208395, "sequence_confidence_first": 0.720838504485202, "sequence_confidence_second": null, "sequence_confidence_final": 0.720838504485202, "token_confidences_first": [0.4837951362133026, 0.9972776770591736, 0.39882636070251465, 0.9334188103675842, 0.9997991919517517, 0.6543429493904114, 0.6245619654655457, 0.9999961853027344, 0.9977808594703674, 0.9252750277519226, 0.4545119106769562, 0.9998691082000732, 0.999976396560669, 0.5317113995552063, 0.9997629523277283, 0.6483271718025208, 0.45225790143013, 0.6722849011421204, 0.5144993662834167, 0.8642784357070923], "token_confidences_second": null, "final_mean_entropy": 0.7323034850996919, "final_min_margin": 0.031490325927734375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Men vs. Women: The Economic", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Men vs. Women: The Economic", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Men vs. Women: The Economic", "raw_answer_first": "The image you've provided is an infographic titled \"Men vs. Women: The Economic", "raw_answer_second": null, "mean_entropy_first": 0.49127388224005697, "normalized_entropy_first": -1.9469060142119041, "min_margin_first": 0.2907142639160156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1450, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1462, "total_latency_s": 1.462, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.845037579536438, 0.08526809513568878, 1.7641624212265015, 0.30015450716018677, 0.0038540095556527376, 0.6437690258026123, 0.9061729907989502, 0.8544799089431763, 0.28955239057540894, 0.010281406342983246, 2.1859538555145264, 0.012699245475232601, 1.2425421476364136, 0.21264389157295227, 0.19379492104053497, 0.04952637478709221, 0.07095647603273392, 0.11250568926334381, 0.04105369374155998, 0.001069014659151435], "entropies_second": null, "final_normalized_entropy": -1.9469060142119041, "sequence_confidence_first": 0.7957426767402509, "sequence_confidence_second": null, "sequence_confidence_final": 0.7957426767402509, "token_confidences_first": [0.5999152064323425, 0.9888823628425598, 0.40257611870765686, 0.9309067130088806, 0.9995983242988586, 0.731683611869812, 0.5485015511512756, 0.5798497796058655, 0.9470628499984741, 0.9987951517105103, 0.3849083483219147, 0.9987020492553711, 0.6252121329307556, 0.9618750214576721, 0.9526761770248413, 0.9918307662010193, 0.9914587140083313, 0.9835631847381592, 0.9945104122161865, 0.9999043941497803], "token_confidences_second": null, "final_mean_entropy": 0.49127388224005697, "final_min_margin": 0.2907142639160156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a graphical representation of data related", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a graphical representation of data related", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a graphical representation of data related", "raw_answer_first": "The image you've provided appears to be an infographic or a graphical representation of data related", "raw_answer_second": null, "mean_entropy_first": 1.1222755318980489, "normalized_entropy_first": 0.593015372332873, "min_margin_first": 0.0040645599365234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1201, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1202, "total_latency_s": 1.202, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9092379808425903, 0.15834370255470276, 1.7018728256225586, 0.34967613220214844, 0.0035206195898354053, 0.6724495887756348, 1.0164114236831665, 0.00011560927669052035, 0.04835887253284454, 0.7406817674636841, 0.43345069885253906, 0.011937776580452919, 2.279752731323242, 2.3894684314727783, 3.4090895652770996, 1.7918674945831299, 1.2990138530731201, 2.0147175788879395, 2.0297136306762695, 1.1858303546905518], "entropies_second": null, "final_normalized_entropy": 0.593015372332873, "sequence_confidence_first": 0.5862684235322273, "sequence_confidence_second": null, "sequence_confidence_final": 0.5862684235322273, "token_confidences_first": [0.6353517174720764, 0.9737023115158081, 0.4488033354282379, 0.9153305292129517, 0.9996709823608398, 0.6889357566833496, 0.511107325553894, 0.999992847442627, 0.9939633011817932, 0.5670828223228455, 0.9003475308418274, 0.9985730648040771, 0.40054887533187866, 0.25995734333992004, 0.11286124587059021, 0.4091865122318268, 0.7149390578269958, 0.5336217284202576, 0.3646007478237152, 0.7588573098182678], "token_confidences_second": null, "final_mean_entropy": 1.1222755318980489, "final_min_margin": 0.0040645599365234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic that appears to be a survey or infographic related", "used_ocr": false, "answer_first": "The image you've provided is a graphic that appears to be a survey or infographic related", "answer_second": null, "raw_answer": "The image you've provided is a graphic that appears to be a survey or infographic related", "raw_answer_first": "The image you've provided is a graphic that appears to be a survey or infographic related", "raw_answer_second": null, "mean_entropy_first": 1.3592837284668349, "normalized_entropy_first": 1.395756810212104, "min_margin_first": 0.05471611022949219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1329, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1330, "total_latency_s": 1.33, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9072444438934326, 0.031833209097385406, 1.522145390510559, 0.23399826884269714, 0.003824827028438449, 0.7243030667304993, 0.9314058423042297, 0.8708593249320984, 3.1491479873657227, 0.4854338765144348, 2.591794967651367, 2.5149831771850586, 0.0027695558965206146, 0.943084716796875, 2.3607778549194336, 3.6880593299865723, 1.3237452507019043, 2.3624987602233887, 0.010887792333960533, 2.526876926422119], "entropies_second": null, "final_normalized_entropy": 1.395756810212104, "sequence_confidence_first": 0.5261197485760104, "sequence_confidence_second": null, "sequence_confidence_final": 0.5261197485760104, "token_confidences_first": [0.5019367337226868, 0.9962254762649536, 0.5290035605430603, 0.9509149789810181, 0.9995853304862976, 0.6174276471138, 0.4902760684490204, 0.5463913679122925, 0.19812358915805817, 0.8223153948783875, 0.2830774486064911, 0.41400429606437683, 0.9997708201408386, 0.8124735951423645, 0.32254692912101746, 0.1682487577199936, 0.7325038909912109, 0.34918951988220215, 0.9987602233886719, 0.295299768447876], "token_confidences_second": null, "final_mean_entropy": 1.3592837284668349, "final_min_margin": 0.05471611022949219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Spending Habits Men vs. Women", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Spending Habits Men vs. Women", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Spending Habits Men vs. Women", "raw_answer_first": "The image you've provided is an infographic titled \"Spending Habits Men vs. Women", "raw_answer_second": null, "mean_entropy_first": 0.5111143223126419, "normalized_entropy_first": -1.697181192557583, "min_margin_first": 0.07005119323730469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1411, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1413, "total_latency_s": 1.413, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8254462480545044, 0.2133975625038147, 1.6984081268310547, 0.28634411096572876, 0.006446198560297489, 0.7274631261825562, 0.855708122253418, 0.9482558369636536, 0.367284893989563, 0.010904498398303986, 1.5630900859832764, 0.014371241442859173, 1.0047111511230469, 0.0024884298909455538, 0.06579114496707916, 0.0023274561390280724, 1.1580315828323364, 0.10891348868608475, 0.35224637389183044, 0.010656766593456268], "entropies_second": null, "final_normalized_entropy": -1.697181192557583, "sequence_confidence_first": 0.7744356344992631, "sequence_confidence_second": null, "sequence_confidence_final": 0.7744356344992631, "token_confidences_first": [0.5246935486793518, 0.9613165855407715, 0.4193604588508606, 0.9355214238166809, 0.9992533326148987, 0.670653760433197, 0.6875300407409668, 0.48102012276649475, 0.9181986451148987, 0.9987176656723022, 0.48608630895614624, 0.9985155463218689, 0.5776047706604004, 0.9997678399085999, 0.9887185096740723, 0.9997774958610535, 0.6180338263511658, 0.9841662645339966, 0.889404296875, 0.998920202255249], "token_confidences_second": null, "final_mean_entropy": 0.5111143223126419, "final_min_margin": 0.07005119323730469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a promotional or informational graphic for a", "used_ocr": false, "answer_first": "The image you've provided appears to be a promotional or informational graphic for a", "answer_second": null, "raw_answer": "The image you've provided appears to be a promotional or informational graphic for a", "raw_answer_first": "The image you've provided appears to be a promotional or informational graphic for a", "raw_answer_second": null, "mean_entropy_first": 0.7730367804433627, "normalized_entropy_first": -0.5844566865906449, "min_margin_first": 0.0968780517578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1117, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1122, "total_latency_s": 1.122, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2453988790512085, 0.048425257205963135, 1.1831934452056885, 0.29162150621414185, 0.0019924002699553967, 0.6579973101615906, 0.8705489635467529, 6.310809112619609e-05, 0.024433264508843422, 0.6364535093307495, 3.431224822998047, 0.0021937850397080183, 0.0005876251962035894, 1.8303149938583374, 0.246076762676239, 0.3186083734035492, 1.406172752380371, 0.0036178317386657, 1.850126028060913, 1.4116849899291992], "entropies_second": null, "final_normalized_entropy": -0.5844566865906449, "sequence_confidence_first": 0.6952491832431259, "sequence_confidence_second": null, "sequence_confidence_final": 0.6952491832431259, "token_confidences_first": [0.5216136574745178, 0.9947618842124939, 0.681224524974823, 0.9334783554077148, 0.9998130202293396, 0.6939294338226318, 0.6054939031600952, 0.9999964237213135, 0.9967137575149536, 0.779484748840332, 0.17896421253681183, 0.9998041987419128, 0.999957799911499, 0.4892387390136719, 0.9476674199104309, 0.9045791029930115, 0.5522364974021912, 0.9996240139007568, 0.36928990483283997, 0.4226796627044678], "token_confidences_second": null, "final_mean_entropy": 0.7730367804433627, "final_min_margin": 0.0968780517578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Inspiring Women\" from the \"", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Inspiring Women\" from the \"", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Inspiring Women\" from the \"", "raw_answer_first": "The image you've provided is an infographic titled \"Inspiring Women\" from the \"", "raw_answer_second": null, "mean_entropy_first": 0.7251164389424958, "normalized_entropy_first": -0.7034331566058659, "min_margin_first": 0.12226676940917969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1529, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1544, "total_latency_s": 1.544, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9445613622665405, 0.17366504669189453, 1.6960887908935547, 0.2892706096172333, 0.0037028167862445116, 0.7306589484214783, 1.020062804222107, 1.0402287244796753, 0.29851454496383667, 0.014631589874625206, 0.9892334938049316, 0.007397486362606287, 0.15888437628746033, 0.006086570676416159, 0.005409326404333115, 0.15537016093730927, 1.3323595523834229, 1.8752264976501465, 1.359036922454834, 2.4019391536712646], "entropies_second": null, "final_normalized_entropy": -0.7034331566058659, "sequence_confidence_first": 0.7100083255399766, "sequence_confidence_second": null, "sequence_confidence_final": 0.7100083255399766, "token_confidences_first": [0.5451438426971436, 0.9692925810813904, 0.4533573091030121, 0.9352424740791321, 0.999618649482727, 0.630419909954071, 0.5203433632850647, 0.4903067946434021, 0.9380313754081726, 0.9982101917266846, 0.8095678091049194, 0.999254047870636, 0.971233606338501, 0.9993264675140381, 0.9993929862976074, 0.9754400849342346, 0.4622161388397217, 0.38664931058883667, 0.5764033794403076, 0.39855697751045227], "token_confidences_second": null, "final_mean_entropy": 0.7251164389424958, "final_min_margin": 0.12226676940917969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a collage of various statistics and information related to women in technology and education", "used_ocr": false, "answer_first": "This image appears to be a collage of various statistics and information related to women in technology and education", "answer_second": null, "raw_answer": "This image appears to be a collage of various statistics and information related to women in technology and education", "raw_answer_first": "This image appears to be a collage of various statistics and information related to women in technology and education", "raw_answer_second": null, "mean_entropy_first": 1.1101169895671774, "normalized_entropy_first": 0.656404392450654, "min_margin_first": 0.028703689575195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1374, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1375, "total_latency_s": 1.375, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8339513540267944, 0.8643071055412292, 0.9340516924858093, 0.00011058908421546221, 0.011732072569429874, 0.7134142518043518, 3.045325994491577, 0.003909048158675432, 1.0068033933639526, 1.8803040981292725, 2.6901018619537354, 0.7387055158615112, 2.3485684394836426, 1.2437342405319214, 0.01139346044510603, 0.821132242679596, 0.6540313959121704, 0.5691790580749512, 1.0735485553741455, 2.75803542137146], "entropies_second": null, "final_normalized_entropy": 0.656404392450654, "sequence_confidence_first": 0.5951883951066431, "sequence_confidence_second": null, "sequence_confidence_final": 0.5951883951066431, "token_confidences_first": [0.4951572120189667, 0.6414933204650879, 0.5702039003372192, 0.9999926090240479, 0.9988797307014465, 0.5952872633934021, 0.18773730099201202, 0.999626874923706, 0.5920372605323792, 0.5781351327896118, 0.3317038118839264, 0.7123327255249023, 0.36665964126586914, 0.744117796421051, 0.9987132549285889, 0.8214865922927856, 0.8256263136863708, 0.8857755661010742, 0.5260907411575317, 0.22109191119670868], "token_confidences_second": null, "final_mean_entropy": 1.1101169895671774, "final_min_margin": 0.028703689575195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an informational poster or flyer about menstruation", "used_ocr": false, "answer_first": "The image you've provided appears to be an informational poster or flyer about menstruation", "answer_second": null, "raw_answer": "The image you've provided appears to be an informational poster or flyer about menstruation", "raw_answer_first": "The image you've provided appears to be an informational poster or flyer about menstruation", "raw_answer_second": null, "mean_entropy_first": 0.745775731640606, "normalized_entropy_first": -0.6676354425618837, "min_margin_first": 0.021905899047851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1360, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1364, "total_latency_s": 1.364, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.026940107345581, 0.06447498500347137, 1.6574735641479492, 0.25431692600250244, 0.0034966873936355114, 0.7404708862304688, 0.8405258655548096, 8.192316454369575e-05, 0.009526163339614868, 0.7773541212081909, 1.230210542678833, 0.6087033152580261, 1.076514482498169, 1.1923167705535889, 2.394779682159424, 0.000526457850355655, 1.9726407527923584, 0.360080748796463, 0.00046279290108941495, 0.7046178579330444], "entropies_second": null, "final_normalized_entropy": -0.6676354425618837, "sequence_confidence_first": 0.651322600370461, "sequence_confidence_second": null, "sequence_confidence_final": 0.651322600370461, "token_confidences_first": [0.47198477387428284, 0.9912400841712952, 0.3865998089313507, 0.946099042892456, 0.9996558427810669, 0.6301513314247131, 0.520307183265686, 0.9999949932098389, 0.9990313053131104, 0.4986844062805176, 0.4714282751083374, 0.7072199583053589, 0.5359352231025696, 0.6892539262771606, 0.2203611582517624, 0.9999603033065796, 0.506887674331665, 0.9402981996536255, 0.9999638795852661, 0.5222985744476318], "token_confidences_second": null, "final_mean_entropy": 0.745775731640606, "final_min_margin": 0.021905899047851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or educational poster titled \"STRESS AND", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or educational poster titled \"STRESS AND", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or educational poster titled \"STRESS AND", "raw_answer_first": "The image you've provided appears to be an infographic or educational poster titled \"STRESS AND", "raw_answer_second": null, "mean_entropy_first": 0.7338037964331307, "normalized_entropy_first": -0.6630027356305725, "min_margin_first": 0.07883644104003906, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1195, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1200, "total_latency_s": 1.2, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.997435450553894, 0.05017302557826042, 1.3658127784729004, 0.3098597824573517, 0.0025308867916464806, 0.67165607213974, 0.9286902546882629, 5.961284477962181e-05, 0.023384544998407364, 0.7343198657035828, 0.7764673829078674, 0.012725629843771458, 2.064847946166992, 1.906403660774231, 1.6232850551605225, 2.073882818222046, 0.005861502140760422, 0.8162719011306763, 0.03275546059012413, 0.27965229749679565], "entropies_second": null, "final_normalized_entropy": -0.6630027356305725, "sequence_confidence_first": 0.6872986544591106, "sequence_confidence_second": null, "sequence_confidence_final": 0.6872986544591106, "token_confidences_first": [0.5842541456222534, 0.994125247001648, 0.6288954019546509, 0.9286408424377441, 0.9997532963752747, 0.692100465297699, 0.4971974492073059, 0.9999964237213135, 0.9972895383834839, 0.6074361205101013, 0.74364173412323, 0.9984670281410217, 0.371280312538147, 0.33436915278434753, 0.47174710035324097, 0.3150511682033539, 0.999421238899231, 0.6070259213447571, 0.9956105351448059, 0.9457325339317322], "token_confidences_second": null, "final_mean_entropy": 0.7338037964331307, "final_min_margin": 0.07883644104003906, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to women in business", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to women in business", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to women in business", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to women in business", "raw_answer_second": null, "mean_entropy_first": 0.8339458642316459, "normalized_entropy_first": -0.24202900834575156, "min_margin_first": 0.07598114013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1377, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1380, "total_latency_s": 1.38, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8705559968948364, 0.05682408809661865, 1.513869047164917, 0.28952500224113464, 0.003322155214846134, 0.6699528098106384, 0.8919129371643066, 5.51403354620561e-05, 0.021723400801420212, 0.6749511361122131, 3.4002652168273926, 0.5673910975456238, 1.8550031185150146, 2.1271519660949707, 0.012701893225312233, 1.77985680103302, 0.00031976966420188546, 1.638342261314392, 0.22106020152568817, 0.08413324505090714], "entropies_second": null, "final_normalized_entropy": -0.24202900834575156, "sequence_confidence_first": 0.6871550861801823, "sequence_confidence_second": null, "sequence_confidence_final": 0.6871550861801823, "token_confidences_first": [0.5181149244308472, 0.9926032423973083, 0.48543596267700195, 0.933333694934845, 0.9996579885482788, 0.694082498550415, 0.5378827452659607, 0.9999966621398926, 0.9972827434539795, 0.7207334637641907, 0.20332413911819458, 0.7618796229362488, 0.626960039138794, 0.49913662672042847, 0.9984710812568665, 0.6223733425140381, 0.9999765157699585, 0.310376912355423, 0.9554683566093445, 0.9866573214530945], "token_confidences_second": null, "final_mean_entropy": 0.8339458642316459, "final_min_margin": 0.07598114013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various infographics and statistics related", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various infographics and statistics related", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various infographics and statistics related", "raw_answer_first": "The image you've provided appears to be a collage of various infographics and statistics related", "raw_answer_second": null, "mean_entropy_first": 0.9858049210619356, "normalized_entropy_first": 0.36645151558266004, "min_margin_first": 0.2494821548461914, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1158, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1161, "total_latency_s": 1.161, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1779085397720337, 0.05569872260093689, 1.2398252487182617, 0.30856752395629883, 0.0021762100514024496, 0.6422837972640991, 0.9167597889900208, 7.832834671717137e-05, 0.03902699425816536, 0.4713927209377289, 3.373818874359131, 0.0026127833407372236, 0.7774819135665894, 2.4370903968811035, 2.766528606414795, 0.2937415838241577, 0.0008548182086087763, 1.8350515365600586, 2.351482391357422, 1.0237176418304443], "entropies_second": null, "final_normalized_entropy": 0.36645151558266004, "sequence_confidence_first": 0.6529384495926999, "sequence_confidence_second": null, "sequence_confidence_final": 0.6529384495926999, "token_confidences_first": [0.51686030626297, 0.9936515092849731, 0.6345474123954773, 0.9283629655838013, 0.9998002648353577, 0.7146357893943787, 0.6491754055023193, 0.9999953508377075, 0.9944962859153748, 0.8559741377830505, 0.1745203733444214, 0.9997605681419373, 0.6950398683547974, 0.413085013628006, 0.2831452488899231, 0.9164137840270996, 0.9999204874038696, 0.3556312322616577, 0.44841817021369934, 0.8011117577552795], "token_confidences_second": null, "final_mean_entropy": 0.9858049210619356, "final_min_margin": 0.2494821548461914, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to social media and priv", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to social media and priv", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to social media and priv", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to social media and priv", "raw_answer_second": null, "mean_entropy_first": 0.9325641833665941, "normalized_entropy_first": 0.12676692180997912, "min_margin_first": 0.04019355773925781, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1357, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1360, "total_latency_s": 1.36, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8722595572471619, 0.02155129984021187, 1.536323070526123, 0.2363005429506302, 0.0018713853787630796, 0.6841363310813904, 0.9104089140892029, 5.2304585551610216e-05, 0.019667256623506546, 0.7767461538314819, 3.0860953330993652, 0.3779851496219635, 1.633793830871582, 2.102076768875122, 2.237396717071533, 0.00014111425844021142, 2.1024391651153564, 0.0028873516712337732, 1.3685212135314941, 0.6806302070617676], "entropies_second": null, "final_normalized_entropy": 0.12676692180997912, "sequence_confidence_first": 0.6321092588808045, "sequence_confidence_second": null, "sequence_confidence_final": 0.6321092588808045, "token_confidences_first": [0.5080485343933105, 0.9976637363433838, 0.5040128231048584, 0.9501057267189026, 0.9998340606689453, 0.6801967620849609, 0.5046083331108093, 0.9999969005584717, 0.9976703524589539, 0.5029711127281189, 0.3125954270362854, 0.8767566084861755, 0.6801843047142029, 0.31571164727211, 0.38401922583580017, 0.9999909400939941, 0.25902995467185974, 0.9997549653053284, 0.5118158459663391, 0.8283411860466003], "token_confidences_second": null, "final_mean_entropy": 0.9325641833665941, "final_min_margin": 0.04019355773925781, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to the topic of women", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to the topic of women", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to the topic of women", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to the topic of women", "raw_answer_second": null, "mean_entropy_first": 1.0525723724051204, "normalized_entropy_first": 0.6390662007645819, "min_margin_first": 0.00605010986328125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1079, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1080, "total_latency_s": 1.08, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1128203868865967, 0.10782161355018616, 1.5591825246810913, 0.3308490812778473, 0.004355947487056255, 0.6788961291313171, 0.9965456128120422, 0.00013915014278609306, 0.02986392378807068, 0.7278362512588501, 3.2682418823242188, 0.39724528789520264, 1.5981152057647705, 1.7248785495758057, 2.2146108150482178, 0.00037477764999493957, 2.356834888458252, 2.258037805557251, 0.15533027052879333, 1.5294673442840576], "entropies_second": null, "final_normalized_entropy": 0.6390662007645819, "sequence_confidence_first": 0.610157318295727, "sequence_confidence_second": null, "sequence_confidence_final": 0.610157318295727, "token_confidences_first": [0.5601035952568054, 0.9855967164039612, 0.5372959971427917, 0.9219790697097778, 0.9996022582054138, 0.708825945854187, 0.5240129232406616, 0.9999911785125732, 0.9964706897735596, 0.5427757501602173, 0.2992470860481262, 0.8702716827392578, 0.6515459418296814, 0.400496244430542, 0.2792224586009979, 0.9999736547470093, 0.18751868605613708, 0.564888060092926, 0.966532826423645, 0.4794612526893616], "token_confidences_second": null, "final_mean_entropy": 1.0525723724051204, "final_min_margin": 0.00605010986328125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a promotional poster or advertisement for the Wimbledon", "used_ocr": false, "answer_first": "This image appears to be a promotional poster or advertisement for the Wimbledon", "answer_second": null, "raw_answer": "This image appears to be a promotional poster or advertisement for the Wimbledon", "raw_answer_first": "This image appears to be a promotional poster or advertisement for the Wimbledon", "raw_answer_second": null, "mean_entropy_first": 0.7080927134105878, "normalized_entropy_first": -0.9446898652081201, "min_margin_first": 0.00301361083984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1376, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1377, "total_latency_s": 1.377, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8611295223236084, 0.7956415414810181, 0.8461662530899048, 6.603884685318917e-05, 0.019379448145627975, 0.6293824315071106, 3.20002818107605, 0.002204976277425885, 0.0002498644171282649, 1.6517095565795898, 0.8548681735992432, 2.2221975326538086, 0.004470186773687601, 0.0008975518867373466, 0.8303453922271729, 0.7485308647155762, 1.4596331119537354, 0.030853837728500366, 0.0010770255466923118, 0.003022777382284403], "entropies_second": null, "final_normalized_entropy": -0.9446898652081201, "sequence_confidence_first": 0.7073172718763043, "sequence_confidence_second": null, "sequence_confidence_final": 0.7073172718763043, "token_confidences_first": [0.48521116375923157, 0.5829582810401917, 0.6783273220062256, 0.9999959468841553, 0.9978214502334595, 0.75320965051651, 0.20034927129745483, 0.9998001456260681, 0.9999830722808838, 0.39503902196884155, 0.5980085730552673, 0.4708960950374603, 0.9995500445365906, 0.9999208450317383, 0.809502124786377, 0.8285422325134277, 0.4580009877681732, 0.9962108135223389, 0.9999041557312012, 0.9996910095214844], "token_confidences_second": null, "final_mean_entropy": 0.7080927134105878, "final_min_margin": 0.00301361083984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a graphic or infographic related to a survey or study about the prefer", "used_ocr": false, "answer_first": "The image appears to be a graphic or infographic related to a survey or study about the prefer", "answer_second": null, "raw_answer": "The image appears to be a graphic or infographic related to a survey or study about the prefer", "raw_answer_first": "The image appears to be a graphic or infographic related to a survey or study about the prefer", "raw_answer_second": null, "mean_entropy_first": 1.462966336988029, "normalized_entropy_first": 2.547334993430895, "min_margin_first": 0.046001434326171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1354, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8287614583969116, 0.12766413390636444, 1.7042996883392334, 0.0002939247351605445, 0.05878371745347977, 0.7402175068855286, 3.227181911468506, 0.301594078540802, 2.1307218074798584, 1.8126274347305298, 0.01384328305721283, 2.326683521270752, 0.0002960481506306678, 2.9476547241210938, 1.7270419597625732, 0.9602707028388977, 2.2310898303985596, 1.9392225742340088, 2.7880778312683105, 3.393000602722168], "entropies_second": null, "final_normalized_entropy": 2.547334993430895, "sequence_confidence_first": 0.5287428035179056, "sequence_confidence_second": null, "sequence_confidence_final": 0.5287428035179056, "token_confidences_first": [0.5576153993606567, 0.9795332551002502, 0.3302963972091675, 0.9999781847000122, 0.9928695559501648, 0.5227991342544556, 0.335494726896286, 0.9181253910064697, 0.5007330179214478, 0.5282984972000122, 0.9983378648757935, 0.35221585631370544, 0.9999810457229614, 0.20823262631893158, 0.5420249700546265, 0.7868760228157043, 0.4541815221309662, 0.36984047293663025, 0.3575475811958313, 0.2038029134273529], "token_confidences_second": null, "final_mean_entropy": 1.462966336988029, "final_min_margin": 0.046001434326171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a graphic or poster related to social media usage statistics. It includes the", "used_ocr": false, "answer_first": "The image appears to be a graphic or poster related to social media usage statistics. It includes the", "answer_second": null, "raw_answer": "The image appears to be a graphic or poster related to social media usage statistics. It includes the", "raw_answer_first": "The image appears to be a graphic or poster related to social media usage statistics. It includes the", "raw_answer_second": null, "mean_entropy_first": 1.177020588198502, "normalized_entropy_first": 0.8228495184770686, "min_margin_first": 0.013471603393554688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1235, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1236, "total_latency_s": 1.236, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8840715885162354, 0.10396026074886322, 1.8182297945022583, 0.0002531963982619345, 0.04922398179769516, 0.7439733147621155, 2.77262544631958, 0.24437998235225677, 1.5151336193084717, 2.144280433654785, 1.9631516933441162, 0.00010492940782569349, 0.6030922532081604, 0.003879884723573923, 2.0239923000335693, 1.917619228363037, 0.8522740602493286, 1.2909324169158936, 2.6024866104125977, 2.006746768951416], "entropies_second": null, "final_normalized_entropy": 0.8228495184770686, "sequence_confidence_first": 0.5762379343582302, "sequence_confidence_second": null, "sequence_confidence_final": 0.5762379343582302, "token_confidences_first": [0.5680614113807678, 0.9853165745735168, 0.29863908886909485, 0.9999816417694092, 0.9938765168190002, 0.5114184021949768, 0.38456636667251587, 0.938226044178009, 0.643464982509613, 0.30592361092567444, 0.5573297142982483, 0.9999934434890747, 0.8523337244987488, 0.9996623992919922, 0.3756919801235199, 0.2986178994178772, 0.7771996259689331, 0.5421343445777893, 0.3037576973438263, 0.39601781964302063], "token_confidences_second": null, "final_mean_entropy": 1.177020588198502, "final_min_margin": 0.013471603393554688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to social media rec", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to social media rec", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to social media rec", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to social media rec", "raw_answer_second": null, "mean_entropy_first": 0.9151710736921814, "normalized_entropy_first": -0.23391386344127577, "min_margin_first": 0.40300941467285156, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 990, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 991, "total_latency_s": 0.991, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.6404545307159424, 0.12005162239074707, 1.789612889289856, 0.3659486174583435, 0.002466170582920313, 0.6847057342529297, 1.1679151058197021, 0.00030625605722889304, 0.06863328069448471, 0.6577699184417725, 3.0028796195983887, 0.45309823751449585, 1.6719043254852295, 2.2138593196868896, 0.01418579276651144, 1.8397259712219238, 0.0002388221473665908, 1.4085849523544312, 0.057543739676475525, 1.1435365676879883], "entropies_second": null, "final_normalized_entropy": -0.23391386344127577, "sequence_confidence_first": 0.6922897694041356, "sequence_confidence_second": null, "sequence_confidence_final": 0.6922897694041356, "token_confidences_first": [0.45845308899879456, 0.9851970672607422, 0.4126022756099701, 0.9106374979019165, 0.9997783303260803, 0.6963569521903992, 0.570769727230072, 0.9999803304672241, 0.9906186461448669, 0.6889411211013794, 0.274019330739975, 0.8477821946144104, 0.6435493230819702, 0.3279065191745758, 0.9983056783676147, 0.5836445689201355, 0.9999841451644897, 0.6527369618415833, 0.9907537698745728, 0.7521077394485474], "token_confidences_second": null, "final_mean_entropy": 0.9151710736921814, "final_min_margin": 0.40300941467285156, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "ANSWER", "used_ocr": false, "answer_first": "ANSWER", "answer_second": null, "raw_answer": "ANSWER: [ANSWER]", "raw_answer_first": "ANSWER: [ANSWER]", "raw_answer_second": null, "mean_entropy_first": 0.8081212989985943, "normalized_entropy_first": -0.6465036049727703, "min_margin_first": 0.5596389770507812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 720, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 721, "total_latency_s": 0.721, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.6235125064849854, 0.09382675588130951, 0.004538728389889002, 0.00639759236946702, 0.042812857776880264, 1.6650291681289673, 4.571063041687012, 0.01618674211204052, 0.005011164583265781, 0.05283443257212639], "entropies_second": null, "final_normalized_entropy": -0.6465036049727703, "sequence_confidence_first": 0.7988412341805936, "sequence_confidence_second": null, "sequence_confidence_final": 0.7988412341805936, "token_confidences_first": [0.5209434628486633, 0.9907692074775696, 0.9996116757392883, 0.9993948936462402, 0.9948837161064148, 0.6521471738815308, 0.2548738718032837, 0.9985106587409973, 0.9995143413543701, 0.9952712655067444, 0.9981762170791626], "token_confidences_second": null, "final_mean_entropy": 0.8081212989985943, "final_min_margin": 0.5596389770507812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic that appears to be related to a survey or report on the use of", "used_ocr": false, "answer_first": "This image is a graphic that appears to be related to a survey or report on the use of", "answer_second": null, "raw_answer": "This image is a graphic that appears to be related to a survey or report on the use of", "raw_answer_first": "This image is a graphic that appears to be related to a survey or report on the use of", "raw_answer_second": null, "mean_entropy_first": 1.3463585779478309, "normalized_entropy_first": 1.6069366654900274, "min_margin_first": 0.0382232666015625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1420, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1422, "total_latency_s": 1.422, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7866197824478149, 0.976681113243103, 1.0184144973754883, 0.9067014455795288, 2.790398597717285, 0.5462836623191833, 2.8682479858398438, 2.4342598915100098, 0.01327520702034235, 0.9434581995010376, 2.490365505218506, 0.00043861556332558393, 1.4835882186889648, 1.598574161529541, 1.2877405881881714, 1.2665187120437622, 1.4614399671554565, 1.6279882192611694, 2.2764792442321777, 0.149697944521904], "entropies_second": null, "final_normalized_entropy": 1.6069366654900274, "sequence_confidence_first": 0.5168983204984846, "sequence_confidence_second": null, "sequence_confidence_final": 0.5168983204984846, "token_confidences_first": [0.5250199437141418, 0.5177084803581238, 0.49651530385017395, 0.527987539768219, 0.30466708540916443, 0.7705616354942322, 0.23062413930892944, 0.4105389416217804, 0.9986392855644226, 0.8237948417663574, 0.25026804208755493, 0.9999692440032959, 0.6203992366790771, 0.3669379651546478, 0.6571671962738037, 0.5705977082252502, 0.4873942732810974, 0.36059093475341797, 0.39186805486679077, 0.9670932292938232], "token_confidences_second": null, "final_mean_entropy": 1.3463585779478309, "final_min_margin": 0.0382232666015625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a timeline or infographic summarizing the history of", "used_ocr": false, "answer_first": "The image you've provided appears to be a timeline or infographic summarizing the history of", "answer_second": null, "raw_answer": "The image you've provided appears to be a timeline or infographic summarizing the history of", "raw_answer_first": "The image you've provided appears to be a timeline or infographic summarizing the history of", "raw_answer_second": null, "mean_entropy_first": 0.7886546885347343, "normalized_entropy_first": -0.7907231073771723, "min_margin_first": 0.07374000549316406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1190, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1191, "total_latency_s": 1.191, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2630729675292969, 0.04874753579497337, 1.622788429260254, 0.3018393814563751, 0.002350762952119112, 0.6864297389984131, 1.0422606468200684, 0.00014397871564142406, 0.08680471777915955, 0.6109107732772827, 2.9303691387176514, 0.011003904044628143, 2.0554800033569336, 1.6555001735687256, 0.012607102282345295, 2.492616891860962, 0.0032743941992521286, 0.19745342433452606, 0.44942009449005127, 0.3000197112560272], "entropies_second": null, "final_normalized_entropy": -0.7907231073771723, "sequence_confidence_first": 0.6897421496012497, "sequence_confidence_second": null, "sequence_confidence_final": 0.6897421496012497, "token_confidences_first": [0.4448106288909912, 0.9947676658630371, 0.5077494978904724, 0.9301308989524841, 0.999779999256134, 0.7096117734909058, 0.583844006061554, 0.9999909400939941, 0.9879494905471802, 0.7417399287223816, 0.2521774172782898, 0.9988930821418762, 0.3128417432308197, 0.6052465438842773, 0.9985195994377136, 0.24292831122875214, 0.9996767044067383, 0.9683239459991455, 0.9067400097846985, 0.9220626354217529], "token_confidences_second": null, "final_mean_entropy": 0.7886546885347343, "final_min_margin": 0.07374000549316406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a chart that provides statistics and ins", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a chart that provides statistics and ins", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a chart that provides statistics and ins", "raw_answer_first": "The image you've provided appears to be an infographic or a chart that provides statistics and ins", "raw_answer_second": null, "mean_entropy_first": 1.2495615558040298, "normalized_entropy_first": 1.089092619999129, "min_margin_first": 0.017681121826171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1082, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1083, "total_latency_s": 1.083, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0467963218688965, 0.2536812126636505, 1.6547868251800537, 0.321697473526001, 0.0025123986415565014, 0.679737389087677, 1.2895784378051758, 6.082848267396912e-05, 0.05976945161819458, 0.7248862981796265, 0.13979986310005188, 0.010152098722755909, 2.4995217323303223, 2.267932891845703, 3.3720271587371826, 2.6040232181549072, 2.6917355060577393, 2.0148794651031494, 1.3699365854263306, 1.9877159595489502], "entropies_second": null, "final_normalized_entropy": 1.089092619999129, "sequence_confidence_first": 0.5218539860265912, "sequence_confidence_second": null, "sequence_confidence_final": 0.5218539860265912, "token_confidences_first": [0.6458244323730469, 0.9525769352912903, 0.5021694898605347, 0.9248918294906616, 0.9997679591178894, 0.6796720027923584, 0.4457613527774811, 0.999996542930603, 0.9918151497840881, 0.5035092830657959, 0.9754884839057922, 0.998831570148468, 0.25635865330696106, 0.39179959893226624, 0.16312876343727112, 0.34689295291900635, 0.22740763425827026, 0.32363125681877136, 0.45298993587493896, 0.2811836898326874], "token_confidences_second": null, "final_mean_entropy": 1.2495615558040298, "final_min_margin": 0.017681121826171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or poster related to the science behind happiness", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or poster related to the science behind happiness", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or poster related to the science behind happiness", "raw_answer_first": "The image you've provided appears to be an infographic or poster related to the science behind happiness", "raw_answer_second": null, "mean_entropy_first": 0.880974336926738, "normalized_entropy_first": -0.4711499776753795, "min_margin_first": 0.015459060668945312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1073, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1074, "total_latency_s": 1.074, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0527504682540894, 0.19578400254249573, 1.8616058826446533, 0.311826229095459, 0.006507519166916609, 0.7197723388671875, 0.97119140625, 0.0001417791354469955, 0.02990943193435669, 0.7980773448944092, 0.9237912893295288, 0.020570823922753334, 1.8019969463348389, 1.909301996231079, 2.3598151206970215, 0.0001656086096772924, 1.6960034370422363, 2.104832172393799, 0.7423115968704224, 0.11313134431838989], "entropies_second": null, "final_normalized_entropy": -0.4711499776753795, "sequence_confidence_first": 0.6734150656724781, "sequence_confidence_second": null, "sequence_confidence_final": 0.6734150656724781, "token_confidences_first": [0.6880882978439331, 0.9747811555862427, 0.35481172800064087, 0.9277269244194031, 0.9992745518684387, 0.6733143925666809, 0.5313559770584106, 0.9999908208847046, 0.9962508082389832, 0.49441203474998474, 0.7390977740287781, 0.9975019097328186, 0.5846330523490906, 0.4458859860897064, 0.29034584760665894, 0.9999890327453613, 0.5285817980766296, 0.44988760352134705, 0.7228297591209412, 0.9864138960838318], "token_confidences_second": null, "final_mean_entropy": 0.880974336926738, "final_min_margin": 0.015459060668945312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "ANSWER", "used_ocr": false, "answer_first": "ANSWER", "answer_second": null, "raw_answer": "ANSWER: [ANSWER]", "raw_answer_first": "ANSWER: [ANSWER]", "raw_answer_second": null, "mean_entropy_first": 0.7314326320076361, "normalized_entropy_first": -1.0546036579968574, "min_margin_first": 0.1190948486328125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 740, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 742, "total_latency_s": 0.742, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.6255792379379272, 0.04037325084209442, 0.00429474888369441, 0.0014923266135156155, 0.018594760447740555, 2.1733882427215576, 3.4078750610351562, 0.0069592236541211605, 0.0019653865601867437, 0.03380408138036728], "entropies_second": null, "final_normalized_entropy": -1.0546036579968574, "sequence_confidence_first": 0.7407971677363957, "sequence_confidence_second": null, "sequence_confidence_final": 0.7407971677363957, "token_confidences_first": [0.42447081208229065, 0.9960712194442749, 0.9996315240859985, 0.9998650550842285, 0.998102605342865, 0.3249087333679199, 0.27078506350517273, 0.9993756413459778, 0.9998155236244202, 0.9969883561134338, 0.9974080920219421], "token_confidences_second": null, "final_mean_entropy": 0.7314326320076361, "final_min_margin": 0.1190948486328125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a chart or infographic titled \"FAST CASUAL SOCI", "used_ocr": false, "answer_first": "This image appears to be a chart or infographic titled \"FAST CASUAL SOCI", "answer_second": null, "raw_answer": "This image appears to be a chart or infographic titled \"FAST CASUAL SOCI", "raw_answer_first": "This image appears to be a chart or infographic titled \"FAST CASUAL SOCI", "raw_answer_second": null, "mean_entropy_first": 0.7388023206383878, "normalized_entropy_first": -0.9189130427621169, "min_margin_first": 0.15857505798339844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1281, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1283, "total_latency_s": 1.283, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9017629623413086, 0.7637661099433899, 1.3936936855316162, 7.91987040429376e-05, 0.05711441859602928, 0.8000324368476868, 3.419468402862549, 2.030113935470581, 2.010795831680298, 0.009587747976183891, 2.008910655975342, 0.005448273848742247, 0.7968471050262451, 0.013725366443395615, 0.29595857858657837, 0.0002178570139221847, 0.0004618663224391639, 3.0926352337701246e-05, 0.266006737947464, 0.002024315297603607], "entropies_second": null, "final_normalized_entropy": -0.9189130427621169, "sequence_confidence_first": 0.7068543218885049, "sequence_confidence_second": null, "sequence_confidence_final": 0.7068543218885049, "token_confidences_first": [0.5269400477409363, 0.6398023962974548, 0.49696552753448486, 0.9999947547912598, 0.9915627241134644, 0.5348667502403259, 0.1646198332309723, 0.4600224196910858, 0.495321124792099, 0.9989005327224731, 0.5301894545555115, 0.9994731545448303, 0.6201630234718323, 0.9986516833305359, 0.9297370910644531, 0.9999854564666748, 0.9999666213989258, 0.9999980926513672, 0.9546445608139038, 0.9998363256454468], "token_confidences_second": null, "final_mean_entropy": 0.7388023206383878, "final_min_margin": 0.15857505798339844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a colorful infographic that presents various statistics related to social media", "used_ocr": false, "answer_first": "The image you've provided is a colorful infographic that presents various statistics related to social media", "answer_second": null, "raw_answer": "The image you've provided is a colorful infographic that presents various statistics related to social media", "raw_answer_first": "The image you've provided is a colorful infographic that presents various statistics related to social media", "raw_answer_second": null, "mean_entropy_first": 0.9741197067225584, "normalized_entropy_first": 0.1384453308979559, "min_margin_first": 0.14463424682617188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1276, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1278, "total_latency_s": 1.278, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3507356643676758, 0.2727447748184204, 2.0062344074249268, 0.38289135694503784, 0.0015556981088593602, 0.6792624592781067, 1.1966338157653809, 0.80002760887146, 2.817410469055176, 0.011484473943710327, 0.4787645936012268, 0.010376114398241043, 2.1971774101257324, 1.8399704694747925, 1.8177341222763062, 1.084874153137207, 1.391512393951416, 0.0006838074768893421, 1.1284821033477783, 0.013838238082826138], "entropies_second": null, "final_normalized_entropy": 0.1384453308979559, "sequence_confidence_first": 0.6393836172633482, "sequence_confidence_second": null, "sequence_confidence_final": 0.6393836172633482, "token_confidences_first": [0.5778617262840271, 0.9516192078590393, 0.3613324761390686, 0.9033232927322388, 0.9998629093170166, 0.6902140378952026, 0.474381685256958, 0.6702102422714233, 0.28905385732650757, 0.9987151622772217, 0.8969634175300598, 0.9987943172454834, 0.38828426599502563, 0.44729238748550415, 0.5572053790092468, 0.6600149869918823, 0.3552279770374298, 0.9999476671218872, 0.5651859641075134, 0.9983254075050354], "token_confidences_second": null, "final_mean_entropy": 0.9741197067225584, "final_min_margin": 0.14463424682617188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that provides an overview of Social Media Marketing. It includes various", "used_ocr": false, "answer_first": "This image is an infographic that provides an overview of Social Media Marketing. It includes various", "answer_second": null, "raw_answer": "This image is an infographic that provides an overview of Social Media Marketing. It includes various", "raw_answer_first": "This image is an infographic that provides an overview of Social Media Marketing. It includes various", "raw_answer_second": null, "mean_entropy_first": 0.8959894416886527, "normalized_entropy_first": -0.2098906412773932, "min_margin_first": 0.0012340545654296875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1450, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1454, "total_latency_s": 1.454, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7985324859619141, 0.9292967319488525, 1.3471193313598633, 0.8622667193412781, 0.3495292067527771, 0.010457987897098064, 2.329827308654785, 0.915976881980896, 1.9739171266555786, 0.03302735090255737, 7.958742935443297e-05, 0.016054078936576843, 1.4523019790649414, 0.001272339839488268, 0.0039224945940077305, 3.0526629416272044e-05, 1.3934448957443237, 1.3235719203948975, 2.4876291751861572, 1.691530704498291], "entropies_second": null, "final_normalized_entropy": -0.2098906412773932, "sequence_confidence_first": 0.6105286769663999, "sequence_confidence_second": null, "sequence_confidence_final": 0.6105286769663999, "token_confidences_first": [0.49446630477905273, 0.687700629234314, 0.4620364010334015, 0.5030284523963928, 0.9230467081069946, 0.9987764954566956, 0.29557281732559204, 0.8213353753089905, 0.3115104138851166, 0.9966171383857727, 0.9999946355819702, 0.9980006814002991, 0.374671995639801, 0.9998908042907715, 0.9996165037155151, 0.9999980926513672, 0.49248528480529785, 0.4598867893218994, 0.2851766049861908, 0.3905850052833557], "token_confidences_second": null, "final_mean_entropy": 0.8959894416886527, "final_min_margin": 0.0012340545654296875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a timeline of the social media landscape, showc", "used_ocr": false, "answer_first": "The image you've provided appears to be a timeline of the social media landscape, showc", "answer_second": null, "raw_answer": "The image you've provided appears to be a timeline of the social media landscape, showc", "raw_answer_first": "The image you've provided appears to be a timeline of the social media landscape, showc", "raw_answer_second": null, "mean_entropy_first": 0.9799734133339371, "normalized_entropy_first": 0.18696475422236616, "min_margin_first": 0.1025543212890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1046, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1048, "total_latency_s": 1.048, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.4167454242706299, 0.09542810916900635, 1.747286081314087, 0.3625456988811493, 0.003953978419303894, 0.7156533598899841, 1.1155937910079956, 0.0001901994546642527, 0.08396667242050171, 0.6262487173080444, 2.6763243675231934, 0.009986345656216145, 2.133172035217285, 0.7506076097488403, 1.255606770515442, 0.05176868662238121, 1.744791030883789, 1.577921748161316, 3.2315049171447754, 0.00017272307013627142], "entropies_second": null, "final_normalized_entropy": 0.18696475422236616, "sequence_confidence_first": 0.6273206800844824, "sequence_confidence_second": null, "sequence_confidence_final": 0.6273206800844824, "token_confidences_first": [0.42624834179878235, 0.9891688823699951, 0.46906912326812744, 0.9111853837966919, 0.9996814727783203, 0.6814447045326233, 0.5338127017021179, 0.9999877214431763, 0.9881147146224976, 0.7205661535263062, 0.30422553420066833, 0.9990205764770508, 0.33360716700553894, 0.7995129227638245, 0.5828094482421875, 0.9921877384185791, 0.4065875709056854, 0.4995662271976471, 0.2005232572555542, 0.9999884366989136], "token_confidences_second": null, "final_mean_entropy": 0.9799734133339371, "final_min_margin": 0.1025543212890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed map of the field of computer science. It includes various branches", "used_ocr": false, "answer_first": "This image is a colorful and detailed map of the field of computer science. It includes various branches", "answer_second": null, "raw_answer": "This image is a colorful and detailed map of the field of computer science. It includes various branches", "raw_answer_first": "This image is a colorful and detailed map of the field of computer science. It includes various branches", "raw_answer_second": null, "mean_entropy_first": 1.0886682377313264, "normalized_entropy_first": 0.7022600485570323, "min_margin_first": 0.0349273681640625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1358, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1360, "total_latency_s": 1.36, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9746360778808594, 0.8358772993087769, 1.4841234683990479, 0.39393630623817444, 1.959133505821228, 0.023792531341314316, 1.4004169702529907, 2.4633595943450928, 1.5956780910491943, 0.7989224195480347, 1.4184629917144775, 1.1534830331802368, 0.06907843798398972, 0.8392890095710754, 0.0020534826908260584, 0.8825733661651611, 0.7595704793930054, 2.0931077003479004, 0.5825052261352539, 2.0433647632598877], "entropies_second": null, "final_normalized_entropy": 0.7022600485570323, "sequence_confidence_first": 0.6096937634187033, "sequence_confidence_second": null, "sequence_confidence_final": 0.6096937634187033, "token_confidences_first": [0.4816059172153473, 0.5774377584457397, 0.5196744799613953, 0.9172943234443665, 0.40328866243362427, 0.9968260526657104, 0.6113646626472473, 0.41063734889030457, 0.4749964475631714, 0.8536298871040344, 0.4383971393108368, 0.7557735443115234, 0.9897839426994324, 0.6813982725143433, 0.9998149275779724, 0.5558649301528931, 0.7832686901092529, 0.3099283277988434, 0.8774081468582153, 0.351059228181839], "token_confidences_second": null, "final_mean_entropy": 1.0886682377313264, "final_min_margin": 0.0349273681640625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic that illustrates the growth", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic that illustrates the growth", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic that illustrates the growth", "raw_answer_first": "The image you've provided appears to be a graphic or infographic that illustrates the growth", "raw_answer_second": null, "mean_entropy_first": 1.0991964805667522, "normalized_entropy_first": 0.7027255923200068, "min_margin_first": 0.018520355224609375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1029, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1034, "total_latency_s": 1.034, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.6906074285507202, 0.0868123397231102, 1.7319660186767578, 0.37683889269828796, 0.0018140105530619621, 0.6651573181152344, 1.0554537773132324, 0.00015965342754498124, 0.11632034927606583, 0.43057915568351746, 2.608489990234375, 0.8110076785087585, 2.1485719680786133, 1.9823952913284302, 0.01251205988228321, 2.708751678466797, 2.7355597019195557, 0.00048735737800598145, 1.2365583181381226, 1.5838866233825684], "entropies_second": null, "final_normalized_entropy": 0.7027255923200068, "sequence_confidence_first": 0.6000246432607838, "sequence_confidence_second": null, "sequence_confidence_final": 0.6000246432607838, "token_confidences_first": [0.3523400127887726, 0.9895981550216675, 0.46004369854927063, 0.904047429561615, 0.9998356103897095, 0.7241272926330566, 0.584112286567688, 0.9999901056289673, 0.980519711971283, 0.8610130548477173, 0.3211665451526642, 0.5688907504081726, 0.3798523247241974, 0.4749494194984436, 0.9985125660896301, 0.20281825959682465, 0.23933075368404388, 0.9999618530273438, 0.7005258798599243, 0.6315110325813293], "token_confidences_second": null, "final_mean_entropy": 1.0991964805667522, "final_min_margin": 0.018520355224609375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic that outlines various benefits", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic that outlines various benefits", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic that outlines various benefits", "raw_answer_first": "The image you've provided appears to be a graphic or infographic that outlines various benefits", "raw_answer_second": null, "mean_entropy_first": 1.0392319380356638, "normalized_entropy_first": 0.343995000778043, "min_margin_first": 0.23183631896972656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1108, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1109, "total_latency_s": 1.109, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0943256616592407, 0.40251147747039795, 1.7796893119812012, 0.3418389856815338, 0.0032433606684207916, 0.7202164530754089, 1.0900065898895264, 0.00025801832089200616, 0.048460256308317184, 0.6156361103057861, 3.149686336517334, 0.36309608817100525, 1.7854070663452148, 1.8336780071258545, 0.014796871691942215, 2.7651801109313965, 2.127351760864258, 0.000212137631024234, 1.4597655534744263, 1.1892786026000977], "entropies_second": null, "final_normalized_entropy": 0.343995000778043, "sequence_confidence_first": 0.6370185648901185, "sequence_confidence_second": null, "sequence_confidence_final": 0.6370185648901185, "token_confidences_first": [0.6440240740776062, 0.9350659251213074, 0.4582855999469757, 0.9187836050987244, 0.9996939897537231, 0.6442179083824158, 0.5081807374954224, 0.9999827146530151, 0.9935669302940369, 0.7338446378707886, 0.2802872955799103, 0.8982686996459961, 0.6160523891448975, 0.49740901589393616, 0.9981653094291687, 0.23017224669456482, 0.3895271122455597, 0.9999847412109375, 0.4540579915046692, 0.6382596492767334], "token_confidences_second": null, "final_mean_entropy": 1.0392319380356638, "final_min_margin": 0.23183631896972656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic that appears to be related to college tennis, specifically featuring the top", "used_ocr": false, "answer_first": "The image is a graphic that appears to be related to college tennis, specifically featuring the top", "answer_second": null, "raw_answer": "The image is a graphic that appears to be related to college tennis, specifically featuring the top", "raw_answer_first": "The image is a graphic that appears to be related to college tennis, specifically featuring the top", "raw_answer_second": null, "mean_entropy_first": 1.3313714290765346, "normalized_entropy_first": 1.8906989263120761, "min_margin_first": 0.07379722595214844, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1134, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1134, "total_latency_s": 1.134, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1750355958938599, 0.12033193558454514, 1.8235234022140503, 0.4297492206096649, 2.626574993133545, 0.4339883029460907, 3.3189661502838135, 2.220043659210205, 0.004054771736264229, 0.6560903787612915, 2.122933864593506, 0.0003382783033885062, 1.5088770389556885, 0.04142313078045845, 1.767052173614502, 2.0716552734375, 2.738272190093994, 1.9435756206512451, 0.9751535058021545, 0.6497890949249268], "entropies_second": null, "final_normalized_entropy": 1.8906989263120761, "sequence_confidence_first": 0.520593915238247, "sequence_confidence_second": null, "sequence_confidence_final": 0.520593915238247, "token_confidences_first": [0.5651541352272034, 0.9838570356369019, 0.26025232672691345, 0.8947655558586121, 0.3584718704223633, 0.8473894596099854, 0.17368897795677185, 0.44027847051620483, 0.999650239944458, 0.8842585682868958, 0.329631507396698, 0.9999771118164062, 0.4354347288608551, 0.9957183003425598, 0.3465965986251831, 0.458944708108902, 0.21166808903217316, 0.30974280834198, 0.6594719886779785, 0.8180838227272034], "token_confidences_second": null, "final_mean_entropy": 1.3313714290765346, "final_min_margin": 0.07379722595214844, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a colorful infographic or poster that provides information about", "used_ocr": false, "answer_first": "The image you've provided appears to be a colorful infographic or poster that provides information about", "answer_second": null, "raw_answer": "The image you've provided appears to be a colorful infographic or poster that provides information about", "raw_answer_first": "The image you've provided appears to be a colorful infographic or poster that provides information about", "raw_answer_second": null, "mean_entropy_first": 1.0667588958574925, "normalized_entropy_first": 0.25598164043048427, "min_margin_first": 0.0072345733642578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1197, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1200, "total_latency_s": 1.2, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.1588791608810425, 0.1523352712392807, 1.8153280019760132, 0.33897486329078674, 0.0034840970765799284, 0.6997129917144775, 0.9503173828125, 0.00014606246259063482, 0.02367790974676609, 0.720783531665802, 3.067368984222412, 0.007894463837146759, 0.9192046523094177, 0.009257469326257706, 2.4863526821136475, 1.7294337749481201, 2.630929470062256, 2.7000064849853516, 1.2388676404953003, 0.6822230219841003], "entropies_second": null, "final_normalized_entropy": 0.25598164043048427, "sequence_confidence_first": 0.6008622285982138, "sequence_confidence_second": null, "sequence_confidence_final": 0.6008622285982138, "token_confidences_first": [0.6334896683692932, 0.9805148839950562, 0.33932259678840637, 0.9175143241882324, 0.9996510744094849, 0.6647347807884216, 0.5262855291366577, 0.9999905824661255, 0.9972617626190186, 0.5826568007469177, 0.15182191133499146, 0.9990972280502319, 0.7977321147918701, 0.9989745616912842, 0.3948475122451782, 0.6467602849006653, 0.28761592507362366, 0.21175920963287354, 0.6595576405525208, 0.7721614241600037], "token_confidences_second": null, "final_mean_entropy": 1.0667588958574925, "final_min_margin": 0.0072345733642578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a collage of various infographics and statistics related", "used_ocr": false, "answer_first": "The image you've provided appears to be a collage of various infographics and statistics related", "answer_second": null, "raw_answer": "The image you've provided appears to be a collage of various infographics and statistics related", "raw_answer_first": "The image you've provided appears to be a collage of various infographics and statistics related", "raw_answer_second": null, "mean_entropy_first": 0.8860673951419813, "normalized_entropy_first": -0.6787936408017291, "min_margin_first": 0.046718597412109375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1377, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1386, "total_latency_s": 1.386, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8510289192199707, 0.0760146826505661, 1.2688775062561035, 0.23262041807174683, 0.002207361161708832, 0.6864917278289795, 0.8497709035873413, 5.4700500186299905e-05, 0.022420616820454597, 0.733058750629425, 3.0364937782287598, 0.0035933959297835827, 0.8516712784767151, 1.8582186698913574, 2.2271511554718018, 0.25230318307876587, 0.00047081822413019836, 1.764317274093628, 1.9055604934692383, 1.0990222692489624], "entropies_second": null, "final_normalized_entropy": -0.6787936408017291, "sequence_confidence_first": 0.6624274461678235, "sequence_confidence_second": null, "sequence_confidence_final": 0.6624274461678235, "token_confidences_first": [0.5143906474113464, 0.9899632334709167, 0.6419951915740967, 0.950838565826416, 0.9997796416282654, 0.671005368232727, 0.49805328249931335, 0.9999966621398926, 0.9972862005233765, 0.599441647529602, 0.2439848780632019, 0.9996554851531982, 0.6759994029998779, 0.5533741116523743, 0.3515694737434387, 0.9324630498886108, 0.9999581575393677, 0.35695880651474, 0.5091984272003174, 0.7842167615890503], "token_confidences_second": null, "final_mean_entropy": 0.8860673951419813, "final_min_margin": 0.046718597412109375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a timeline of significant events in the history of social media. Starting from", "used_ocr": false, "answer_first": "This image appears to be a timeline of significant events in the history of social media. Starting from", "answer_second": null, "raw_answer": "This image appears to be a timeline of significant events in the history of social media. Starting from", "raw_answer_first": "This image appears to be a timeline of significant events in the history of social media. Starting from", "raw_answer_second": null, "mean_entropy_first": 0.8776814797303814, "normalized_entropy_first": -0.673663452545896, "min_margin_first": 0.059185028076171875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1083, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1084, "total_latency_s": 1.084, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2904971837997437, 0.6979057788848877, 1.5155134201049805, 7.686320168431848e-05, 0.09507948160171509, 0.5307988524436951, 2.256375312805176, 0.007538680452853441, 2.1330759525299072, 2.1228342056274414, 0.8829492330551147, 1.2807869911193848, 0.5450221300125122, 0.4952548146247864, 0.008459027856588364, 0.04222268611192703, 0.004072344861924648, 1.4005881547927856, 1.7037217617034912, 0.5408567190170288], "entropies_second": null, "final_normalized_entropy": -0.673663452545896, "sequence_confidence_first": 0.6471716625669565, "sequence_confidence_second": null, "sequence_confidence_final": 0.6471716625669565, "token_confidences_first": [0.40470269322395325, 0.7444890737533569, 0.5805707573890686, 0.9999953508377075, 0.9868375658988953, 0.7949908375740051, 0.4122382700443268, 0.9992691874504089, 0.3351961374282837, 0.2888462543487549, 0.7952715754508972, 0.4668324291706085, 0.7976166605949402, 0.8941401243209839, 0.999077558517456, 0.9952226281166077, 0.9996280670166016, 0.4258549213409424, 0.30997464060783386, 0.8736066818237305], "token_confidences_second": null, "final_mean_entropy": 0.8776814797303814, "final_min_margin": 0.059185028076171875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic that illustrates the trade", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic that illustrates the trade", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic that illustrates the trade", "raw_answer_first": "The image you've provided appears to be a graphic or infographic that illustrates the trade", "raw_answer_second": null, "mean_entropy_first": 0.8860270198791114, "normalized_entropy_first": -0.5798771232727536, "min_margin_first": 0.04796028137207031, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1244, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1246, "total_latency_s": 1.246, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7732628583908081, 0.04222867637872696, 1.568226933479309, 0.27780085802078247, 0.0026865703985095024, 0.594269871711731, 0.9485026597976685, 4.813341365661472e-05, 0.03874193876981735, 0.7203205227851868, 2.650049924850464, 0.7477998733520508, 1.8552733659744263, 1.6100698709487915, 0.016858290880918503, 2.498753070831299, 1.6794042587280273, 0.0003599216288421303, 0.8556821942329407, 0.8402006030082703], "entropies_second": null, "final_normalized_entropy": -0.5798771232727536, "sequence_confidence_first": 0.6544889957823014, "sequence_confidence_second": null, "sequence_confidence_final": 0.6544889957823014, "token_confidences_first": [0.5342868566513062, 0.9948655962944031, 0.5269924402236938, 0.9356297850608826, 0.9997187256813049, 0.7563561201095581, 0.5308194160461426, 0.9999971389770508, 0.9952644109725952, 0.542977511882782, 0.27210649847984314, 0.6576690077781677, 0.435549795627594, 0.6191905736923218, 0.9978140592575073, 0.24816809594631195, 0.4743153154850006, 0.9999710321426392, 0.7331817150115967, 0.8803582191467285], "token_confidences_second": null, "final_mean_entropy": 0.8860270198791114, "final_min_margin": 0.04796028137207031, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to social media market", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to social media market", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to social media market", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to social media market", "raw_answer_second": null, "mean_entropy_first": 1.0449797842906263, "normalized_entropy_first": 0.3512897945205326, "min_margin_first": 0.004853248596191406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1132, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1133, "total_latency_s": 1.133, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0323333740234375, 0.03474970534443855, 1.191324234008789, 0.25195518136024475, 0.0049956319853663445, 0.6640767455101013, 0.9989557862281799, 6.715752533636987e-05, 0.05831553414463997, 0.47916847467422485, 3.2206015586853027, 1.1845020055770874, 2.1547844409942627, 1.8937112092971802, 0.015176539309322834, 2.3483777046203613, 0.00020867837883997709, 2.411375045776367, 0.050437651574611664, 2.9044790267944336], "entropies_second": null, "final_normalized_entropy": 0.3512897945205326, "sequence_confidence_first": 0.626420805526535, "sequence_confidence_second": null, "sequence_confidence_final": 0.626420805526535, "token_confidences_first": [0.5279424786567688, 0.996069073677063, 0.6680683493614197, 0.945986807346344, 0.999439537525177, 0.692038893699646, 0.6135903000831604, 0.9999960660934448, 0.9915773868560791, 0.8522984981536865, 0.20504982769489288, 0.5527421832084656, 0.49739721417427063, 0.48418131470680237, 0.9980827569961548, 0.3919432759284973, 0.9999867677688599, 0.33143970370292664, 0.9948031306266785, 0.2062719613313675], "token_confidences_second": null, "final_mean_entropy": 1.0449797842906263, "final_min_margin": 0.004853248596191406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides statistics about various social media platforms and their users.", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides statistics about various social media platforms and their users.", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides statistics about various social media platforms and their users.", "raw_answer_first": "This image is a colorful infographic that provides statistics about various social media platforms and their users.", "raw_answer_second": null, "mean_entropy_first": 1.1540428744163365, "normalized_entropy_first": 0.9726985676642593, "min_margin_first": 0.033519744873046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1414, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1415, "total_latency_s": 1.415, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0065680742263794, 1.019443154335022, 1.5723141431808472, 0.4875691533088684, 2.455970287322998, 0.015479659661650658, 0.6584898233413696, 0.007733833976089954, 2.1056599617004395, 2.247943878173828, 1.8498647212982178, 1.4426943063735962, 1.432939052581787, 0.5407015085220337, 0.22346167266368866, 0.5501633286476135, 1.3423523902893066, 1.4638921022415161, 1.962027668952942, 0.6955887675285339], "entropies_second": null, "final_normalized_entropy": 0.9726985676642593, "sequence_confidence_first": 0.6198087021796823, "sequence_confidence_second": null, "sequence_confidence_final": 0.6198087021796823, "token_confidences_first": [0.48635706305503845, 0.5794888138771057, 0.5714524388313293, 0.8413691520690918, 0.4619936943054199, 0.9981045722961426, 0.8707321286201477, 0.9991523027420044, 0.5437886118888855, 0.35617196559906006, 0.35560089349746704, 0.3544006049633026, 0.5308609008789062, 0.9032216668128967, 0.9628739356994629, 0.8537111282348633, 0.5577518939971924, 0.7424948811531067, 0.3820263147354126, 0.8461116552352905], "token_confidences_second": null, "final_mean_entropy": 1.1540428744163365, "final_min_margin": 0.033519744873046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to the use of social", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to the use of social", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to the use of social", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to the use of social", "raw_answer_second": null, "mean_entropy_first": 0.8630638185899443, "normalized_entropy_first": -0.842290727510136, "min_margin_first": 0.004230499267578125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1212, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1214, "total_latency_s": 1.214, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8436217308044434, 0.07360765337944031, 1.4309357404708862, 0.29244938492774963, 0.0025063790380954742, 0.6719295382499695, 0.8369706869125366, 4.318208812037483e-05, 0.01495605893433094, 0.7975649237632751, 2.9901981353759766, 0.45506420731544495, 1.298412799835205, 1.8823208808898926, 2.23709774017334, 9.805672743823379e-05, 1.473734974861145, 1.558691143989563, 0.03796416148543358, 0.3631089925765991], "entropies_second": null, "final_normalized_entropy": -0.842290727510136, "sequence_confidence_first": 0.6834066856542448, "sequence_confidence_second": null, "sequence_confidence_final": 0.6834066856542448, "token_confidences_first": [0.6403149366378784, 0.9903458952903748, 0.5568146705627441, 0.9336954951286316, 0.9997496008872986, 0.7024340629577637, 0.5563409924507141, 0.9999974966049194, 0.9982733726501465, 0.4899615943431854, 0.2905593514442444, 0.8397723436355591, 0.7623279094696045, 0.4340047538280487, 0.3671000599861145, 0.9999939203262329, 0.48976460099220276, 0.5895203351974487, 0.9939747452735901, 0.9216448664665222], "token_confidences_second": null, "final_mean_entropy": 0.8630638185899443, "final_min_margin": 0.004230499267578125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This is an infographic titled \"Women in Numbers: The Year When the Average Woman", "used_ocr": false, "answer_first": "This is an infographic titled \"Women in Numbers: The Year When the Average Woman", "answer_second": null, "raw_answer": "This is an infographic titled \"Women in Numbers: The Year When the Average Woman", "raw_answer_first": "This is an infographic titled \"Women in Numbers: The Year When the Average Woman", "raw_answer_second": null, "mean_entropy_first": 0.7127369803922192, "normalized_entropy_first": -1.6795083143037002, "min_margin_first": 0.09536552429199219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1389, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1391, "total_latency_s": 1.391, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8664392232894897, 1.1797540187835693, 0.6269832253456116, 0.3813974857330322, 0.011011673137545586, 1.578721284866333, 0.008730605244636536, 0.18184992671012878, 0.3166944980621338, 0.2699011266231537, 0.018704649060964584, 8.861550304573029e-05, 1.5751889944076538, 1.0493096113204956, 1.668034553527832, 2.051666498184204, 0.8610842227935791, 0.9490443468093872, 0.011422771029174328, 0.6487122774124146], "entropies_second": null, "final_normalized_entropy": -1.6795083143037002, "sequence_confidence_first": 0.7669264548096814, "sequence_confidence_second": null, "sequence_confidence_final": 0.7669264548096814, "token_confidences_first": [0.5691686272621155, 0.42382144927978516, 0.685606062412262, 0.9164614677429199, 0.9986827969551086, 0.572959303855896, 0.9991268515586853, 0.9783229827880859, 0.9168298244476318, 0.9284862875938416, 0.9977546334266663, 0.9999935626983643, 0.41772302985191345, 0.8095314502716064, 0.6198663115501404, 0.5133374333381653, 0.8424765467643738, 0.8680253028869629, 0.9986799359321594, 0.8757548928260803], "token_confidences_second": null, "final_mean_entropy": 0.7127369803922192, "final_min_margin": 0.09536552429199219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that compares the benefits of", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that compares the benefits of", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that compares the benefits of", "raw_answer_first": "The image you've provided appears to be a graphic or poster that compares the benefits of", "raw_answer_second": null, "mean_entropy_first": 1.1309490942716365, "normalized_entropy_first": 0.9428016629092107, "min_margin_first": 0.2623710632324219, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1224, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1235, "total_latency_s": 1.235, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.078432559967041, 0.0748235434293747, 1.337776780128479, 0.2596771717071533, 0.0026838111225515604, 0.6773009896278381, 0.8540189862251282, 5.987557233311236e-05, 0.021497495472431183, 0.7157180309295654, 3.1550607681274414, 0.272443562746048, 1.625793695449829, 2.3824667930603027, 2.848118782043457, 2.593320608139038, 0.0003124471113551408, 2.330620765686035, 2.2890400886535645, 0.09981513023376465], "entropies_second": null, "final_normalized_entropy": 0.9428016629092107, "sequence_confidence_first": 0.5987567752218059, "sequence_confidence_second": null, "sequence_confidence_final": 0.5987567752218059, "token_confidences_first": [0.5466547012329102, 0.9907206296920776, 0.6148108243942261, 0.9429028630256653, 0.9997355341911316, 0.6878277659416199, 0.620712399482727, 0.9999964237213135, 0.9972584247589111, 0.6168100237846375, 0.2356167435646057, 0.9239934682846069, 0.5963486433029175, 0.350776731967926, 0.2614911198616028, 0.24994228780269623, 0.9999732971191406, 0.2602832019329071, 0.5573339462280273, 0.9855313301086426], "token_confidences_second": null, "final_mean_entropy": 1.1309490942716365, "final_min_margin": 0.2623710632324219, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": [], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to the history of World War II,", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to the history of World War II,", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to the history of World War II,", "raw_answer_first": "This image appears to be a graphic or infographic related to the history of World War II,", "raw_answer_second": null, "mean_entropy_first": 1.2016329563273758, "normalized_entropy_first": 1.258348427850044, "min_margin_first": 0.03574180603027344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1382, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1383, "total_latency_s": 1.383, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8345857858657837, 0.5922563076019287, 1.3329637050628662, 0.00011038105003535748, 0.05009026080369949, 0.7697125673294067, 2.5887584686279297, 0.9616044759750366, 2.3694264888763428, 2.143406391143799, 0.015277113765478134, 2.4214444160461426, 0.00023241467715706676, 1.7520639896392822, 3.1933465003967285, 0.8001304268836975, 2.12869930267334, 0.05836257338523865, 0.6039635539054871, 1.4162240028381348], "entropies_second": null, "final_normalized_entropy": 1.258348427850044, "sequence_confidence_first": 0.5620220107296455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5620220107296455, "token_confidences_first": [0.5395451188087463, 0.817196249961853, 0.532986044883728, 0.9999926090240479, 0.9936698079109192, 0.6283349394798279, 0.3026316165924072, 0.5572741627693176, 0.26487085223197937, 0.38257235288619995, 0.9980794191360474, 0.31301426887512207, 0.9999849796295166, 0.5792708992958069, 0.3215394616127014, 0.6056236028671265, 0.3573700785636902, 0.9901629686355591, 0.7357247471809387, 0.42978745698928833], "token_confidences_second": null, "final_mean_entropy": 1.2016329563273758, "final_min_margin": 0.03574180603027344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.0, "wer": 1.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
