{"question": "", "image_id": null, "answers": ["Tony Blair."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a humorous infographic or advertisement related to condoms. It", "used_ocr": false, "answer_first": "This image appears to be a humorous infographic or advertisement related to condoms. It", "answer_second": null, "raw_answer": "This image appears to be a humorous infographic or advertisement related to condoms. It", "raw_answer_first": "This image appears to be a humorous infographic or advertisement related to condoms. It", "raw_answer_second": null, "mean_entropy_first": 0.9192355739876803, "normalized_entropy_first": 0.0, "min_margin_first": 0.00266265869140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1538, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1539, "total_latency_s": 1.539, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8998276591300964, 0.7093388438224792, 0.9009451866149902, 0.0001297406997764483, 0.028330817818641663, 0.6494687795639038, 2.8367362022399902, 0.0016397011931985617, 2.234201431274414, 0.01656216010451317, 2.3251771926879883, 2.704763174057007, 0.013479250483214855, 0.005042472388595343, 2.2836384773254395, 0.00024471592041663826, 0.8748568892478943, 0.09682801365852356, 0.6957770586013794, 1.1077237129211426], "entropies_second": null, "final_normalized_entropy": 0.0, "sequence_confidence_first": 0.6612259057883294, "sequence_confidence_second": null, "sequence_confidence_final": 0.6612259057883294, "token_confidences_first": [0.48342305421829224, 0.7299785017967224, 0.705132782459259, 0.9999912977218628, 0.996460497379303, 0.7583044767379761, 0.41952651739120483, 0.9998593330383301, 0.2591133415699005, 0.9981070756912231, 0.2727759778499603, 0.36401090025901794, 0.9984139204025269, 0.9994592070579529, 0.3347862958908081, 0.9999839067459106, 0.8273906707763672, 0.9814819097518921, 0.8068686127662659, 0.5757992267608643], "token_confidences_second": null, "final_mean_entropy": 0.9192355739876803, "final_min_margin": 0.00266265869140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.181818181818182, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1964."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -136.17281784027003, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1291, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1292, "total_latency_s": 1.292, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -136.17281784027003, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.4, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Road rider crusier."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -3.0000000000000675, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1287, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1289, "total_latency_s": 1.289, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -3.0000000000000675, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.052631578947368, "wer": 4.666666666666667, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["10."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -2.0647416048351452, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1299, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1300, "total_latency_s": 1.3, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -2.0647416048351452, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 29.0, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["G and S Fiberflex Pintail."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -1.6401331959231586, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1287, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1288, "total_latency_s": 1.288, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -1.6401331959231586, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.769230769230769, "wer": 2.8, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["White."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -1.381239313548215, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1286, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1287, "total_latency_s": 1.287, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -1.381239313548215, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.833333333333334, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1980."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -1.200809231185125, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1289, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1289, "total_latency_s": 1.289, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -1.200809231185125, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.4, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1960."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "used_ocr": false, "answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "answer_second": null, "raw_answer": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_first": "This image is an infographic that illustrates the evolution of skateboarding from the 1", "raw_answer_second": null, "mean_entropy_first": 0.7830627561474103, "normalized_entropy_first": -1.0649896477878686, "min_margin_first": 0.16119766235351562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1289, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1290, "total_latency_s": 1.29, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8873264789581299, 0.8936961889266968, 1.8738648891448975, 0.8962317705154419, 1.145644187927246, 0.011267186142504215, 1.7726166248321533, 2.239461898803711, 0.0002683299535419792, 0.01633337140083313, 0.2690383791923523, 0.016449090093374252, 0.7535825967788696, 0.0006985065992921591, 0.5062248706817627, 0.581532895565033, 1.6207563877105713, 0.9936188459396362, 1.1713436841964722, 0.01129893958568573], "entropies_second": null, "final_normalized_entropy": -1.0649896477878686, "sequence_confidence_first": 0.6771133662216167, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771133662216167, "token_confidences_first": [0.5589171051979065, 0.5718016624450684, 0.3709190785884857, 0.5750375986099243, 0.6804912090301514, 0.9987092018127441, 0.5428052544593811, 0.30379000306129456, 0.9999794960021973, 0.9982010126113892, 0.9466364979743958, 0.998026430606842, 0.5379215478897095, 0.999943733215332, 0.8012822866439819, 0.8656765818595886, 0.44551968574523926, 0.4954635500907898, 0.6926319599151611, 0.9986864924430847], "token_confidences_second": null, "final_mean_entropy": 0.7830627561474103, "final_min_margin": 0.16119766235351562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.4, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["12."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "used_ocr": false, "answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "answer_second": null, "raw_answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_second": null, "mean_entropy_first": 1.2556557370116934, "normalized_entropy_first": 5.9901284722345425, "min_margin_first": 0.22307205200195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1300, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1301, "total_latency_s": 1.301, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810091972351074, 0.8073989152908325, 1.1804457902908325, 0.5225983262062073, 2.572787046432495, 0.014881671406328678, 1.5915279388427734, 2.495455741882324, 1.2954890727996826, 0.004867779556661844, 1.9692333936691284, 3.2593719959259033, 1.1464431285858154, 0.33664095401763916, 1.4835284948349, 2.213690757751465, 1.1878440380096436, 0.11123129725456238, 2.1202750205993652, 0.01839417964220047], "entropies_second": null, "final_normalized_entropy": 5.9901284722345425, "sequence_confidence_first": 0.581105671573493, "sequence_confidence_second": null, "sequence_confidence_final": 0.581105671573493, "token_confidences_first": [0.7197906970977783, 0.571538507938385, 0.5365172624588013, 0.8329945206642151, 0.4501662254333496, 0.9982168078422546, 0.4429086446762085, 0.3084643483161926, 0.7485946416854858, 0.9994946718215942, 0.6214486956596375, 0.18154211342334747, 0.4618507921695709, 0.9142830967903137, 0.46197277307510376, 0.38363730907440186, 0.7252474427223206, 0.9848787188529968, 0.3793528974056244, 0.9978516101837158], "token_confidences_second": null, "final_mean_entropy": 1.2556557370116934, "final_min_margin": 0.22307205200195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 36.666666666666664, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["The skarasen."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "used_ocr": false, "answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "answer_second": null, "raw_answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_second": null, "mean_entropy_first": 1.2556557370116934, "normalized_entropy_first": 2.6530041501689414, "min_margin_first": 0.22307205200195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1298, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1300, "total_latency_s": 1.3, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810091972351074, 0.8073989152908325, 1.1804457902908325, 0.5225983262062073, 2.572787046432495, 0.014881671406328678, 1.5915279388427734, 2.495455741882324, 1.2954890727996826, 0.004867779556661844, 1.9692333936691284, 3.2593719959259033, 1.1464431285858154, 0.33664095401763916, 1.4835284948349, 2.213690757751465, 1.1878440380096436, 0.11123129725456238, 2.1202750205993652, 0.01839417964220047], "entropies_second": null, "final_normalized_entropy": 2.6530041501689414, "sequence_confidence_first": 0.581105671573493, "sequence_confidence_second": null, "sequence_confidence_final": 0.581105671573493, "token_confidences_first": [0.7197906970977783, 0.571538507938385, 0.5365172624588013, 0.8329945206642151, 0.4501662254333496, 0.9982168078422546, 0.4429086446762085, 0.3084643483161926, 0.7485946416854858, 0.9994946718215942, 0.6214486956596375, 0.18154211342334747, 0.4618507921695709, 0.9142830967903137, 0.46197277307510376, 0.38363730907440186, 0.7252474427223206, 0.9848787188529968, 0.3793528974056244, 0.9978516101837158], "token_confidences_second": null, "final_mean_entropy": 1.2556557370116934, "final_min_margin": 0.22307205200195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.615384615384615, "wer": 8.0, "precision": 0.058823529411764705, "recall": 0.5, "f1": 0.10526315789473684, "rouge_l": 0.10526315789473684, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Ice warriors."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "used_ocr": false, "answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "answer_second": null, "raw_answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_second": null, "mean_entropy_first": 1.2556557370116934, "normalized_entropy_first": 1.9281658373142725, "min_margin_first": 0.22307205200195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1301, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1302, "total_latency_s": 1.302, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810091972351074, 0.8073989152908325, 1.1804457902908325, 0.5225983262062073, 2.572787046432495, 0.014881671406328678, 1.5915279388427734, 2.495455741882324, 1.2954890727996826, 0.004867779556661844, 1.9692333936691284, 3.2593719959259033, 1.1464431285858154, 0.33664095401763916, 1.4835284948349, 2.213690757751465, 1.1878440380096436, 0.11123129725456238, 2.1202750205993652, 0.01839417964220047], "entropies_second": null, "final_normalized_entropy": 1.9281658373142725, "sequence_confidence_first": 0.581105671573493, "sequence_confidence_second": null, "sequence_confidence_final": 0.581105671573493, "token_confidences_first": [0.7197906970977783, 0.571538507938385, 0.5365172624588013, 0.8329945206642151, 0.4501662254333496, 0.9982168078422546, 0.4429086446762085, 0.3084643483161926, 0.7485946416854858, 0.9994946718215942, 0.6214486956596375, 0.18154211342334747, 0.4618507921695709, 0.9142830967903137, 0.46197277307510376, 0.38363730907440186, 0.7252474427223206, 0.9848787188529968, 0.3793528974056244, 0.9978516101837158], "token_confidences_second": null, "final_mean_entropy": 1.2556557370116934, "final_min_margin": 0.22307205200195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.615384615384615, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["10."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "used_ocr": false, "answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "answer_second": null, "raw_answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_second": null, "mean_entropy_first": 1.2556557370116934, "normalized_entropy_first": 1.5617914224786331, "min_margin_first": 0.22307205200195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1305, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1306, "total_latency_s": 1.306, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810091972351074, 0.8073989152908325, 1.1804457902908325, 0.5225983262062073, 2.572787046432495, 0.014881671406328678, 1.5915279388427734, 2.495455741882324, 1.2954890727996826, 0.004867779556661844, 1.9692333936691284, 3.2593719959259033, 1.1464431285858154, 0.33664095401763916, 1.4835284948349, 2.213690757751465, 1.1878440380096436, 0.11123129725456238, 2.1202750205993652, 0.01839417964220047], "entropies_second": null, "final_normalized_entropy": 1.5617914224786331, "sequence_confidence_first": 0.581105671573493, "sequence_confidence_second": null, "sequence_confidence_final": 0.581105671573493, "token_confidences_first": [0.7197906970977783, 0.571538507938385, 0.5365172624588013, 0.8329945206642151, 0.4501662254333496, 0.9982168078422546, 0.4429086446762085, 0.3084643483161926, 0.7485946416854858, 0.9994946718215942, 0.6214486956596375, 0.18154211342334747, 0.4618507921695709, 0.9142830967903137, 0.46197277307510376, 0.38363730907440186, 0.7252474427223206, 0.9848787188529968, 0.3793528974056244, 0.9978516101837158], "token_confidences_second": null, "final_mean_entropy": 1.2556557370116934, "final_min_margin": 0.22307205200195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 36.666666666666664, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Blue."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "used_ocr": false, "answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "answer_second": null, "raw_answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_second": null, "mean_entropy_first": 1.2556557370116934, "normalized_entropy_first": 1.3284591199080669, "min_margin_first": 0.22307205200195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1304, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1305, "total_latency_s": 1.305, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810091972351074, 0.8073989152908325, 1.1804457902908325, 0.5225983262062073, 2.572787046432495, 0.014881671406328678, 1.5915279388427734, 2.495455741882324, 1.2954890727996826, 0.004867779556661844, 1.9692333936691284, 3.2593719959259033, 1.1464431285858154, 0.33664095401763916, 1.4835284948349, 2.213690757751465, 1.1878440380096436, 0.11123129725456238, 2.1202750205993652, 0.01839417964220047], "entropies_second": null, "final_normalized_entropy": 1.3284591199080669, "sequence_confidence_first": 0.581105671573493, "sequence_confidence_second": null, "sequence_confidence_final": 0.581105671573493, "token_confidences_first": [0.7197906970977783, 0.571538507938385, 0.5365172624588013, 0.8329945206642151, 0.4501662254333496, 0.9982168078422546, 0.4429086446762085, 0.3084643483161926, 0.7485946416854858, 0.9994946718215942, 0.6214486956596375, 0.18154211342334747, 0.4618507921695709, 0.9142830967903137, 0.46197277307510376, 0.38363730907440186, 0.7252474427223206, 0.9848787188529968, 0.3793528974056244, 0.9978516101837158], "token_confidences_second": null, "final_mean_entropy": 1.2556557370116934, "final_min_margin": 0.22307205200195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 21.4, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Sontarans."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "used_ocr": false, "answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "answer_second": null, "raw_answer": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_first": "This image is a colorful and detailed infographic that provides information about various enemies from the \"Do", "raw_answer_second": null, "mean_entropy_first": 1.2556557370116934, "normalized_entropy_first": 1.1619223562715735, "min_margin_first": 0.22307205200195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1300, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1301, "total_latency_s": 1.301, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810091972351074, 0.8073989152908325, 1.1804457902908325, 0.5225983262062073, 2.572787046432495, 0.014881671406328678, 1.5915279388427734, 2.495455741882324, 1.2954890727996826, 0.004867779556661844, 1.9692333936691284, 3.2593719959259033, 1.1464431285858154, 0.33664095401763916, 1.4835284948349, 2.213690757751465, 1.1878440380096436, 0.11123129725456238, 2.1202750205993652, 0.01839417964220047], "entropies_second": null, "final_normalized_entropy": 1.1619223562715735, "sequence_confidence_first": 0.581105671573493, "sequence_confidence_second": null, "sequence_confidence_final": 0.581105671573493, "token_confidences_first": [0.7197906970977783, 0.571538507938385, 0.5365172624588013, 0.8329945206642151, 0.4501662254333496, 0.9982168078422546, 0.4429086446762085, 0.3084643483161926, 0.7485946416854858, 0.9994946718215942, 0.6214486956596375, 0.18154211342334747, 0.4618507921695709, 0.9142830967903137, 0.46197277307510376, 0.38363730907440186, 0.7252474427223206, 0.9848787188529968, 0.3793528974056244, 0.9978516101837158], "token_confidences_second": null, "final_mean_entropy": 1.2556557370116934, "final_min_margin": 0.22307205200195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.1, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Tim Hortons."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.9783136941395062, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1198, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1200, "total_latency_s": 1.2, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.9783136941395062, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.416666666666667, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Taco bell."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.8866487796899427, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1199, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1200, "total_latency_s": 1.2, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.8866487796899427, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.6, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Burger King."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.8099155306924596, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1201, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1202, "total_latency_s": 1.202, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.8099155306924596, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.583333333333333, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["McDonalds."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.7443284624429483, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1196, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1197, "total_latency_s": 1.197, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.7443284624429483, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.6, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["KFC."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.6873482394889407, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1198, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1199, "total_latency_s": 1.199, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.6873482394889407, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.75, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["IN-N-OUT Burger."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.6371973798454476, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1198, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1199, "total_latency_s": 1.199, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.6371973798454476, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.125, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["18."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.5925880610617377, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1203, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1204, "total_latency_s": 1.204, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.5925880610617377, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 24.333333333333332, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["SONIC."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "used_ocr": false, "answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "answer_second": null, "raw_answer": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_first": "The image you've provided is a flowchart titled \"Where Should I Eat?\" It'", "raw_answer_second": null, "mean_entropy_first": 0.8343651134986431, "normalized_entropy_first": -0.5525602360999533, "min_margin_first": 0.015613555908203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1200, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1202, "total_latency_s": 1.202, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.222895860671997, 0.06300729513168335, 1.6414599418640137, 0.3167988359928131, 0.009274020791053772, 0.789871335029602, 0.9522055387496948, 0.4245121479034424, 2.143139123916626, 0.05323001742362976, 2.3513152599334717, 0.020255859941244125, 0.707065761089325, 0.08596964180469513, 0.023286372423171997, 0.3667227029800415, 0.01420698780566454, 1.0830620527267456, 2.4821555614471436, 1.9368679523468018], "entropies_second": null, "final_normalized_entropy": -0.5525602360999533, "sequence_confidence_first": 0.6887471601658711, "sequence_confidence_second": null, "sequence_confidence_final": 0.6887471601658711, "token_confidences_first": [0.5609843134880066, 0.9934345483779907, 0.509621262550354, 0.9267326593399048, 0.9988452196121216, 0.6543941497802734, 0.583189070224762, 0.9159923791885376, 0.36587658524513245, 0.991637647151947, 0.45457977056503296, 0.9976217150688171, 0.7225219011306763, 0.988021969795227, 0.9974581599235535, 0.8830544352531433, 0.9980941414833069, 0.6078863143920898, 0.23475496470928192, 0.4260823428630829], "token_confidences_second": null, "final_mean_entropy": 0.8343651134986431, "final_min_margin": 0.015613555908203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.666666666666666, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["6."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.9879420908392353, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1231, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1232, "total_latency_s": 1.232, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.9879420908392353, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 40.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Egg yolks."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.8946025034948171, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1232, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1233, "total_latency_s": 1.233, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.8946025034948171, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.3, "wer": 7.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Liver."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.816644760737553, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1233, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1234, "total_latency_s": 1.234, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.816644760737553, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.666666666666666, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Shellfish."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.7501276026850279, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1236, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1237, "total_latency_s": 1.237, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.7501276026850279, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.4, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Avocado."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.6924192052004559, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1229, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1231, "total_latency_s": 1.231, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.6924192052004559, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.25, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Egg yolks."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.6416840314799889, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1229, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1231, "total_latency_s": 1.231, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.6416840314799889, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.3, "wer": 7.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Rice."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.5965961489998287, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1231, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1232, "total_latency_s": 1.232, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.5965961489998287, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.2, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Fortified milk."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.5561695540565487, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1235, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1236, "total_latency_s": 1.236, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.5561695540565487, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.866666666666666, "wer": 7.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Bread."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.5196529047043125, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1235, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1237, "total_latency_s": 1.237, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.5196529047043125, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.666666666666666, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Avocados."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.48646160071300526, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1239, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1240, "total_latency_s": 1.24, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.48646160071300526, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.11111111111111, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.45613248213813057, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1234, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1235, "total_latency_s": 1.235, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.45613248213813057, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 40.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["5."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_first": "The image you've provided appears to be a graphic or poster that outlines the \"1", "raw_answer_second": null, "mean_entropy_first": 1.091127901179425, "normalized_entropy_first": 0.4282927424853175, "min_margin_first": 0.027923583984375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1231, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1232, "total_latency_s": 1.232, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8677656054496765, 0.06208277866244316, 1.8534897565841675, 0.28838902711868286, 0.007353088818490505, 0.7530537843704224, 1.0260694026947021, 0.00016181744285859168, 0.08421311527490616, 0.7853155732154846, 2.566930055618286, 0.41358518600463867, 1.5192621946334839, 2.4186174869537354, 2.6054489612579346, 3.430161237716675, 0.00026661850279197097, 1.5886545181274414, 1.354933738708496, 0.19680407643318176], "entropies_second": null, "final_normalized_entropy": 0.4282927424853175, "sequence_confidence_first": 0.5765365280633478, "sequence_confidence_second": null, "sequence_confidence_final": 0.5765365280633478, "token_confidences_first": [0.6279481649398804, 0.9926693439483643, 0.3365701735019684, 0.9342041015625, 0.9991520643234253, 0.6628305912017822, 0.6350244879722595, 0.9999895095825195, 0.986835241317749, 0.5229015350341797, 0.4418163597583771, 0.8577842116355896, 0.4723275899887085, 0.3039425015449524, 0.2671695649623871, 0.10907454043626785, 0.9999796152114868, 0.5659323334693909, 0.4457554817199707, 0.9678071737289429], "token_confidences_second": null, "final_mean_entropy": 1.091127901179425, "final_min_margin": 0.027923583984375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 40.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["7 Years."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "used_ocr": false, "answer_first": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "answer_second": null, "raw_answer": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "raw_answer_first": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "raw_answer_second": null, "mean_entropy_first": 0.7779981679146658, "normalized_entropy_first": -2.2446823951278865, "min_margin_first": 0.11018180847167969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1243, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1244, "total_latency_s": 1.244, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8460489511489868, 0.8414146900177002, 1.2839646339416504, 0.5257264971733093, 1.670454978942871, 0.00089421053417027, 2.037405014038086, 0.006875866092741489, 1.597168207168579, 2.9582056999206543, 1.864621877670288, 0.2143002450466156, 0.0023628221824765205, 0.9088718891143799, 0.24874639511108398, 0.25346142053604126, 0.009847601875662804, 5.6138687796192244e-05, 0.05217508599162102, 0.2373611330986023], "entropies_second": null, "final_normalized_entropy": -2.2446823951278865, "sequence_confidence_first": 0.7490534778138555, "sequence_confidence_second": null, "sequence_confidence_final": 0.7490534778138555, "token_confidences_first": [0.5161365270614624, 0.6089209914207458, 0.6150052547454834, 0.8635112643241882, 0.6684542894363403, 0.999927282333374, 0.46554961800575256, 0.9992796778678894, 0.525010883808136, 0.3987029492855072, 0.4497794508934021, 0.9606289863586426, 0.9997972846031189, 0.7891175150871277, 0.9340417981147766, 0.9530559778213501, 0.9989356398582458, 0.9999966621398926, 0.9919182658195496, 0.9466024041175842], "token_confidences_second": null, "final_mean_entropy": 0.7779981679146658, "final_min_margin": 0.11018180847167969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.375, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["It strains your eyes, making them tired and achy."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "used_ocr": false, "answer_first": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "answer_second": null, "raw_answer": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "raw_answer_first": "This image is a humorous infographic that presents \"10 Ways Your Parents Lied to", "raw_answer_second": null, "mean_entropy_first": 0.7779981679146658, "normalized_entropy_first": -1.7364907122274043, "min_margin_first": 0.11018180847167969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1243, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1244, "total_latency_s": 1.244, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8460489511489868, 0.8414146900177002, 1.2839646339416504, 0.5257264971733093, 1.670454978942871, 0.00089421053417027, 2.037405014038086, 0.006875866092741489, 1.597168207168579, 2.9582056999206543, 1.864621877670288, 0.2143002450466156, 0.0023628221824765205, 0.9088718891143799, 0.24874639511108398, 0.25346142053604126, 0.009847601875662804, 5.6138687796192244e-05, 0.05217508599162102, 0.2373611330986023], "entropies_second": null, "final_normalized_entropy": -1.7364907122274043, "sequence_confidence_first": 0.7490534778138555, "sequence_confidence_second": null, "sequence_confidence_final": 0.7490534778138555, "token_confidences_first": [0.5161365270614624, 0.6089209914207458, 0.6150052547454834, 0.8635112643241882, 0.6684542894363403, 0.999927282333374, 0.46554961800575256, 0.9992796778678894, 0.525010883808136, 0.3987029492855072, 0.4497794508934021, 0.9606289863586426, 0.9997972846031189, 0.7891175150871277, 0.9340417981147766, 0.9530559778213501, 0.9989356398582458, 0.9999966621398926, 0.9919182658195496, 0.9466024041175842], "token_confidences_second": null, "final_mean_entropy": 0.7779981679146658, "final_min_margin": 0.11018180847167969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.3061224489795917, "wer": 1.5555555555555556, "precision": 0.07142857142857142, "recall": 0.1111111111111111, "f1": 0.08695652173913043, "rouge_l": 0.08695652173913043, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Dance."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "used_ocr": false, "answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "answer_second": null, "raw_answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_second": null, "mean_entropy_first": 0.5687409419333562, "normalized_entropy_first": -2.849029415687402, "min_margin_first": 0.17942333221435547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1418, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1419, "total_latency_s": 1.419, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8553504943847656, 0.060401178896427155, 1.8548954725265503, 0.21083730459213257, 0.012787893414497375, 0.8230500221252441, 0.8179854154586792, 1.0972607135772705, 2.8154826164245605, 0.6672003269195557, 1.2704970836639404, 0.011268936097621918, 0.7233947515487671, 0.02720414474606514, 0.06551450490951538, 0.01041676476597786, 0.035003695636987686, 0.003285663668066263, 0.0009142328053712845, 0.012067622505128384], "entropies_second": null, "final_normalized_entropy": -2.849029415687402, "sequence_confidence_first": 0.7587565599869758, "sequence_confidence_second": null, "sequence_confidence_final": 0.7587565599869758, "token_confidences_first": [0.5483267307281494, 0.9928501844406128, 0.3783944547176361, 0.9572584629058838, 0.9983519315719604, 0.572521448135376, 0.7339297533035278, 0.5879330635070801, 0.22235248982906342, 0.8859431743621826, 0.6911757588386536, 0.9988282322883606, 0.6166207790374756, 0.9969882369041443, 0.9899060130119324, 0.9990999698638916, 0.9965477585792542, 0.9996957778930664, 0.9999228715896606, 0.9984435439109802], "token_confidences_second": null, "final_mean_entropy": 0.5687409419333562, "final_min_margin": 0.17942333221435547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.5, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Food."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "used_ocr": false, "answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "answer_second": null, "raw_answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_second": null, "mean_entropy_first": 0.5687409419333562, "normalized_entropy_first": -2.008054159492031, "min_margin_first": 0.17942333221435547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1417, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1418, "total_latency_s": 1.418, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8553504943847656, 0.060401178896427155, 1.8548954725265503, 0.21083730459213257, 0.012787893414497375, 0.8230500221252441, 0.8179854154586792, 1.0972607135772705, 2.8154826164245605, 0.6672003269195557, 1.2704970836639404, 0.011268936097621918, 0.7233947515487671, 0.02720414474606514, 0.06551450490951538, 0.01041676476597786, 0.035003695636987686, 0.003285663668066263, 0.0009142328053712845, 0.012067622505128384], "entropies_second": null, "final_normalized_entropy": -2.008054159492031, "sequence_confidence_first": 0.7587565599869758, "sequence_confidence_second": null, "sequence_confidence_final": 0.7587565599869758, "token_confidences_first": [0.5483267307281494, 0.9928501844406128, 0.3783944547176361, 0.9572584629058838, 0.9983519315719604, 0.572521448135376, 0.7339297533035278, 0.5879330635070801, 0.22235248982906342, 0.8859431743621826, 0.6911757588386536, 0.9988282322883606, 0.6166207790374756, 0.9969882369041443, 0.9899060130119324, 0.9990999698638916, 0.9965477585792542, 0.9996957778930664, 0.9999228715896606, 0.9984435439109802], "token_confidences_second": null, "final_mean_entropy": 0.5687409419333562, "final_min_margin": 0.17942333221435547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.8, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Clothes."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "used_ocr": false, "answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "answer_second": null, "raw_answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_second": null, "mean_entropy_first": 0.5687409419333562, "normalized_entropy_first": -1.6081721337320274, "min_margin_first": 0.17942333221435547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1418, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1419, "total_latency_s": 1.419, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8553504943847656, 0.060401178896427155, 1.8548954725265503, 0.21083730459213257, 0.012787893414497375, 0.8230500221252441, 0.8179854154586792, 1.0972607135772705, 2.8154826164245605, 0.6672003269195557, 1.2704970836639404, 0.011268936097621918, 0.7233947515487671, 0.02720414474606514, 0.06551450490951538, 0.01041676476597786, 0.035003695636987686, 0.003285663668066263, 0.0009142328053712845, 0.012067622505128384], "entropies_second": null, "final_normalized_entropy": -1.6081721337320274, "sequence_confidence_first": 0.7587565599869758, "sequence_confidence_second": null, "sequence_confidence_final": 0.7587565599869758, "token_confidences_first": [0.5483267307281494, 0.9928501844406128, 0.3783944547176361, 0.9572584629058838, 0.9983519315719604, 0.572521448135376, 0.7339297533035278, 0.5879330635070801, 0.22235248982906342, 0.8859431743621826, 0.6911757588386536, 0.9988282322883606, 0.6166207790374756, 0.9969882369041443, 0.9899060130119324, 0.9990999698638916, 0.9965477585792542, 0.9996957778930664, 0.9999228715896606, 0.9984435439109802], "token_confidences_second": null, "final_mean_entropy": 0.5687409419333562, "final_min_margin": 0.17942333221435547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.5, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "used_ocr": false, "answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "answer_second": null, "raw_answer": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_first": "The image you've provided is a visual guide titled \"HOW TO BE PRODUCTIVE", "raw_answer_second": null, "mean_entropy_first": 0.5687409419333562, "normalized_entropy_first": -1.3598974800193373, "min_margin_first": 0.17942333221435547, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1414, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1415, "total_latency_s": 1.415, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8553504943847656, 0.060401178896427155, 1.8548954725265503, 0.21083730459213257, 0.012787893414497375, 0.8230500221252441, 0.8179854154586792, 1.0972607135772705, 2.8154826164245605, 0.6672003269195557, 1.2704970836639404, 0.011268936097621918, 0.7233947515487671, 0.02720414474606514, 0.06551450490951538, 0.01041676476597786, 0.035003695636987686, 0.003285663668066263, 0.0009142328053712845, 0.012067622505128384], "entropies_second": null, "final_normalized_entropy": -1.3598974800193373, "sequence_confidence_first": 0.7587565599869758, "sequence_confidence_second": null, "sequence_confidence_final": 0.7587565599869758, "token_confidences_first": [0.5483267307281494, 0.9928501844406128, 0.3783944547176361, 0.9572584629058838, 0.9983519315719604, 0.572521448135376, 0.7339297533035278, 0.5879330635070801, 0.22235248982906342, 0.8859431743621826, 0.6911757588386536, 0.9988282322883606, 0.6166207790374756, 0.9969882369041443, 0.9899060130119324, 0.9990999698638916, 0.9965477585792542, 0.9996957778930664, 0.9999228715896606, 0.9984435439109802], "token_confidences_second": null, "final_mean_entropy": 0.5687409419333562, "final_min_margin": 0.17942333221435547, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 36.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["CHOCOLATE."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a creative and humorous infographic that pairs wine and food. It's", "used_ocr": false, "answer_first": "This image is a creative and humorous infographic that pairs wine and food. It's", "answer_second": null, "raw_answer": "This image is a creative and humorous infographic that pairs wine and food. It's", "raw_answer_first": "This image is a creative and humorous infographic that pairs wine and food. It's", "raw_answer_second": null, "mean_entropy_first": 1.311252455135309, "normalized_entropy_first": 1.9758274346902827, "min_margin_first": 0.014682769775390625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9620682001113892, 0.8562377691268921, 1.7712078094482422, 0.7341403961181641, 3.0625734329223633, 0.019085988402366638, 2.0675907135009766, 2.5780811309814453, 0.000572871882468462, 1.9135150909423828, 0.007519010920077562, 1.2338979244232178, 1.0856080055236816, 1.8432667255401611, 1.4738240242004395, 0.21839691698551178, 2.0084400177001953, 1.5072652101516724, 2.8817288875579834, 2.8976266548852436e-05], "entropies_second": null, "final_normalized_entropy": 1.9758274346902827, "sequence_confidence_first": 0.5558451234121002, "sequence_confidence_second": null, "sequence_confidence_final": 0.5558451234121002, "token_confidences_first": [0.5543299317359924, 0.5633981227874756, 0.47244757413864136, 0.7905731201171875, 0.1958596259355545, 0.9972846508026123, 0.45126399397850037, 0.2227715104818344, 0.999958872795105, 0.5648603439331055, 0.9991816878318787, 0.7494526505470276, 0.8174154758453369, 0.4629426598548889, 0.5014262199401855, 0.9645423293113708, 0.36148905754089355, 0.5100171566009521, 0.2424670159816742, 0.9999984502792358], "token_confidences_second": null, "final_mean_entropy": 1.311252455135309, "final_min_margin": 0.014682769775390625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.2, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["LIGHT RED."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a creative and humorous infographic that pairs wine and food. It's", "used_ocr": false, "answer_first": "This image is a creative and humorous infographic that pairs wine and food. It's", "answer_second": null, "raw_answer": "This image is a creative and humorous infographic that pairs wine and food. It's", "raw_answer_first": "This image is a creative and humorous infographic that pairs wine and food. It's", "raw_answer_second": null, "mean_entropy_first": 1.311252455135309, "normalized_entropy_first": 1.589651928390658, "min_margin_first": 0.014682769775390625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1353, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9620682001113892, 0.8562377691268921, 1.7712078094482422, 0.7341403961181641, 3.0625734329223633, 0.019085988402366638, 2.0675907135009766, 2.5780811309814453, 0.000572871882468462, 1.9135150909423828, 0.007519010920077562, 1.2338979244232178, 1.0856080055236816, 1.8432667255401611, 1.4738240242004395, 0.21839691698551178, 2.0084400177001953, 1.5072652101516724, 2.8817288875579834, 2.8976266548852436e-05], "entropies_second": null, "final_normalized_entropy": 1.589651928390658, "sequence_confidence_first": 0.5558451234121002, "sequence_confidence_second": null, "sequence_confidence_final": 0.5558451234121002, "token_confidences_first": [0.5543299317359924, 0.5633981227874756, 0.47244757413864136, 0.7905731201171875, 0.1958596259355545, 0.9972846508026123, 0.45126399397850037, 0.2227715104818344, 0.999958872795105, 0.5648603439331055, 0.9991816878318787, 0.7494526505470276, 0.8174154758453369, 0.4629426598548889, 0.5014262199401855, 0.9645423293113708, 0.36148905754089355, 0.5100171566009521, 0.2424670159816742, 0.9999984502792358], "token_confidences_second": null, "final_mean_entropy": 1.311252455135309, "final_min_margin": 0.014682769775390625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.1, "wer": 7.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Green."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a flowchart or mind map related to the TV show \"The Simps", "used_ocr": false, "answer_first": "This image appears to be a flowchart or mind map related to the TV show \"The Simps", "answer_second": null, "raw_answer": "This image appears to be a flowchart or mind map related to the TV show \"The Simps", "raw_answer_first": "This image appears to be a flowchart or mind map related to the TV show \"The Simps", "raw_answer_second": null, "mean_entropy_first": 1.0607421473090652, "normalized_entropy_first": 0.4495425070660962, "min_margin_first": 0.14381027221679688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1386, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1388, "total_latency_s": 1.388, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9192484617233276, 0.8097786903381348, 1.056195855140686, 0.00016667481395415962, 0.016997231170535088, 0.5994060039520264, 3.0071630477905273, 0.044309861958026886, 1.716851830482483, 2.4975943565368652, 0.062278591096401215, 2.750333786010742, 0.0006076536956243217, 2.034022808074951, 2.088716506958008, 0.2380695641040802, 0.06171972304582596, 2.224618434906006, 1.0740212202072144, 0.012742644175887108], "entropies_second": null, "final_normalized_entropy": 0.4495425070660962, "sequence_confidence_first": 0.6242555581948847, "sequence_confidence_second": null, "sequence_confidence_final": 0.6242555581948847, "token_confidences_first": [0.5540412068367004, 0.627608597278595, 0.5593825578689575, 0.9999880790710449, 0.9981694221496582, 0.7535849809646606, 0.2591265141963959, 0.9933263063430786, 0.6467705965042114, 0.24964651465415955, 0.9899152517318726, 0.28855815529823303, 0.9999568462371826, 0.38184046745300293, 0.547575831413269, 0.9643959403038025, 0.9937060475349426, 0.31624096632003784, 0.7350724935531616, 0.9983829259872437], "token_confidences_second": null, "final_mean_entropy": 1.0607421473090652, "final_min_margin": 0.14381027221679688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.0, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["100 sq. feet."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a poster outlining steps for", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a poster outlining steps for", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a poster outlining steps for", "raw_answer_first": "The image you've provided appears to be an infographic or a poster outlining steps for", "raw_answer_second": null, "mean_entropy_first": 0.926301965379389, "normalized_entropy_first": -0.08063505563874679, "min_margin_first": 0.0047607421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1236, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1237, "total_latency_s": 1.237, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0143890380859375, 0.05819319188594818, 1.4958593845367432, 0.2678294777870178, 0.00406606262549758, 0.7108851075172424, 0.923087477684021, 7.729687058599666e-05, 0.020650386810302734, 0.7327737212181091, 1.3090327978134155, 0.016977380961179733, 1.8279982805252075, 2.409954071044922, 2.6573235988616943, 2.254330635070801, 0.00298580271191895, 1.571512984810397e-05, 1.4851163625717163, 1.3344935178756714], "entropies_second": null, "final_normalized_entropy": -0.08063505563874679, "sequence_confidence_first": 0.613251077156034, "sequence_confidence_second": null, "sequence_confidence_final": 0.613251077156034, "token_confidences_first": [0.5005213022232056, 0.993029773235321, 0.5018328428268433, 0.9420434832572937, 0.9995664954185486, 0.6385208368301392, 0.5662127733230591, 0.9999953508377075, 0.9976819753646851, 0.5105884671211243, 0.47529634833335876, 0.9980005621910095, 0.5029541850090027, 0.2449946254491806, 0.28862646222114563, 0.3697749674320221, 0.9997230172157288, 0.9999991655349731, 0.5219209790229797, 0.40196293592453003], "token_confidences_second": null, "final_mean_entropy": 0.926301965379389, "final_min_margin": 0.0047607421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.153846153846154, "wer": 5.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Boston."], "experiment": "baseline_no_ocr", "routed": {"answer": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "used_ocr": false, "answer_first": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "answer_second": null, "raw_answer": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "raw_answer_first": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "raw_answer_second": null, "mean_entropy_first": 1.127188179118093, "normalized_entropy_first": 0.7153154204589419, "min_margin_first": 0.1580829620361328, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1286, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1287, "total_latency_s": 1.287, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9467146396636963, 1.0651311874389648, 0.3390358090400696, 2.600796699523926, 0.0319608673453331, 1.0040937662124634, 2.3648626804351807, 2.731952428817749, 1.2880165576934814, 2.574467182159424, 1.4810835123062134, 0.007782385218888521, 0.2971409857273102, 0.6907618641853333, 1.0861539840698242, 0.0032291675452142954, 0.2537917494773865, 0.48349082469940186, 1.455070972442627, 1.838226318359375], "entropies_second": null, "final_normalized_entropy": 0.7153154204589419, "sequence_confidence_first": 0.6095659179404592, "sequence_confidence_second": null, "sequence_confidence_final": 0.6095659179404592, "token_confidences_first": [0.556664228439331, 0.4641276001930237, 0.8968478441238403, 0.4547146260738373, 0.9951063394546509, 0.802984893321991, 0.30832332372665405, 0.3171643018722534, 0.6108274459838867, 0.39842408895492554, 0.32524943351745605, 0.9993135929107666, 0.9498127102851868, 0.7911922335624695, 0.5277038812637329, 0.9997201561927795, 0.9426642656326294, 0.859346866607666, 0.4351966679096222, 0.5511569380760193], "token_confidences_second": null, "final_mean_entropy": 1.127188179118093, "final_min_margin": 0.1580829620361328, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.571428571428571, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["SAMUEL ADAMS."], "experiment": "baseline_no_ocr", "routed": {"answer": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "used_ocr": false, "answer_first": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "answer_second": null, "raw_answer": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "raw_answer_first": "This is a resume or a professional profile for an individual named Daniel Tewfik. The profile", "raw_answer_second": null, "mean_entropy_first": 1.127188179118093, "normalized_entropy_first": 0.6618854831103674, "min_margin_first": 0.1580829620361328, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1280, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1281, "total_latency_s": 1.281, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9467146396636963, 1.0651311874389648, 0.3390358090400696, 2.600796699523926, 0.0319608673453331, 1.0040937662124634, 2.3648626804351807, 2.731952428817749, 1.2880165576934814, 2.574467182159424, 1.4810835123062134, 0.007782385218888521, 0.2971409857273102, 0.6907618641853333, 1.0861539840698242, 0.0032291675452142954, 0.2537917494773865, 0.48349082469940186, 1.455070972442627, 1.838226318359375], "entropies_second": null, "final_normalized_entropy": 0.6618854831103674, "sequence_confidence_first": 0.6095659179404592, "sequence_confidence_second": null, "sequence_confidence_final": 0.6095659179404592, "token_confidences_first": [0.556664228439331, 0.4641276001930237, 0.8968478441238403, 0.4547146260738373, 0.9951063394546509, 0.802984893321991, 0.30832332372665405, 0.3171643018722534, 0.6108274459838867, 0.39842408895492554, 0.32524943351745605, 0.9993135929107666, 0.9498127102851868, 0.7911922335624695, 0.5277038812637329, 0.9997201561927795, 0.9426642656326294, 0.859346866607666, 0.4351966679096222, 0.5511569380760193], "token_confidences_second": null, "final_mean_entropy": 1.127188179118093, "final_min_margin": 0.1580829620361328, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.3076923076923075, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["M."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "used_ocr": false, "answer_first": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "answer_second": null, "raw_answer": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "raw_answer_first": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "raw_answer_second": null, "mean_entropy_first": 1.0574358169433253, "normalized_entropy_first": 0.32297389931779, "min_margin_first": 0.03285980224609375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1348, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1349, "total_latency_s": 1.349, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8608479499816895, 0.10222122073173523, 1.8766965866088867, 0.7218496203422546, 2.99674129486084, 0.3290707767009735, 0.012125196866691113, 1.9364718198776245, 0.01190621592104435, 1.2705235481262207, 1.3611394166946411, 3.5286173820495605, 1.9136629104614258, 2.628319501876831, 0.40226519107818604, 1.049504280090332, 0.14138194918632507, 0.0018229209817945957, 0.0033138664439320564, 0.00023468998551834375], "entropies_second": null, "final_normalized_entropy": 0.32297389931779, "sequence_confidence_first": 0.5916202244953797, "sequence_confidence_second": null, "sequence_confidence_final": 0.5916202244953797, "token_confidences_first": [0.533277153968811, 0.9867746829986572, 0.32534295320510864, 0.7358837723731995, 0.15908323228359222, 0.9099851250648499, 0.9983695149421692, 0.33167028427124023, 0.9986193180084229, 0.5561398863792419, 0.6218953728675842, 0.11822808533906937, 0.36233365535736084, 0.46007558703422546, 0.9137774705886841, 0.7511749863624573, 0.9785134792327881, 0.9998650550842285, 0.9996312856674194, 0.9999829530715942], "token_confidences_second": null, "final_mean_entropy": 1.0574358169433253, "final_min_margin": 0.03285980224609375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 46.5, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "", "image_id": null, "answers": ["1991."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "used_ocr": false, "answer_first": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "answer_second": null, "raw_answer": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "raw_answer_first": "The image is a vibrant infographic comparing the pop culture personas of Madonna and Lady Gaga", "raw_answer_second": null, "mean_entropy_first": 1.0574358169433253, "normalized_entropy_first": 0.3048142768987245, "min_margin_first": 0.03285980224609375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1353, "total_latency_s": 1.353, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8608479499816895, 0.10222122073173523, 1.8766965866088867, 0.7218496203422546, 2.99674129486084, 0.3290707767009735, 0.012125196866691113, 1.9364718198776245, 0.01190621592104435, 1.2705235481262207, 1.3611394166946411, 3.5286173820495605, 1.9136629104614258, 2.628319501876831, 0.40226519107818604, 1.049504280090332, 0.14138194918632507, 0.0018229209817945957, 0.0033138664439320564, 0.00023468998551834375], "entropies_second": null, "final_normalized_entropy": 0.3048142768987245, "sequence_confidence_first": 0.5916202244953797, "sequence_confidence_second": null, "sequence_confidence_final": 0.5916202244953797, "token_confidences_first": [0.533277153968811, 0.9867746829986572, 0.32534295320510864, 0.7358837723731995, 0.15908323228359222, 0.9099851250648499, 0.9983695149421692, 0.33167028427124023, 0.9986193180084229, 0.5561398863792419, 0.6218953728675842, 0.11822808533906937, 0.36233365535736084, 0.46007558703422546, 0.9137774705886841, 0.7511749863624573, 0.9785134792327881, 0.9998650550842285, 0.9996312856674194, 0.9999829530715942], "token_confidences_second": null, "final_mean_entropy": 1.0574358169433253, "final_min_margin": 0.03285980224609375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.8, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["97%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to the tourism", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to the tourism", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to the tourism", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to the tourism", "raw_answer_second": null, "mean_entropy_first": 0.857470489397383, "normalized_entropy_first": -0.632018463708274, "min_margin_first": 0.3029365539550781, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7848620414733887, 0.05001632869243622, 1.649468183517456, 0.2807919979095459, 0.00157132965978235, 0.5761290788650513, 0.9268217086791992, 3.4489043173380196e-05, 0.03516527637839317, 0.7355327606201172, 2.676772117614746, 0.599709153175354, 1.7291533946990967, 2.2819085121154785, 0.016967760398983955, 2.009279489517212, 0.00012987342779524624, 1.09199857711792, 1.6974895000457764, 0.005608214996755123], "entropies_second": null, "final_normalized_entropy": -0.632018463708274, "sequence_confidence_first": 0.6868138356685528, "sequence_confidence_second": null, "sequence_confidence_final": 0.6868138356685528, "token_confidences_first": [0.566371738910675, 0.9936960339546204, 0.4039463400840759, 0.9346914291381836, 0.9998528957366943, 0.7746975421905518, 0.5854039788246155, 0.9999980926513672, 0.9952796697616577, 0.7690122723579407, 0.303450345993042, 0.7880931496620178, 0.5082430243492126, 0.3909826874732971, 0.997793436050415, 0.489065945148468, 0.999991774559021, 0.7038354277610779, 0.45346301794052124, 0.9993565678596497], "token_confidences_second": null, "final_mean_entropy": 0.857470489397383, "final_min_margin": 0.3029365539550781, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 21.75, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.40917718853170876, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1788, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1789, "total_latency_s": 1.789, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.40917718853170876, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 57.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2 in 5 people."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.384970241211575, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1196, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1197, "total_latency_s": 1.197, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.384970241211575, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.5, "wer": 4.0, "precision": 0.0625, "recall": 0.25, "f1": 0.1, "rouge_l": 0.1, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Female."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.36253827200873817, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1199, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1199, "total_latency_s": 1.199, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.36253827200873817, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.571428571428571, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["10%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.3416958115718279, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1196, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1197, "total_latency_s": 1.197, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.3416958115718279, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 28.5, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["45%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.3222851327575222, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1198, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1199, "total_latency_s": 1.199, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.3222851327575222, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 28.5, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Mental health condition."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.3041709265956822, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1193, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1194, "total_latency_s": 1.194, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.3041709265956822, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.083333333333333, "wer": 5.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["5."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "used_ocr": false, "answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "answer_second": null, "raw_answer": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_first": "The image you've provided is an infographic that provides statistics and information about disability in Australia", "raw_answer_second": null, "mean_entropy_first": 0.895069853804307, "normalized_entropy_first": -0.28723618318173905, "min_margin_first": 0.18006134033203125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1192, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1193, "total_latency_s": 1.193, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8358629941940308, 0.13949181139469147, 2.0557353496551514, 0.35196924209594727, 0.0017665611812844872, 0.6564803719520569, 1.2314271926879883, 0.7741877436637878, 0.6889969706535339, 0.010239219292998314, 2.570322036743164, 2.3882532119750977, 1.7209806442260742, 1.4246716499328613, 1.1496577262878418, 0.8628367781639099, 0.17028042674064636, 0.06460607051849365, 0.7316783666610718, 0.0719527080655098], "entropies_second": null, "final_normalized_entropy": -0.28723618318173905, "sequence_confidence_first": 0.636519039983397, "sequence_confidence_second": null, "sequence_confidence_final": 0.636519039983397, "token_confidences_first": [0.6567086577415466, 0.978669285774231, 0.2767903208732605, 0.9130047559738159, 0.999843955039978, 0.7080227136611938, 0.5054326057434082, 0.5976619720458984, 0.7526035308837891, 0.998808741569519, 0.23619438707828522, 0.344363272190094, 0.3924921751022339, 0.3364390432834625, 0.744629979133606, 0.7159951329231262, 0.9709563851356506, 0.9881212711334229, 0.8401710391044617, 0.9890002608299255], "token_confidences_second": null, "final_mean_entropy": 0.895069853804307, "final_min_margin": 0.18006134033203125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 57.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "used_ocr": false, "answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "answer_second": null, "raw_answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_second": null, "mean_entropy_first": 0.7906676439568401, "normalized_entropy_first": -0.9597394652211514, "min_margin_first": 0.011142730712890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1134, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1135, "total_latency_s": 1.135, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.137905478477478, 0.1456771194934845, 2.1684536933898926, 1.2114273309707642, 2.001927375793457, 0.3597111105918884, 2.5044145584106445, 1.6978821754455566, 1.139235496520996, 0.01840558648109436, 1.2384262084960938, 0.0030030785128474236, 0.01955777406692505, 0.0037752934731543064, 0.5845799446105957, 0.8026753664016724, 0.5494741201400757, 0.00175148481503129, 0.17957830429077148, 0.045491378754377365], "entropies_second": null, "final_normalized_entropy": -0.9597394652211514, "sequence_confidence_first": 0.6940421798113289, "sequence_confidence_second": null, "sequence_confidence_final": 0.6940421798113289, "token_confidences_first": [0.6416327953338623, 0.9801161289215088, 0.3705257773399353, 0.4339473247528076, 0.5451046824455261, 0.8860549330711365, 0.3065987825393677, 0.3451178967952728, 0.7620182633399963, 0.9981582760810852, 0.4488134980201721, 0.9997084736824036, 0.997198224067688, 0.9996902942657471, 0.8294879198074341, 0.6268858909606934, 0.7716584801673889, 0.9998525381088257, 0.959724485874176, 0.9934866428375244], "token_confidences_second": null, "final_mean_entropy": 0.7906676439568401, "final_min_margin": 0.011142730712890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 32.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["More than 3500."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "used_ocr": false, "answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "answer_second": null, "raw_answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_second": null, "mean_entropy_first": 0.7906676439568401, "normalized_entropy_first": -0.8712472487538543, "min_margin_first": 0.011142730712890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1135, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1135, "total_latency_s": 1.135, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.137905478477478, 0.1456771194934845, 2.1684536933898926, 1.2114273309707642, 2.001927375793457, 0.3597111105918884, 2.5044145584106445, 1.6978821754455566, 1.139235496520996, 0.01840558648109436, 1.2384262084960938, 0.0030030785128474236, 0.01955777406692505, 0.0037752934731543064, 0.5845799446105957, 0.8026753664016724, 0.5494741201400757, 0.00175148481503129, 0.17957830429077148, 0.045491378754377365], "entropies_second": null, "final_normalized_entropy": -0.8712472487538543, "sequence_confidence_first": 0.6940421798113289, "sequence_confidence_second": null, "sequence_confidence_final": 0.6940421798113289, "token_confidences_first": [0.6416327953338623, 0.9801161289215088, 0.3705257773399353, 0.4339473247528076, 0.5451046824455261, 0.8860549330711365, 0.3065987825393677, 0.3451178967952728, 0.7620182633399963, 0.9981582760810852, 0.4488134980201721, 0.9997084736824036, 0.997198224067688, 0.9996902942657471, 0.8294879198074341, 0.6268858909606934, 0.7716584801673889, 0.9998525381088257, 0.959724485874176, 0.9934866428375244], "token_confidences_second": null, "final_mean_entropy": 0.7906676439568401, "final_min_margin": 0.011142730712890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.8, "wer": 4.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Red."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "used_ocr": false, "answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "answer_second": null, "raw_answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_second": null, "mean_entropy_first": 0.7906676439568401, "normalized_entropy_first": -0.7968476059052367, "min_margin_first": 0.011142730712890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1132, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1132, "total_latency_s": 1.132, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.137905478477478, 0.1456771194934845, 2.1684536933898926, 1.2114273309707642, 2.001927375793457, 0.3597111105918884, 2.5044145584106445, 1.6978821754455566, 1.139235496520996, 0.01840558648109436, 1.2384262084960938, 0.0030030785128474236, 0.01955777406692505, 0.0037752934731543064, 0.5845799446105957, 0.8026753664016724, 0.5494741201400757, 0.00175148481503129, 0.17957830429077148, 0.045491378754377365], "entropies_second": null, "final_normalized_entropy": -0.7968476059052367, "sequence_confidence_first": 0.6940421798113289, "sequence_confidence_second": null, "sequence_confidence_final": 0.6940421798113289, "token_confidences_first": [0.6416327953338623, 0.9801161289215088, 0.3705257773399353, 0.4339473247528076, 0.5451046824455261, 0.8860549330711365, 0.3065987825393677, 0.3451178967952728, 0.7620182633399963, 0.9981582760810852, 0.4488134980201721, 0.9997084736824036, 0.997198224067688, 0.9996902942657471, 0.8294879198074341, 0.6268858909606934, 0.7716584801673889, 0.9998525381088257, 0.959724485874176, 0.9934866428375244], "token_confidences_second": null, "final_mean_entropy": 0.7906676439568401, "final_min_margin": 0.011142730712890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.25, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "used_ocr": false, "answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "answer_second": null, "raw_answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_second": null, "mean_entropy_first": 0.7906676439568401, "normalized_entropy_first": -0.7330413473313203, "min_margin_first": 0.011142730712890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1132, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1133, "total_latency_s": 1.133, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.137905478477478, 0.1456771194934845, 2.1684536933898926, 1.2114273309707642, 2.001927375793457, 0.3597111105918884, 2.5044145584106445, 1.6978821754455566, 1.139235496520996, 0.01840558648109436, 1.2384262084960938, 0.0030030785128474236, 0.01955777406692505, 0.0037752934731543064, 0.5845799446105957, 0.8026753664016724, 0.5494741201400757, 0.00175148481503129, 0.17957830429077148, 0.045491378754377365], "entropies_second": null, "final_normalized_entropy": -0.7330413473313203, "sequence_confidence_first": 0.6940421798113289, "sequence_confidence_second": null, "sequence_confidence_final": 0.6940421798113289, "token_confidences_first": [0.6416327953338623, 0.9801161289215088, 0.3705257773399353, 0.4339473247528076, 0.5451046824455261, 0.8860549330711365, 0.3065987825393677, 0.3451178967952728, 0.7620182633399963, 0.9981582760810852, 0.4488134980201721, 0.9997084736824036, 0.997198224067688, 0.9996902942657471, 0.8294879198074341, 0.6268858909606934, 0.7716584801673889, 0.9998525381088257, 0.959724485874176, 0.9934866428375244], "token_confidences_second": null, "final_mean_entropy": 0.7906676439568401, "final_min_margin": 0.011142730712890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 32.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "used_ocr": false, "answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "answer_second": null, "raw_answer": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_first": "The image is a graphic with the title \"SPEEDING: Fast Facts on a", "raw_answer_second": null, "mean_entropy_first": 0.7906676439568401, "normalized_entropy_first": -0.6774605832328775, "min_margin_first": 0.011142730712890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1136, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1136, "total_latency_s": 1.136, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.137905478477478, 0.1456771194934845, 2.1684536933898926, 1.2114273309707642, 2.001927375793457, 0.3597111105918884, 2.5044145584106445, 1.6978821754455566, 1.139235496520996, 0.01840558648109436, 1.2384262084960938, 0.0030030785128474236, 0.01955777406692505, 0.0037752934731543064, 0.5845799446105957, 0.8026753664016724, 0.5494741201400757, 0.00175148481503129, 0.17957830429077148, 0.045491378754377365], "entropies_second": null, "final_normalized_entropy": -0.6774605832328775, "sequence_confidence_first": 0.6940421798113289, "sequence_confidence_second": null, "sequence_confidence_final": 0.6940421798113289, "token_confidences_first": [0.6416327953338623, 0.9801161289215088, 0.3705257773399353, 0.4339473247528076, 0.5451046824455261, 0.8860549330711365, 0.3065987825393677, 0.3451178967952728, 0.7620182633399963, 0.9981582760810852, 0.4488134980201721, 0.9997084736824036, 0.997198224067688, 0.9996902942657471, 0.8294879198074341, 0.6268858909606934, 0.7716584801673889, 0.9998525381088257, 0.959724485874176, 0.9934866428375244], "token_confidences_second": null, "final_mean_entropy": 0.7906676439568401, "final_min_margin": 0.011142730712890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 32.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["8."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "raw_answer_second": null, "mean_entropy_first": 0.8980719066574239, "normalized_entropy_first": 0.15684112501065398, "min_margin_first": 0.1304464340209961, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1343, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1344, "total_latency_s": 1.344, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8706020712852478, 0.06724387407302856, 1.5117067098617554, 0.29421067237854004, 0.002818887121975422, 0.6422077417373657, 0.7936318516731262, 4.009070107713342e-05, 0.01304424088448286, 0.6572479605674744, 3.302764892578125, 0.6260450482368469, 1.9591901302337646, 1.7681870460510254, 0.016600806266069412, 1.7181552648544312, 0.00025222793919965625, 1.1556140184402466, 2.318309783935547, 0.24356481432914734], "entropies_second": null, "final_normalized_entropy": 0.15684112501065398, "sequence_confidence_first": 0.6822875809525434, "sequence_confidence_second": null, "sequence_confidence_final": 0.6822875809525434, "token_confidences_first": [0.5200639963150024, 0.9912607073783875, 0.44912630319595337, 0.9323201179504395, 0.9997233748435974, 0.7188332080841064, 0.5706810355186462, 0.999997615814209, 0.9985184073448181, 0.7095879912376404, 0.2727380394935608, 0.6959134340286255, 0.5859693884849548, 0.5927104949951172, 0.9979050159454346, 0.5423612594604492, 0.9999827146530151, 0.7658323049545288, 0.2884683907032013, 0.9666908979415894], "token_confidences_second": null, "final_mean_entropy": 0.8980719066574239, "final_min_margin": 0.1304464340209961, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 50.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Roads."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "raw_answer_first": "The image you've provided appears to be a graphic or infographic related to the Australian Broadcast", "raw_answer_second": null, "mean_entropy_first": 0.8980719066574239, "normalized_entropy_first": 0.1486098842588608, "min_margin_first": 0.1304464340209961, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8706020712852478, 0.06724387407302856, 1.5117067098617554, 0.29421067237854004, 0.002818887121975422, 0.6422077417373657, 0.7936318516731262, 4.009070107713342e-05, 0.01304424088448286, 0.6572479605674744, 3.302764892578125, 0.6260450482368469, 1.9591901302337646, 1.7681870460510254, 0.016600806266069412, 1.7181552648544312, 0.00025222793919965625, 1.1556140184402466, 2.318309783935547, 0.24356481432914734], "entropies_second": null, "final_normalized_entropy": 0.1486098842588608, "sequence_confidence_first": 0.6822875809525434, "sequence_confidence_second": null, "sequence_confidence_final": 0.6822875809525434, "token_confidences_first": [0.5200639963150024, 0.9912607073783875, 0.44912630319595337, 0.9323201179504395, 0.9997233748435974, 0.7188332080841064, 0.5706810355186462, 0.999997615814209, 0.9985184073448181, 0.7095879912376404, 0.2727380394935608, 0.6959134340286255, 0.5859693884849548, 0.5927104949951172, 0.9979050159454346, 0.5423612594604492, 0.9999827146530151, 0.7658323049545288, 0.2884683907032013, 0.9666908979415894], "token_confidences_second": null, "final_mean_entropy": 0.8980719066574239, "final_min_margin": 0.1304464340209961, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.833333333333334, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["12."], "experiment": "baseline_no_ocr", "routed": {"answer": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "used_ocr": false, "answer_first": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "answer_second": null, "raw_answer": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "raw_answer_first": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "raw_answer_second": null, "mean_entropy_first": 1.3561130702496484, "normalized_entropy_first": 3.8531959872002606, "min_margin_first": 0.03833198547363281, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1358, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1360, "total_latency_s": 1.36, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3626796007156372, 0.6829910278320312, 0.6952599883079529, 1.1363362073898315, 0.06772252917289734, 0.9660614728927612, 1.2073900699615479, 1.3778096437454224, 2.617004156112671, 2.0734572410583496, 2.1119887828826904, 1.6924228475545533e-05, 0.007491159252822399, 1.797402024269104, 1.9034217596054077, 1.7162812948226929, 2.390126943588257, 2.9919252395629883, 0.0013280368875712156, 2.0155673027038574], "entropies_second": null, "final_normalized_entropy": 3.8531959872002606, "sequence_confidence_first": 0.5488360943518859, "sequence_confidence_second": null, "sequence_confidence_final": 0.5488360943518859, "token_confidences_first": [0.42299121618270874, 0.7228522896766663, 0.5476363897323608, 0.7235378623008728, 0.9875580072402954, 0.7942795753479004, 0.3948483169078827, 0.6408357620239258, 0.2684613764286041, 0.3420039117336273, 0.5240696668624878, 0.9999990463256836, 0.9992006421089172, 0.49240607023239136, 0.44950151443481445, 0.6771718263626099, 0.34936508536338806, 0.18243208527565002, 0.9999040365219116, 0.5568767786026001], "token_confidences_second": null, "final_mean_entropy": 1.3561130702496484, "final_min_margin": 0.03833198547363281, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 33.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["5."], "experiment": "baseline_no_ocr", "routed": {"answer": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "used_ocr": false, "answer_first": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "answer_second": null, "raw_answer": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "raw_answer_first": "This is an illustration of Australia, featuring a stylized map with various elements such as cities", "raw_answer_second": null, "mean_entropy_first": 1.3561130702496484, "normalized_entropy_first": 2.3190191304772054, "min_margin_first": 0.03833198547363281, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1355, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1357, "total_latency_s": 1.357, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3626796007156372, 0.6829910278320312, 0.6952599883079529, 1.1363362073898315, 0.06772252917289734, 0.9660614728927612, 1.2073900699615479, 1.3778096437454224, 2.617004156112671, 2.0734572410583496, 2.1119887828826904, 1.6924228475545533e-05, 0.007491159252822399, 1.797402024269104, 1.9034217596054077, 1.7162812948226929, 2.390126943588257, 2.9919252395629883, 0.0013280368875712156, 2.0155673027038574], "entropies_second": null, "final_normalized_entropy": 2.3190191304772054, "sequence_confidence_first": 0.5488360943518859, "sequence_confidence_second": null, "sequence_confidence_final": 0.5488360943518859, "token_confidences_first": [0.42299121618270874, 0.7228522896766663, 0.5476363897323608, 0.7235378623008728, 0.9875580072402954, 0.7942795753479004, 0.3948483169078827, 0.6408357620239258, 0.2684613764286041, 0.3420039117336273, 0.5240696668624878, 0.9999990463256836, 0.9992006421089172, 0.49240607023239136, 0.44950151443481445, 0.6771718263626099, 0.34936508536338806, 0.18243208527565002, 0.9999040365219116, 0.5568767786026001], "token_confidences_second": null, "final_mean_entropy": 1.3561130702496484, "final_min_margin": 0.03833198547363281, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 49.5, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Internet."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_second": null, "mean_entropy_first": 0.7117708985955687, "normalized_entropy_first": -1.1943851623899573, "min_margin_first": 0.10040664672851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8819787502288818, 0.03805650770664215, 1.7109628915786743, 0.24726001918315887, 0.005591533612459898, 0.7574103474617004, 0.9273022413253784, 0.00016639573732391, 0.03439990431070328, 0.7555526494979858, 2.4822919368743896, 0.7007344961166382, 0.04105493053793907, 0.06377904117107391, 2.383218288421631, 2.506983757019043, 0.02099352329969406, 0.6674097776412964, 0.006362087093293667, 0.003908893093466759], "entropies_second": null, "final_normalized_entropy": -1.1943851623899573, "sequence_confidence_first": 0.6978113041468613, "sequence_confidence_second": null, "sequence_confidence_final": 0.6978113041468613, "token_confidences_first": [0.5110316872596741, 0.9957226514816284, 0.38575130701065063, 0.9467407464981079, 0.9993858337402344, 0.6256653070449829, 0.5319452881813049, 0.9999885559082031, 0.9960663914680481, 0.6183473467826843, 0.4284181296825409, 0.7329336404800415, 0.9935429096221924, 0.9922378659248352, 0.29102402925491333, 0.29715341329574585, 0.9977867603302002, 0.7377503514289856, 0.9993857145309448, 0.9996603727340698], "token_confidences_second": null, "final_mean_entropy": 0.7117708985955687, "final_min_margin": 0.10040664672851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.555555555555555, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["6."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_second": null, "mean_entropy_first": 0.7117708985955687, "normalized_entropy_first": -1.0600051608330026, "min_margin_first": 0.10040664672851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8819787502288818, 0.03805650770664215, 1.7109628915786743, 0.24726001918315887, 0.005591533612459898, 0.7574103474617004, 0.9273022413253784, 0.00016639573732391, 0.03439990431070328, 0.7555526494979858, 2.4822919368743896, 0.7007344961166382, 0.04105493053793907, 0.06377904117107391, 2.383218288421631, 2.506983757019043, 0.02099352329969406, 0.6674097776412964, 0.006362087093293667, 0.003908893093466759], "entropies_second": null, "final_normalized_entropy": -1.0600051608330026, "sequence_confidence_first": 0.6978113041468613, "sequence_confidence_second": null, "sequence_confidence_final": 0.6978113041468613, "token_confidences_first": [0.5110316872596741, 0.9957226514816284, 0.38575130701065063, 0.9467407464981079, 0.9993858337402344, 0.6256653070449829, 0.5319452881813049, 0.9999885559082031, 0.9960663914680481, 0.6183473467826843, 0.4284181296825409, 0.7329336404800415, 0.9935429096221924, 0.9922378659248352, 0.29102402925491333, 0.29715341329574585, 0.9977867603302002, 0.7377503514289856, 0.9993857145309448, 0.9996603727340698], "token_confidences_second": null, "final_mean_entropy": 0.7117708985955687, "final_min_margin": 0.10040664672851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 41.5, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["White."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_second": null, "mean_entropy_first": 0.7117708985955687, "normalized_entropy_first": -0.9534684767834302, "min_margin_first": 0.10040664672851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1354, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1354, "total_latency_s": 1.354, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8819787502288818, 0.03805650770664215, 1.7109628915786743, 0.24726001918315887, 0.005591533612459898, 0.7574103474617004, 0.9273022413253784, 0.00016639573732391, 0.03439990431070328, 0.7555526494979858, 2.4822919368743896, 0.7007344961166382, 0.04105493053793907, 0.06377904117107391, 2.383218288421631, 2.506983757019043, 0.02099352329969406, 0.6674097776412964, 0.006362087093293667, 0.003908893093466759], "entropies_second": null, "final_normalized_entropy": -0.9534684767834302, "sequence_confidence_first": 0.6978113041468613, "sequence_confidence_second": null, "sequence_confidence_final": 0.6978113041468613, "token_confidences_first": [0.5110316872596741, 0.9957226514816284, 0.38575130701065063, 0.9467407464981079, 0.9993858337402344, 0.6256653070449829, 0.5319452881813049, 0.9999885559082031, 0.9960663914680481, 0.6183473467826843, 0.4284181296825409, 0.7329336404800415, 0.9935429096221924, 0.9922378659248352, 0.29102402925491333, 0.29715341329574585, 0.9977867603302002, 0.7377503514289856, 0.9993857145309448, 0.9996603727340698], "token_confidences_second": null, "final_mean_entropy": 0.7117708985955687, "final_min_margin": 0.10040664672851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.166666666666666, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Media."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_first": "The image you've provided appears to be a slide from a presentation, titled \"Deadly", "raw_answer_second": null, "mean_entropy_first": 0.7117708985955687, "normalized_entropy_first": -0.8660303057673101, "min_margin_first": 0.10040664672851562, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1353, "total_latency_s": 1.353, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8819787502288818, 0.03805650770664215, 1.7109628915786743, 0.24726001918315887, 0.005591533612459898, 0.7574103474617004, 0.9273022413253784, 0.00016639573732391, 0.03439990431070328, 0.7555526494979858, 2.4822919368743896, 0.7007344961166382, 0.04105493053793907, 0.06377904117107391, 2.383218288421631, 2.506983757019043, 0.02099352329969406, 0.6674097776412964, 0.006362087093293667, 0.003908893093466759], "entropies_second": null, "final_normalized_entropy": -0.8660303057673101, "sequence_confidence_first": 0.6978113041468613, "sequence_confidence_second": null, "sequence_confidence_final": 0.6978113041468613, "token_confidences_first": [0.5110316872596741, 0.9957226514816284, 0.38575130701065063, 0.9467407464981079, 0.9993858337402344, 0.6256653070449829, 0.5319452881813049, 0.9999885559082031, 0.9960663914680481, 0.6183473467826843, 0.4284181296825409, 0.7329336404800415, 0.9935429096221924, 0.9922378659248352, 0.29102402925491333, 0.29715341329574585, 0.9977867603302002, 0.7377503514289856, 0.9993857145309448, 0.9996603727340698], "token_confidences_second": null, "final_mean_entropy": 0.7117708985955687, "final_min_margin": 0.10040664672851562, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.0, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Order, tip well, walk away."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "used_ocr": false, "answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "answer_second": null, "raw_answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_second": null, "mean_entropy_first": 0.7033220976998564, "normalized_entropy_first": -0.8317694104226351, "min_margin_first": 0.25957489013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1822, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1823, "total_latency_s": 1.823, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3446378707885742, 0.14021073281764984, 2.0802814960479736, 0.2757672667503357, 0.010136852972209454, 0.8546233177185059, 1.1475260257720947, 0.5300256013870239, 1.9481968879699707, 0.0012884365860372782, 1.9726252555847168, 0.03219215199351311, 2.3003177642822266, 0.04386093467473984, 0.5495318174362183, 0.11976876854896545, 0.5345867872238159, 0.03240580856800079, 0.14786092936992645, 0.0005972475046291947], "entropies_second": null, "final_normalized_entropy": -0.8317694104226351, "sequence_confidence_first": 0.7391203771543623, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391203771543623, "token_confidences_first": [0.5354316830635071, 0.9828269481658936, 0.28088316321372986, 0.9398400187492371, 0.9987414479255676, 0.5574624538421631, 0.5755826830863953, 0.8714181780815125, 0.4598904550075531, 0.9998927116394043, 0.5175709128379822, 0.9954342246055603, 0.40972059965133667, 0.9936898946762085, 0.8464242815971375, 0.9750951528549194, 0.7908768653869629, 0.9956489205360413, 0.9736035466194153, 0.9999504089355469], "token_confidences_second": null, "final_mean_entropy": 0.7033220976998564, "final_min_margin": 0.25957489013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.4074074074074074, "wer": 2.8, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Order, tip well, walk away."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "used_ocr": false, "answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "answer_second": null, "raw_answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_second": null, "mean_entropy_first": 0.7033220976998564, "normalized_entropy_first": -0.7631290155418297, "min_margin_first": 0.25957489013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1225, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1225, "total_latency_s": 1.225, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3446378707885742, 0.14021073281764984, 2.0802814960479736, 0.2757672667503357, 0.010136852972209454, 0.8546233177185059, 1.1475260257720947, 0.5300256013870239, 1.9481968879699707, 0.0012884365860372782, 1.9726252555847168, 0.03219215199351311, 2.3003177642822266, 0.04386093467473984, 0.5495318174362183, 0.11976876854896545, 0.5345867872238159, 0.03240580856800079, 0.14786092936992645, 0.0005972475046291947], "entropies_second": null, "final_normalized_entropy": -0.7631290155418297, "sequence_confidence_first": 0.7391203771543623, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391203771543623, "token_confidences_first": [0.5354316830635071, 0.9828269481658936, 0.28088316321372986, 0.9398400187492371, 0.9987414479255676, 0.5574624538421631, 0.5755826830863953, 0.8714181780815125, 0.4598904550075531, 0.9998927116394043, 0.5175709128379822, 0.9954342246055603, 0.40972059965133667, 0.9936898946762085, 0.8464242815971375, 0.9750951528549194, 0.7908768653869629, 0.9956489205360413, 0.9736035466194153, 0.9999504089355469], "token_confidences_second": null, "final_mean_entropy": 0.7033220976998564, "final_min_margin": 0.25957489013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.4074074074074074, "wer": 2.8, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["You're a regular."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "used_ocr": false, "answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "answer_second": null, "raw_answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_second": null, "mean_entropy_first": 0.7033220976998564, "normalized_entropy_first": -0.7037652754597127, "min_margin_first": 0.25957489013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1222, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1223, "total_latency_s": 1.223, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3446378707885742, 0.14021073281764984, 2.0802814960479736, 0.2757672667503357, 0.010136852972209454, 0.8546233177185059, 1.1475260257720947, 0.5300256013870239, 1.9481968879699707, 0.0012884365860372782, 1.9726252555847168, 0.03219215199351311, 2.3003177642822266, 0.04386093467473984, 0.5495318174362183, 0.11976876854896545, 0.5345867872238159, 0.03240580856800079, 0.14786092936992645, 0.0005972475046291947], "entropies_second": null, "final_normalized_entropy": -0.7037652754597127, "sequence_confidence_first": 0.7391203771543623, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391203771543623, "token_confidences_first": [0.5354316830635071, 0.9828269481658936, 0.28088316321372986, 0.9398400187492371, 0.9987414479255676, 0.5574624538421631, 0.5755826830863953, 0.8714181780815125, 0.4598904550075531, 0.9998927116394043, 0.5175709128379822, 0.9954342246055603, 0.40972059965133667, 0.9936898946762085, 0.8464242815971375, 0.9750951528549194, 0.7908768653869629, 0.9956489205360413, 0.9736035466194153, 0.9999504089355469], "token_confidences_second": null, "final_mean_entropy": 0.7033220976998564, "final_min_margin": 0.25957489013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.764705882352941, "wer": 4.333333333333333, "precision": 0.07142857142857142, "recall": 0.3333333333333333, "f1": 0.11764705882352941, "rouge_l": 0.11764705882352941, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "used_ocr": false, "answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "answer_second": null, "raw_answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_second": null, "mean_entropy_first": 0.7033220976998564, "normalized_entropy_first": -0.6517063600933943, "min_margin_first": 0.25957489013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1232, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1233, "total_latency_s": 1.233, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3446378707885742, 0.14021073281764984, 2.0802814960479736, 0.2757672667503357, 0.010136852972209454, 0.8546233177185059, 1.1475260257720947, 0.5300256013870239, 1.9481968879699707, 0.0012884365860372782, 1.9726252555847168, 0.03219215199351311, 2.3003177642822266, 0.04386093467473984, 0.5495318174362183, 0.11976876854896545, 0.5345867872238159, 0.03240580856800079, 0.14786092936992645, 0.0005972475046291947], "entropies_second": null, "final_normalized_entropy": -0.6517063600933943, "sequence_confidence_first": 0.7391203771543623, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391203771543623, "token_confidences_first": [0.5354316830635071, 0.9828269481658936, 0.28088316321372986, 0.9398400187492371, 0.9987414479255676, 0.5574624538421631, 0.5755826830863953, 0.8714181780815125, 0.4598904550075531, 0.9998927116394043, 0.5175709128379822, 0.9954342246055603, 0.40972059965133667, 0.9936898946762085, 0.8464242815971375, 0.9750951528549194, 0.7908768653869629, 0.9956489205360413, 0.9736035466194153, 0.9999504089355469], "token_confidences_second": null, "final_mean_entropy": 0.7033220976998564, "final_min_margin": 0.25957489013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 38.5, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "used_ocr": false, "answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "answer_second": null, "raw_answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_second": null, "mean_entropy_first": 0.7033220976998564, "normalized_entropy_first": -0.6055374254216883, "min_margin_first": 0.25957489013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1221, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1221, "total_latency_s": 1.221, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3446378707885742, 0.14021073281764984, 2.0802814960479736, 0.2757672667503357, 0.010136852972209454, 0.8546233177185059, 1.1475260257720947, 0.5300256013870239, 1.9481968879699707, 0.0012884365860372782, 1.9726252555847168, 0.03219215199351311, 2.3003177642822266, 0.04386093467473984, 0.5495318174362183, 0.11976876854896545, 0.5345867872238159, 0.03240580856800079, 0.14786092936992645, 0.0005972475046291947], "entropies_second": null, "final_normalized_entropy": -0.6055374254216883, "sequence_confidence_first": 0.7391203771543623, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391203771543623, "token_confidences_first": [0.5354316830635071, 0.9828269481658936, 0.28088316321372986, 0.9398400187492371, 0.9987414479255676, 0.5574624538421631, 0.5755826830863953, 0.8714181780815125, 0.4598904550075531, 0.9998927116394043, 0.5175709128379822, 0.9954342246055603, 0.40972059965133667, 0.9936898946762085, 0.8464242815971375, 0.9750951528549194, 0.7908768653869629, 0.9956489205360413, 0.9736035466194153, 0.9999504089355469], "token_confidences_second": null, "final_mean_entropy": 0.7033220976998564, "final_min_margin": 0.25957489013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 38.5, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["6."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "used_ocr": false, "answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "answer_second": null, "raw_answer": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_first": "The image you've provided is a humorous flowchart titled \"How to order a beer", "raw_answer_second": null, "mean_entropy_first": 0.7033220976998564, "normalized_entropy_first": -0.5642122237792654, "min_margin_first": 0.25957489013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1221, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1222, "total_latency_s": 1.222, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.3446378707885742, 0.14021073281764984, 2.0802814960479736, 0.2757672667503357, 0.010136852972209454, 0.8546233177185059, 1.1475260257720947, 0.5300256013870239, 1.9481968879699707, 0.0012884365860372782, 1.9726252555847168, 0.03219215199351311, 2.3003177642822266, 0.04386093467473984, 0.5495318174362183, 0.11976876854896545, 0.5345867872238159, 0.03240580856800079, 0.14786092936992645, 0.0005972475046291947], "entropies_second": null, "final_normalized_entropy": -0.5642122237792654, "sequence_confidence_first": 0.7391203771543623, "sequence_confidence_second": null, "sequence_confidence_final": 0.7391203771543623, "token_confidences_first": [0.5354316830635071, 0.9828269481658936, 0.28088316321372986, 0.9398400187492371, 0.9987414479255676, 0.5574624538421631, 0.5755826830863953, 0.8714181780815125, 0.4598904550075531, 0.9998927116394043, 0.5175709128379822, 0.9954342246055603, 0.40972059965133667, 0.9936898946762085, 0.8464242815971375, 0.9750951528549194, 0.7908768653869629, 0.9956489205360413, 0.9736035466194153, 0.9999504089355469], "token_confidences_second": null, "final_mean_entropy": 0.7033220976998564, "final_min_margin": 0.25957489013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 38.5, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Ismail Enver Pasha."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 2.8395343500071304, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1358, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1359, "total_latency_s": 1.359, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 2.8395343500071304, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.368421052631579, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["17 million."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 2.0043519252498996, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1354, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 2.0043519252498996, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.181818181818182, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Japan."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 1.6060574871074638, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1353, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1354, "total_latency_s": 1.354, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 1.6060574871074638, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.5, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["The soviet union."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 1.3584761589774712, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1360, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1361, "total_latency_s": 1.361, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 1.3584761589774712, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.9411764705882355, "wer": 5.666666666666667, "precision": 0.05555555555555555, "recall": 0.3333333333333333, "f1": 0.09523809523809525, "rouge_l": 0.09523809523809525, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1963 - 1981."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 1.1841244723097766, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1356, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1356, "total_latency_s": 1.356, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 1.1841244723097766, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.916666666666667, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["33."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 1.0520229688688967, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1353, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 1.0520229688688967, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 32.333333333333336, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Red."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 0.947006441049754, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1355, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1356, "total_latency_s": 1.356, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 0.947006441049754, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.5, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_first": "This image appears to be a graphic or poster that compares the number of people killed by various", "raw_answer_second": null, "mean_entropy_first": 1.309528326598229, "normalized_entropy_first": 0.8606454461710964, "min_margin_first": 0.1710662841796875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1355, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1356, "total_latency_s": 1.356, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7870346307754517, 0.7116189002990723, 1.5245387554168701, 0.0007150570163503289, 0.15020106732845306, 0.7531655430793762, 3.5863571166992188, 0.5429486632347107, 2.296196937561035, 2.1782360076904297, 2.5751237869262695, 3.4444456100463867, 0.02392098866403103, 0.855492353439331, 1.8608282804489136, 0.004456809721887112, 1.7594006061553955, 1.154604434967041, 0.6280784010887146, 1.3532025814056396], "entropies_second": null, "final_normalized_entropy": 0.8606454461710964, "sequence_confidence_first": 0.588693572290328, "sequence_confidence_second": null, "sequence_confidence_final": 0.588693572290328, "token_confidences_first": [0.5359008312225342, 0.8095101714134216, 0.4476620852947235, 0.9999401569366455, 0.979903519153595, 0.6396353840827942, 0.2087349146604538, 0.8174806833267212, 0.445204496383667, 0.3059965670108795, 0.40516284108161926, 0.15849165618419647, 0.9964003562927246, 0.8488017916679382, 0.6502853035926819, 0.9996030926704407, 0.5661526322364807, 0.7770384550094604, 0.883750855922699, 0.6435417532920837], "token_confidences_second": null, "final_mean_entropy": 1.309528326598229, "final_min_margin": 0.1710662841796875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 48.5, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["6,424,739 Acres."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "used_ocr": false, "answer_first": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "answer_second": null, "raw_answer": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "raw_answer_first": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "raw_answer_second": null, "mean_entropy_first": 0.9943682454235386, "normalized_entropy_first": -0.3402243563074322, "min_margin_first": 0.10054969787597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1364, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1365, "total_latency_s": 1.365, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7986929416656494, 0.030219176784157753, 1.8736101388931274, 0.2667310833930969, 0.003213337855413556, 0.718991756439209, 0.9842826724052429, 0.00018677313346415758, 0.0992719903588295, 0.849435031414032, 1.1200532913208008, 0.581985354423523, 0.6183739900588989, 0.024132970720529556, 1.8815652132034302, 2.4253554344177246, 2.3234310150146484, 1.870429515838623, 2.485846996307373, 0.931556224822998], "entropies_second": null, "final_normalized_entropy": -0.3402243563074322, "sequence_confidence_first": 0.6151166415826376, "sequence_confidence_second": null, "sequence_confidence_final": 0.6151166415826376, "token_confidences_first": [0.6006045937538147, 0.9968409538269043, 0.3603922426700592, 0.9409216046333313, 0.9996572732925415, 0.6873653531074524, 0.6163033843040466, 0.9999876022338867, 0.9866166114807129, 0.5116577744483948, 0.5152035355567932, 0.7409963011741638, 0.8818603754043579, 0.9972526431083679, 0.49263277649879456, 0.3191712498664856, 0.3028874695301056, 0.3220447599887848, 0.5076958537101746, 0.5300319790840149], "token_confidences_second": null, "final_mean_entropy": 0.9943682454235386, "final_min_margin": 0.10054969787597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.4375, "wer": 7.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Mendocino Complex Fire."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "used_ocr": false, "answer_first": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "answer_second": null, "raw_answer": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "raw_answer_first": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "raw_answer_second": null, "mean_entropy_first": 0.9943682454235386, "normalized_entropy_first": -0.320913181337871, "min_margin_first": 0.10054969787597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1363, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1365, "total_latency_s": 1.365, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7986929416656494, 0.030219176784157753, 1.8736101388931274, 0.2667310833930969, 0.003213337855413556, 0.718991756439209, 0.9842826724052429, 0.00018677313346415758, 0.0992719903588295, 0.849435031414032, 1.1200532913208008, 0.581985354423523, 0.6183739900588989, 0.024132970720529556, 1.8815652132034302, 2.4253554344177246, 2.3234310150146484, 1.870429515838623, 2.485846996307373, 0.931556224822998], "entropies_second": null, "final_normalized_entropy": -0.320913181337871, "sequence_confidence_first": 0.6151166415826376, "sequence_confidence_second": null, "sequence_confidence_final": 0.6151166415826376, "token_confidences_first": [0.6006045937538147, 0.9968409538269043, 0.3603922426700592, 0.9409216046333313, 0.9996572732925415, 0.6873653531074524, 0.6163033843040466, 0.9999876022338867, 0.9866166114807129, 0.5116577744483948, 0.5152035355567932, 0.7409963011741638, 0.8818603754043579, 0.9972526431083679, 0.49263277649879456, 0.3191712498664856, 0.3028874695301056, 0.3220447599887848, 0.5076958537101746, 0.5300319790840149], "token_confidences_second": null, "final_mean_entropy": 0.9943682454235386, "final_min_margin": 0.10054969787597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.4347826086956523, "wer": 5.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Australia."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "used_ocr": false, "answer_first": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "answer_second": null, "raw_answer": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "raw_answer_first": "The image you've provided appears to be an informational graphic or poster about the \"Austral", "raw_answer_second": null, "mean_entropy_first": 0.9943682454235386, "normalized_entropy_first": -0.3028893133714574, "min_margin_first": 0.10054969787597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1367, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1368, "total_latency_s": 1.368, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7986929416656494, 0.030219176784157753, 1.8736101388931274, 0.2667310833930969, 0.003213337855413556, 0.718991756439209, 0.9842826724052429, 0.00018677313346415758, 0.0992719903588295, 0.849435031414032, 1.1200532913208008, 0.581985354423523, 0.6183739900588989, 0.024132970720529556, 1.8815652132034302, 2.4253554344177246, 2.3234310150146484, 1.870429515838623, 2.485846996307373, 0.931556224822998], "entropies_second": null, "final_normalized_entropy": -0.3028893133714574, "sequence_confidence_first": 0.6151166415826376, "sequence_confidence_second": null, "sequence_confidence_final": 0.6151166415826376, "token_confidences_first": [0.6006045937538147, 0.9968409538269043, 0.3603922426700592, 0.9409216046333313, 0.9996572732925415, 0.6873653531074524, 0.6163033843040466, 0.9999876022338867, 0.9866166114807129, 0.5116577744483948, 0.5152035355567932, 0.7409963011741638, 0.8818603754043579, 0.9972526431083679, 0.49263277649879456, 0.3191712498664856, 0.3028874695301056, 0.3220447599887848, 0.5076958537101746, 0.5300319790840149], "token_confidences_second": null, "final_mean_entropy": 0.9943682454235386, "final_min_margin": 0.10054969787597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.4, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["3.6%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "used_ocr": false, "answer_first": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "answer_second": null, "raw_answer": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "raw_answer_first": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "raw_answer_second": null, "mean_entropy_first": 0.5341260333301762, "normalized_entropy_first": -2.1858551368605297, "min_margin_first": 0.10313034057617188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1428, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1429, "total_latency_s": 1.429, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8383822441101074, 0.208962082862854, 2.035022020339966, 0.819374680519104, 0.7001769542694092, 0.01089141983538866, 2.3992269039154053, 0.0076852235943078995, 0.738593339920044, 0.006850004196166992, 0.001315730158239603, 0.0010064846137538552, 0.08950626105070114, 0.04939185082912445, 0.010080335661768913, 0.0002514189691282809, 0.00011879500380018726, 0.0008078358368948102, 1.0190603733062744, 1.745816707611084], "entropies_second": null, "final_normalized_entropy": -2.1858551368605297, "sequence_confidence_first": 0.7672581468341347, "sequence_confidence_second": null, "sequence_confidence_final": 0.7672581468341347, "token_confidences_first": [0.6109627485275269, 0.9613795876502991, 0.3566792905330658, 0.5632951259613037, 0.7463955283164978, 0.9986943602561951, 0.2872663736343384, 0.9992239475250244, 0.6752339005470276, 0.999369204044342, 0.9998801946640015, 0.9999139308929443, 0.9850362539291382, 0.9921103119850159, 0.9987615346908569, 0.9999805688858032, 0.9999923706054688, 0.9999352693557739, 0.6043336391448975, 0.49745720624923706], "token_confidences_second": null, "final_mean_entropy": 0.5341260333301762, "final_min_margin": 0.10313034057617188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.6, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Diabetes."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "used_ocr": false, "answer_first": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "answer_second": null, "raw_answer": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "raw_answer_first": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "raw_answer_second": null, "mean_entropy_first": 0.5341260333301762, "normalized_entropy_first": -1.7058284312514722, "min_margin_first": 0.10313034057617188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1427, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1428, "total_latency_s": 1.428, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8383822441101074, 0.208962082862854, 2.035022020339966, 0.819374680519104, 0.7001769542694092, 0.01089141983538866, 2.3992269039154053, 0.0076852235943078995, 0.738593339920044, 0.006850004196166992, 0.001315730158239603, 0.0010064846137538552, 0.08950626105070114, 0.04939185082912445, 0.010080335661768913, 0.0002514189691282809, 0.00011879500380018726, 0.0008078358368948102, 1.0190603733062744, 1.745816707611084], "entropies_second": null, "final_normalized_entropy": -1.7058284312514722, "sequence_confidence_first": 0.7672581468341347, "sequence_confidence_second": null, "sequence_confidence_final": 0.7672581468341347, "token_confidences_first": [0.6109627485275269, 0.9613795876502991, 0.3566792905330658, 0.5632951259613037, 0.7463955283164978, 0.9986943602561951, 0.2872663736343384, 0.9992239475250244, 0.6752339005470276, 0.999369204044342, 0.9998801946640015, 0.9999139308929443, 0.9850362539291382, 0.9921103119850159, 0.9987615346908569, 0.9999805688858032, 0.9999923706054688, 0.9999352693557739, 0.6043336391448975, 0.49745720624923706], "token_confidences_second": null, "final_mean_entropy": 0.5341260333301762, "final_min_margin": 0.10313034057617188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 6.111111111111111, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "", "image_id": null, "answers": ["Diabetes."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "used_ocr": false, "answer_first": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "answer_second": null, "raw_answer": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "raw_answer_first": "The image is an infographic titled \"DIABETES IN AUSTRALIA\" that", "raw_answer_second": null, "mean_entropy_first": 0.5341260333301762, "normalized_entropy_first": -1.4242820286700402, "min_margin_first": 0.10313034057617188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1429, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1429, "total_latency_s": 1.429, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8383822441101074, 0.208962082862854, 2.035022020339966, 0.819374680519104, 0.7001769542694092, 0.01089141983538866, 2.3992269039154053, 0.0076852235943078995, 0.738593339920044, 0.006850004196166992, 0.001315730158239603, 0.0010064846137538552, 0.08950626105070114, 0.04939185082912445, 0.010080335661768913, 0.0002514189691282809, 0.00011879500380018726, 0.0008078358368948102, 1.0190603733062744, 1.745816707611084], "entropies_second": null, "final_normalized_entropy": -1.4242820286700402, "sequence_confidence_first": 0.7672581468341347, "sequence_confidence_second": null, "sequence_confidence_final": 0.7672581468341347, "token_confidences_first": [0.6109627485275269, 0.9613795876502991, 0.3566792905330658, 0.5632951259613037, 0.7463955283164978, 0.9986943602561951, 0.2872663736343384, 0.9992239475250244, 0.6752339005470276, 0.999369204044342, 0.9998801946640015, 0.9999139308929443, 0.9850362539291382, 0.9921103119850159, 0.9987615346908569, 0.9999805688858032, 0.9999923706054688, 0.9999352693557739, 0.6043336391448975, 0.49745720624923706], "token_confidences_second": null, "final_mean_entropy": 0.5341260333301762, "final_min_margin": 0.10313034057617188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 6.111111111111111, "wer": 10.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "", "image_id": null, "answers": ["1 in 50."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "used_ocr": false, "answer_first": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "answer_second": null, "raw_answer": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "raw_answer_first": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "raw_answer_second": null, "mean_entropy_first": 0.609315374691505, "normalized_entropy_first": -0.9920359043218328, "min_margin_first": 0.029130935668945312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1402, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1403, "total_latency_s": 1.403, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.741968035697937, 0.19259041547775269, 2.033689498901367, 0.8073652982711792, 0.45959392189979553, 0.009941727854311466, 2.5574426651000977, 0.008046556264162064, 0.15461671352386475, 0.06917666643857956, 0.0009726461721584201, 0.48216575384140015, 0.022155538201332092, 0.007116143591701984, 0.0003815102973021567, 0.00012297515058889985, 0.001076691085472703, 1.1872038841247559, 1.9155765771865845, 1.5351042747497559], "entropies_second": null, "final_normalized_entropy": -0.9920359043218328, "sequence_confidence_first": 0.7434216387935273, "sequence_confidence_second": null, "sequence_confidence_final": 0.7434216387935273, "token_confidences_first": [0.5932308435440063, 0.9647256731987, 0.3671954274177551, 0.5683359503746033, 0.8653738498687744, 0.9988101720809937, 0.24219651520252228, 0.9991869330406189, 0.97098708152771, 0.9902280569076538, 0.9999089241027832, 0.8306987881660461, 0.9970591068267822, 0.9991814494132996, 0.9999696016311646, 0.9999920129776001, 0.9999110698699951, 0.3557438254356384, 0.5625805854797363, 0.6684791445732117], "token_confidences_second": null, "final_mean_entropy": 0.609315374691505, "final_min_margin": 0.029130935668945312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.875, "wer": 3.6666666666666665, "precision": 0.08333333333333333, "recall": 0.3333333333333333, "f1": 0.13333333333333333, "rouge_l": 0.13333333333333333, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4 years."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "used_ocr": false, "answer_first": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "answer_second": null, "raw_answer": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "raw_answer_first": "The image is an infographic titled \"ICE USERS IN AUSTRALIA,\" which provides", "raw_answer_second": null, "mean_entropy_first": 0.609315374691505, "normalized_entropy_first": -0.8979780267951728, "min_margin_first": 0.029130935668945312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1402, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1403, "total_latency_s": 1.403, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.741968035697937, 0.19259041547775269, 2.033689498901367, 0.8073652982711792, 0.45959392189979553, 0.009941727854311466, 2.5574426651000977, 0.008046556264162064, 0.15461671352386475, 0.06917666643857956, 0.0009726461721584201, 0.48216575384140015, 0.022155538201332092, 0.007116143591701984, 0.0003815102973021567, 0.00012297515058889985, 0.001076691085472703, 1.1872038841247559, 1.9155765771865845, 1.5351042747497559], "entropies_second": null, "final_normalized_entropy": -0.8979780267951728, "sequence_confidence_first": 0.7434216387935273, "sequence_confidence_second": null, "sequence_confidence_final": 0.7434216387935273, "token_confidences_first": [0.5932308435440063, 0.9647256731987, 0.3671954274177551, 0.5683359503746033, 0.8653738498687744, 0.9988101720809937, 0.24219651520252228, 0.9991869330406189, 0.97098708152771, 0.9902280569076538, 0.9999089241027832, 0.8306987881660461, 0.9970591068267822, 0.9991814494132996, 0.9999696016311646, 0.9999920129776001, 0.9999110698699951, 0.3557438254356384, 0.5625805854797363, 0.6684791445732117], "token_confidences_second": null, "final_mean_entropy": 0.609315374691505, "final_min_margin": 0.029130935668945312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.75, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["$640 million."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "raw_answer_second": null, "mean_entropy_first": 0.8437200722362832, "normalized_entropy_first": -0.05656109483826918, "min_margin_first": 0.058322906494140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1352, "total_latency_s": 1.352, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7915828227996826, 0.049264661967754364, 1.6538864374160767, 0.2663106322288513, 0.0015502636088058352, 0.5676830410957336, 0.8433781266212463, 1.9621245883172378e-05, 0.013295924291014671, 0.8313443660736084, 2.7019755840301514, 0.547969400882721, 1.6115317344665527, 2.2294301986694336, 2.086333990097046, 8.426177373621613e-05, 0.8646450638771057, 0.7529487013816833, 1.06050705909729, 0.0006595531012862921], "entropies_second": null, "final_normalized_entropy": -0.05656109483826918, "sequence_confidence_first": 0.6771206463036854, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771206463036854, "token_confidences_first": [0.5073510408401489, 0.9938806295394897, 0.4009397029876709, 0.9400455951690674, 0.9998537302017212, 0.7864881157875061, 0.5054179430007935, 0.999998927116394, 0.9984353184700012, 0.589759886264801, 0.3543691337108612, 0.7846031188964844, 0.6346941590309143, 0.3579100966453552, 0.42078372836112976, 0.9999948740005493, 0.8183419108390808, 0.8064287304878235, 0.5262585282325745, 0.9999432563781738], "token_confidences_second": null, "final_mean_entropy": 0.8437200722362832, "final_min_margin": 0.058322906494140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.6923076923076925, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["$53.7 b."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "raw_answer_first": "The image you've provided appears to be a graphic or poster related to the Australian Tourism", "raw_answer_second": null, "mean_entropy_first": 0.8437200722362832, "normalized_entropy_first": -0.05364998494131695, "min_margin_first": 0.058322906494140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1345, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1346, "total_latency_s": 1.346, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7915828227996826, 0.049264661967754364, 1.6538864374160767, 0.2663106322288513, 0.0015502636088058352, 0.5676830410957336, 0.8433781266212463, 1.9621245883172378e-05, 0.013295924291014671, 0.8313443660736084, 2.7019755840301514, 0.547969400882721, 1.6115317344665527, 2.2294301986694336, 2.086333990097046, 8.426177373621613e-05, 0.8646450638771057, 0.7529487013816833, 1.06050705909729, 0.0006595531012862921], "entropies_second": null, "final_normalized_entropy": -0.05364998494131695, "sequence_confidence_first": 0.6771206463036854, "sequence_confidence_second": null, "sequence_confidence_final": 0.6771206463036854, "token_confidences_first": [0.5073510408401489, 0.9938806295394897, 0.4009397029876709, 0.9400455951690674, 0.9998537302017212, 0.7864881157875061, 0.5054179430007935, 0.999998927116394, 0.9984353184700012, 0.589759886264801, 0.3543691337108612, 0.7846031188964844, 0.6346941590309143, 0.3579100966453552, 0.42078372836112976, 0.9999948740005493, 0.8183419108390808, 0.8064287304878235, 0.5262585282325745, 0.9999432563781738], "token_confidences_second": null, "final_mean_entropy": 0.8437200722362832, "final_min_margin": 0.058322906494140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.375, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Victoria."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to climate data for the 20", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_second": null, "mean_entropy_first": 1.0679002497105103, "normalized_entropy_first": 0.7595941427392537, "min_margin_first": 0.016111373901367188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1431, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1433, "total_latency_s": 1.433, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.764396071434021, 0.9347745180130005, 1.2819499969482422, 0.0001493269664933905, 0.04446711391210556, 0.5790231227874756, 2.7457268238067627, 0.717508852481842, 2.516850471496582, 2.175997734069824, 0.013964787125587463, 2.214609384536743, 9.622715879231691e-05, 1.7116913795471191, 1.653532862663269, 1.4201369285583496, 1.0639526844024658, 1.5017850399017334, 0.013945848681032658, 0.0034458197187632322], "entropies_second": null, "final_normalized_entropy": 0.7595941427392537, "sequence_confidence_first": 0.590132408577415, "sequence_confidence_second": null, "sequence_confidence_final": 0.590132408577415, "token_confidences_first": [0.5439087152481079, 0.5823920369148254, 0.45463767647743225, 0.9999892711639404, 0.9947918057441711, 0.7771449685096741, 0.3621152937412262, 0.5720065236091614, 0.30875301361083984, 0.32626456022262573, 0.998304009437561, 0.39361751079559326, 0.999994158744812, 0.34027883410453796, 0.5068128108978271, 0.6072834730148315, 0.7009140849113464, 0.39248692989349365, 0.9981297850608826, 0.9996275901794434], "token_confidences_second": null, "final_mean_entropy": 1.0679002497105103, "final_min_margin": 0.016111373901367188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.666666666666666, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["New south wales."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to climate data for the 20", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_second": null, "mean_entropy_first": 1.0679002497105103, "normalized_entropy_first": 0.7006836012244563, "min_margin_first": 0.016111373901367188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1432, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1433, "total_latency_s": 1.433, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.764396071434021, 0.9347745180130005, 1.2819499969482422, 0.0001493269664933905, 0.04446711391210556, 0.5790231227874756, 2.7457268238067627, 0.717508852481842, 2.516850471496582, 2.175997734069824, 0.013964787125587463, 2.214609384536743, 9.622715879231691e-05, 1.7116913795471191, 1.653532862663269, 1.4201369285583496, 1.0639526844024658, 1.5017850399017334, 0.013945848681032658, 0.0034458197187632322], "entropies_second": null, "final_normalized_entropy": 0.7006836012244563, "sequence_confidence_first": 0.590132408577415, "sequence_confidence_second": null, "sequence_confidence_final": 0.590132408577415, "token_confidences_first": [0.5439087152481079, 0.5823920369148254, 0.45463767647743225, 0.9999892711639404, 0.9947918057441711, 0.7771449685096741, 0.3621152937412262, 0.5720065236091614, 0.30875301361083984, 0.32626456022262573, 0.998304009437561, 0.39361751079559326, 0.999994158744812, 0.34027883410453796, 0.5068128108978271, 0.6072834730148315, 0.7009140849113464, 0.39248692989349365, 0.9981297850608826, 0.9996275901794434], "token_confidences_second": null, "final_mean_entropy": 1.0679002497105103, "final_min_margin": 0.016111373901367188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.75, "wer": 5.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Tasmania."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to climate data for the 20", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_second": null, "mean_entropy_first": 1.0679002497105103, "normalized_entropy_first": 0.6489864719909635, "min_margin_first": 0.016111373901367188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1433, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1434, "total_latency_s": 1.434, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.764396071434021, 0.9347745180130005, 1.2819499969482422, 0.0001493269664933905, 0.04446711391210556, 0.5790231227874756, 2.7457268238067627, 0.717508852481842, 2.516850471496582, 2.175997734069824, 0.013964787125587463, 2.214609384536743, 9.622715879231691e-05, 1.7116913795471191, 1.653532862663269, 1.4201369285583496, 1.0639526844024658, 1.5017850399017334, 0.013945848681032658, 0.0034458197187632322], "entropies_second": null, "final_normalized_entropy": 0.6489864719909635, "sequence_confidence_first": 0.590132408577415, "sequence_confidence_second": null, "sequence_confidence_final": 0.590132408577415, "token_confidences_first": [0.5439087152481079, 0.5823920369148254, 0.45463767647743225, 0.9999892711639404, 0.9947918057441711, 0.7771449685096741, 0.3621152937412262, 0.5720065236091614, 0.30875301361083984, 0.32626456022262573, 0.998304009437561, 0.39361751079559326, 0.999994158744812, 0.34027883410453796, 0.5068128108978271, 0.6072834730148315, 0.7009140849113464, 0.39248692989349365, 0.9981297850608826, 0.9996275901794434], "token_confidences_second": null, "final_mean_entropy": 1.0679002497105103, "final_min_margin": 0.016111373901367188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.555555555555555, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Orange."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to climate data for the 20", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_second": null, "mean_entropy_first": 1.0679002497105103, "normalized_entropy_first": 0.6031125677592261, "min_margin_first": 0.016111373901367188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1432, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1433, "total_latency_s": 1.433, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.764396071434021, 0.9347745180130005, 1.2819499969482422, 0.0001493269664933905, 0.04446711391210556, 0.5790231227874756, 2.7457268238067627, 0.717508852481842, 2.516850471496582, 2.175997734069824, 0.013964787125587463, 2.214609384536743, 9.622715879231691e-05, 1.7116913795471191, 1.653532862663269, 1.4201369285583496, 1.0639526844024658, 1.5017850399017334, 0.013945848681032658, 0.0034458197187632322], "entropies_second": null, "final_normalized_entropy": 0.6031125677592261, "sequence_confidence_first": 0.590132408577415, "sequence_confidence_second": null, "sequence_confidence_final": 0.590132408577415, "token_confidences_first": [0.5439087152481079, 0.5823920369148254, 0.45463767647743225, 0.9999892711639404, 0.9947918057441711, 0.7771449685096741, 0.3621152937412262, 0.5720065236091614, 0.30875301361083984, 0.32626456022262573, 0.998304009437561, 0.39361751079559326, 0.999994158744812, 0.34027883410453796, 0.5068128108978271, 0.6072834730148315, 0.7009140849113464, 0.39248692989349365, 0.9981297850608826, 0.9996275901794434], "token_confidences_second": null, "final_mean_entropy": 1.0679002497105103, "final_min_margin": 0.016111373901367188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.142857142857142, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Western australia."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or infographic related to climate data for the 20", "used_ocr": false, "answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "answer_second": null, "raw_answer": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_first": "This image appears to be a graphic or infographic related to climate data for the 20", "raw_answer_second": null, "mean_entropy_first": 1.0679002497105103, "normalized_entropy_first": 0.5620323045426946, "min_margin_first": 0.016111373901367188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1434, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1435, "total_latency_s": 1.435, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.764396071434021, 0.9347745180130005, 1.2819499969482422, 0.0001493269664933905, 0.04446711391210556, 0.5790231227874756, 2.7457268238067627, 0.717508852481842, 2.516850471496582, 2.175997734069824, 0.013964787125587463, 2.214609384536743, 9.622715879231691e-05, 1.7116913795471191, 1.653532862663269, 1.4201369285583496, 1.0639526844024658, 1.5017850399017334, 0.013945848681032658, 0.0034458197187632322], "entropies_second": null, "final_normalized_entropy": 0.5620323045426946, "sequence_confidence_first": 0.590132408577415, "sequence_confidence_second": null, "sequence_confidence_final": 0.590132408577415, "token_confidences_first": [0.5439087152481079, 0.5823920369148254, 0.45463767647743225, 0.9999892711639404, 0.9947918057441711, 0.7771449685096741, 0.3621152937412262, 0.5720065236091614, 0.30875301361083984, 0.32626456022262573, 0.998304009437561, 0.39361751079559326, 0.999994158744812, 0.34027883410453796, 0.5068128108978271, 0.6072834730148315, 0.7009140849113464, 0.39248692989349365, 0.9981297850608826, 0.9996275901794434], "token_confidences_second": null, "final_mean_entropy": 1.0679002497105103, "final_min_margin": 0.016111373901367188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.0, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1830."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "used_ocr": false, "answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "answer_second": null, "raw_answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_second": null, "mean_entropy_first": 1.2973175018618348, "normalized_entropy_first": 1.4957159886087183, "min_margin_first": 0.021648406982421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1364, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1365, "total_latency_s": 1.365, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7507085800170898, 0.9574301838874817, 1.5772286653518677, 0.0002726936945691705, 0.11883668601512909, 0.7905046939849854, 2.6903724670410156, 1.5781993865966797, 0.6218488812446594, 1.7603906393051147, 2.0056443214416504, 1.9078326225280762, 1.8307499885559082, 3.371952772140503, 1.4291666746139526, 1.4203531742095947, 0.017198504880070686, 1.0813674926757812, 0.0032992460764944553, 2.032992362976074], "entropies_second": null, "final_normalized_entropy": 1.4957159886087183, "sequence_confidence_first": 0.5717305326308751, "sequence_confidence_second": null, "sequence_confidence_final": 0.5717305326308751, "token_confidences_first": [0.5347372889518738, 0.6555132865905762, 0.37349122762680054, 0.9999798536300659, 0.9844143390655518, 0.5346194505691528, 0.2973889410495758, 0.6154088377952576, 0.8826025128364563, 0.5905939936637878, 0.3520514965057373, 0.37851008772850037, 0.470342218875885, 0.2099887579679489, 0.7059948444366455, 0.718721330165863, 0.9976624250411987, 0.7169086933135986, 0.9997585415840149, 0.4436805248260498], "token_confidences_second": null, "final_mean_entropy": 1.2973175018618348, "final_min_margin": 0.021648406982421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 22.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["USA."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "used_ocr": false, "answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "answer_second": null, "raw_answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_second": null, "mean_entropy_first": 1.2973175018618348, "normalized_entropy_first": 1.282714368252157, "min_margin_first": 0.021648406982421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1360, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1361, "total_latency_s": 1.361, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7507085800170898, 0.9574301838874817, 1.5772286653518677, 0.0002726936945691705, 0.11883668601512909, 0.7905046939849854, 2.6903724670410156, 1.5781993865966797, 0.6218488812446594, 1.7603906393051147, 2.0056443214416504, 1.9078326225280762, 1.8307499885559082, 3.371952772140503, 1.4291666746139526, 1.4203531742095947, 0.017198504880070686, 1.0813674926757812, 0.0032992460764944553, 2.032992362976074], "entropies_second": null, "final_normalized_entropy": 1.282714368252157, "sequence_confidence_first": 0.5717305326308751, "sequence_confidence_second": null, "sequence_confidence_final": 0.5717305326308751, "token_confidences_first": [0.5347372889518738, 0.6555132865905762, 0.37349122762680054, 0.9999798536300659, 0.9844143390655518, 0.5346194505691528, 0.2973889410495758, 0.6154088377952576, 0.8826025128364563, 0.5905939936637878, 0.3520514965057373, 0.37851008772850037, 0.470342218875885, 0.2099887579679489, 0.7059948444366455, 0.718721330165863, 0.9976624250411987, 0.7169086933135986, 0.9997585415840149, 0.4436805248260498], "token_confidences_second": null, "final_mean_entropy": 1.2973175018618348, "final_min_margin": 0.021648406982421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.75, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2012."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "used_ocr": false, "answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "answer_second": null, "raw_answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_second": null, "mean_entropy_first": 1.2973175018618348, "normalized_entropy_first": 1.1276513045004257, "min_margin_first": 0.021648406982421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1362, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1363, "total_latency_s": 1.363, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7507085800170898, 0.9574301838874817, 1.5772286653518677, 0.0002726936945691705, 0.11883668601512909, 0.7905046939849854, 2.6903724670410156, 1.5781993865966797, 0.6218488812446594, 1.7603906393051147, 2.0056443214416504, 1.9078326225280762, 1.8307499885559082, 3.371952772140503, 1.4291666746139526, 1.4203531742095947, 0.017198504880070686, 1.0813674926757812, 0.0032992460764944553, 2.032992362976074], "entropies_second": null, "final_normalized_entropy": 1.1276513045004257, "sequence_confidence_first": 0.5717305326308751, "sequence_confidence_second": null, "sequence_confidence_final": 0.5717305326308751, "token_confidences_first": [0.5347372889518738, 0.6555132865905762, 0.37349122762680054, 0.9999798536300659, 0.9844143390655518, 0.5346194505691528, 0.2973889410495758, 0.6154088377952576, 0.8826025128364563, 0.5905939936637878, 0.3520514965057373, 0.37851008772850037, 0.470342218875885, 0.2099887579679489, 0.7059948444366455, 0.718721330165863, 0.9976624250411987, 0.7169086933135986, 0.9997585415840149, 0.4436805248260498], "token_confidences_second": null, "final_mean_entropy": 1.2973175018618348, "final_min_margin": 0.021648406982421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 22.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["8 weeks of global births."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "used_ocr": false, "answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "answer_second": null, "raw_answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_second": null, "mean_entropy_first": 1.2973175018618348, "normalized_entropy_first": 1.0076352361966554, "min_margin_first": 0.021648406982421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1366, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1367, "total_latency_s": 1.367, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7507085800170898, 0.9574301838874817, 1.5772286653518677, 0.0002726936945691705, 0.11883668601512909, 0.7905046939849854, 2.6903724670410156, 1.5781993865966797, 0.6218488812446594, 1.7603906393051147, 2.0056443214416504, 1.9078326225280762, 1.8307499885559082, 3.371952772140503, 1.4291666746139526, 1.4203531742095947, 0.017198504880070686, 1.0813674926757812, 0.0032992460764944553, 2.032992362976074], "entropies_second": null, "final_normalized_entropy": 1.0076352361966554, "sequence_confidence_first": 0.5717305326308751, "sequence_confidence_second": null, "sequence_confidence_final": 0.5717305326308751, "token_confidences_first": [0.5347372889518738, 0.6555132865905762, 0.37349122762680054, 0.9999798536300659, 0.9844143390655518, 0.5346194505691528, 0.2973889410495758, 0.6154088377952576, 0.8826025128364563, 0.5905939936637878, 0.3520514965057373, 0.37851008772850037, 0.470342218875885, 0.2099887579679489, 0.7059948444366455, 0.718721330165863, 0.9976624250411987, 0.7169086933135986, 0.9997585415840149, 0.4436805248260498], "token_confidences_second": null, "final_mean_entropy": 1.2973175018618348, "final_min_margin": 0.021648406982421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.56, "wer": 3.2, "precision": 0.1111111111111111, "recall": 0.4, "f1": 0.1739130434782609, "rouge_l": 0.1739130434782609, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Green."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "used_ocr": false, "answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "answer_second": null, "raw_answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_second": null, "mean_entropy_first": 1.2973175018618348, "normalized_entropy_first": 0.9108059704568932, "min_margin_first": 0.021648406982421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1361, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1362, "total_latency_s": 1.362, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7507085800170898, 0.9574301838874817, 1.5772286653518677, 0.0002726936945691705, 0.11883668601512909, 0.7905046939849854, 2.6903724670410156, 1.5781993865966797, 0.6218488812446594, 1.7603906393051147, 2.0056443214416504, 1.9078326225280762, 1.8307499885559082, 3.371952772140503, 1.4291666746139526, 1.4203531742095947, 0.017198504880070686, 1.0813674926757812, 0.0032992460764944553, 2.032992362976074], "entropies_second": null, "final_normalized_entropy": 0.9108059704568932, "sequence_confidence_first": 0.5717305326308751, "sequence_confidence_second": null, "sequence_confidence_final": 0.5717305326308751, "token_confidences_first": [0.5347372889518738, 0.6555132865905762, 0.37349122762680054, 0.9999798536300659, 0.9844143390655518, 0.5346194505691528, 0.2973889410495758, 0.6154088377952576, 0.8826025128364563, 0.5905939936637878, 0.3520514965057373, 0.37851008772850037, 0.470342218875885, 0.2099887579679489, 0.7059948444366455, 0.718721330165863, 0.9976624250411987, 0.7169086933135986, 0.9997585415840149, 0.4436805248260498], "token_confidences_second": null, "final_mean_entropy": 1.2973175018618348, "final_min_margin": 0.021648406982421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.5, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4.5 mil."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "used_ocr": false, "answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "answer_second": null, "raw_answer": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_first": "This image appears to be a visual representation of global birth statistics, comparing the number of births in", "raw_answer_second": null, "mean_entropy_first": 1.2973175018618348, "normalized_entropy_first": 0.8303124824846841, "min_margin_first": 0.021648406982421875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1361, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1362, "total_latency_s": 1.362, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7507085800170898, 0.9574301838874817, 1.5772286653518677, 0.0002726936945691705, 0.11883668601512909, 0.7905046939849854, 2.6903724670410156, 1.5781993865966797, 0.6218488812446594, 1.7603906393051147, 2.0056443214416504, 1.9078326225280762, 1.8307499885559082, 3.371952772140503, 1.4291666746139526, 1.4203531742095947, 0.017198504880070686, 1.0813674926757812, 0.0032992460764944553, 2.032992362976074], "entropies_second": null, "final_normalized_entropy": 0.8303124824846841, "sequence_confidence_first": 0.5717305326308751, "sequence_confidence_second": null, "sequence_confidence_final": 0.5717305326308751, "token_confidences_first": [0.5347372889518738, 0.6555132865905762, 0.37349122762680054, 0.9999798536300659, 0.9844143390655518, 0.5346194505691528, 0.2973889410495758, 0.6154088377952576, 0.8826025128364563, 0.5905939936637878, 0.3520514965057373, 0.37851008772850037, 0.470342218875885, 0.2099887579679489, 0.7059948444366455, 0.718721330165863, 0.9976624250411987, 0.7169086933135986, 0.9997585415840149, 0.4436805248260498], "token_confidences_second": null, "final_mean_entropy": 1.2973175018618348, "final_min_margin": 0.021648406982421875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.25, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Lower."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a graph titled \"Google Search Volume for STEM in Australia.\" The graph is", "used_ocr": false, "answer_first": "The image shows a graph titled \"Google Search Volume for STEM in Australia.\" The graph is", "answer_second": null, "raw_answer": "The image shows a graph titled \"Google Search Volume for STEM in Australia.\" The graph is", "raw_answer_first": "The image shows a graph titled \"Google Search Volume for STEM in Australia.\" The graph is", "raw_answer_second": null, "mean_entropy_first": 0.733258848375408, "normalized_entropy_first": -1.5257638002644864, "min_margin_first": 0.2191143035888672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1505, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1507, "total_latency_s": 1.507, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7231245040893555, 0.22791489958763123, 1.654940128326416, 0.08514580875635147, 1.7291815280914307, 2.2780110836029053, 0.04643417149782181, 0.1893766224384308, 0.07137893885374069, 0.026380904018878937, 0.0487394742667675, 0.903878927230835, 0.0005719463806599379, 0.0006780136609449983, 0.20756134390830994, 0.04827707260847092, 1.2950735092163086, 1.3153138160705566, 1.3209903240203857, 2.492203950881958], "entropies_second": null, "final_normalized_entropy": -1.5257638002644864, "sequence_confidence_first": 0.70738401773403, "sequence_confidence_second": null, "sequence_confidence_final": 0.70738401773403, "token_confidences_first": [0.5856978297233582, 0.9543718695640564, 0.3475661873817444, 0.9866587519645691, 0.4057706892490387, 0.3727938234806061, 0.994130551815033, 0.9678758382797241, 0.9877845048904419, 0.9969815611839294, 0.9943549633026123, 0.7885973453521729, 0.99996018409729, 0.9999459981918335, 0.9618571400642395, 0.9935278296470642, 0.4650019407272339, 0.47082823514938354, 0.7097228765487671, 0.3076767325401306], "token_confidences_second": null, "final_mean_entropy": 0.733258848375408, "final_min_margin": 0.2191143035888672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 14.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Australian Energy Market Commission."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "used_ocr": false, "answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "answer_second": null, "raw_answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_second": null, "mean_entropy_first": 0.6436328077274084, "normalized_entropy_first": -1.6487452408676577, "min_margin_first": 0.07072067260742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1348, "total_latency_s": 1.348, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810302376747131, 0.016386933624744415, 1.8122882843017578, 6.686656706733629e-05, 0.06677054613828659, 0.6295479536056519, 2.2969796657562256, 0.7256821393966675, 0.03660371154546738, 0.027304403483867645, 2.264162540435791, 5.975149542791769e-05, 0.4952794909477234, 0.4835142493247986, 0.27332574129104614, 0.03002997860312462, 1.0313397645950317, 0.00023834779858589172, 0.586794912815094, 1.3152506351470947], "entropies_second": null, "final_normalized_entropy": -1.6487452408676577, "sequence_confidence_first": 0.7225287717331976, "sequence_confidence_second": null, "sequence_confidence_final": 0.7225287717331976, "token_confidences_first": [0.5945236682891846, 0.9983114004135132, 0.30400004982948303, 0.9999959468841553, 0.9909352660179138, 0.7527753710746765, 0.3469228446483612, 0.6615561246871948, 0.9941619038581848, 0.9970145225524902, 0.20526160299777985, 0.9999963045120239, 0.8950656652450562, 0.8915215730667114, 0.9275489449501038, 0.9963405132293701, 0.7391965389251709, 0.9999830722808838, 0.8709662556648254, 0.503889799118042], "token_confidences_second": null, "final_mean_entropy": 0.6436328077274084, "final_min_margin": 0.07072067260742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.1944444444444446, "wer": 3.75, "precision": 0.16666666666666666, "recall": 0.75, "f1": 0.27272727272727276, "rouge_l": 0.27272727272727276, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Forum of Energy Ministers."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "used_ocr": false, "answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "answer_second": null, "raw_answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_second": null, "mean_entropy_first": 0.6436328077274084, "normalized_entropy_first": -1.3869449958688995, "min_margin_first": 0.07072067260742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1349, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1349, "total_latency_s": 1.349, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810302376747131, 0.016386933624744415, 1.8122882843017578, 6.686656706733629e-05, 0.06677054613828659, 0.6295479536056519, 2.2969796657562256, 0.7256821393966675, 0.03660371154546738, 0.027304403483867645, 2.264162540435791, 5.975149542791769e-05, 0.4952794909477234, 0.4835142493247986, 0.27332574129104614, 0.03002997860312462, 1.0313397645950317, 0.00023834779858589172, 0.586794912815094, 1.3152506351470947], "entropies_second": null, "final_normalized_entropy": -1.3869449958688995, "sequence_confidence_first": 0.7225287717331976, "sequence_confidence_second": null, "sequence_confidence_final": 0.7225287717331976, "token_confidences_first": [0.5945236682891846, 0.9983114004135132, 0.30400004982948303, 0.9999959468841553, 0.9909352660179138, 0.7527753710746765, 0.3469228446483612, 0.6615561246871948, 0.9941619038581848, 0.9970145225524902, 0.20526160299777985, 0.9999963045120239, 0.8950656652450562, 0.8915215730667114, 0.9275489449501038, 0.9963405132293701, 0.7391965389251709, 0.9999830722808838, 0.8709662556648254, 0.503889799118042], "token_confidences_second": null, "final_mean_entropy": 0.6436328077274084, "final_min_margin": 0.07072067260742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.3846153846153846, "wer": 4.25, "precision": 0.05555555555555555, "recall": 0.25, "f1": 0.0909090909090909, "rouge_l": 0.0909090909090909, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Australian Energy Regulator."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "used_ocr": false, "answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "answer_second": null, "raw_answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_second": null, "mean_entropy_first": 0.6436328077274084, "normalized_entropy_first": -1.2049707212860068, "min_margin_first": 0.07072067260742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1353, "total_latency_s": 1.353, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810302376747131, 0.016386933624744415, 1.8122882843017578, 6.686656706733629e-05, 0.06677054613828659, 0.6295479536056519, 2.2969796657562256, 0.7256821393966675, 0.03660371154546738, 0.027304403483867645, 2.264162540435791, 5.975149542791769e-05, 0.4952794909477234, 0.4835142493247986, 0.27332574129104614, 0.03002997860312462, 1.0313397645950317, 0.00023834779858589172, 0.586794912815094, 1.3152506351470947], "entropies_second": null, "final_normalized_entropy": -1.2049707212860068, "sequence_confidence_first": 0.7225287717331976, "sequence_confidence_second": null, "sequence_confidence_final": 0.7225287717331976, "token_confidences_first": [0.5945236682891846, 0.9983114004135132, 0.30400004982948303, 0.9999959468841553, 0.9909352660179138, 0.7527753710746765, 0.3469228446483612, 0.6615561246871948, 0.9941619038581848, 0.9970145225524902, 0.20526160299777985, 0.9999963045120239, 0.8950656652450562, 0.8915215730667114, 0.9275489449501038, 0.9963405132293701, 0.7391965389251709, 0.9999830722808838, 0.8709662556648254, 0.503889799118042], "token_confidences_second": null, "final_mean_entropy": 0.6436328077274084, "final_min_margin": 0.07072067260742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.9642857142857144, "wer": 5.333333333333333, "precision": 0.1111111111111111, "recall": 0.6666666666666666, "f1": 0.1904761904761905, "rouge_l": 0.1904761904761905, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["3."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "used_ocr": false, "answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "answer_second": null, "raw_answer": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_first": "The image appears to be a slide from a presentation related to the Australian Energy Market Governance. It", "raw_answer_second": null, "mean_entropy_first": 0.6436328077274084, "normalized_entropy_first": -1.0682132076259379, "min_margin_first": 0.07072067260742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1352, "total_latency_s": 1.352, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7810302376747131, 0.016386933624744415, 1.8122882843017578, 6.686656706733629e-05, 0.06677054613828659, 0.6295479536056519, 2.2969796657562256, 0.7256821393966675, 0.03660371154546738, 0.027304403483867645, 2.264162540435791, 5.975149542791769e-05, 0.4952794909477234, 0.4835142493247986, 0.27332574129104614, 0.03002997860312462, 1.0313397645950317, 0.00023834779858589172, 0.586794912815094, 1.3152506351470947], "entropies_second": null, "final_normalized_entropy": -1.0682132076259379, "sequence_confidence_first": 0.7225287717331976, "sequence_confidence_second": null, "sequence_confidence_final": 0.7225287717331976, "token_confidences_first": [0.5945236682891846, 0.9983114004135132, 0.30400004982948303, 0.9999959468841553, 0.9909352660179138, 0.7527753710746765, 0.3469228446483612, 0.6615561246871948, 0.9941619038581848, 0.9970145225524902, 0.20526160299777985, 0.9999963045120239, 0.8950656652450562, 0.8915215730667114, 0.9275489449501038, 0.9963405132293701, 0.7391965389251709, 0.9999830722808838, 0.8709662556648254, 0.503889799118042], "token_confidences_second": null, "final_mean_entropy": 0.6436328077274084, "final_min_margin": 0.07072067260742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 52.5, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Perth."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.4940780362749234, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1351, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1352, "total_latency_s": 1.352, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.4940780362749234, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.333333333333334, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Air."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.46310516050459577, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1348, "total_latency_s": 1.348, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.46310516050459577, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.5, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Tasmania."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.43470339632219296, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.43470339632219296, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Canberra."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.4085537577082462, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1347, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.4085537577082462, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["60%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.38439332952951855, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1345, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1346, "total_latency_s": 1.346, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.38439332952951855, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 24.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["7."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.36200289319947654, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1350, "total_latency_s": 1.35, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.36200289319947654, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 48.5, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["64%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "used_ocr": false, "answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "answer_second": null, "raw_answer": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_first": "This image is a graphic representation of Australian domestic travel statistics for the year 2014", "raw_answer_second": null, "mean_entropy_first": 0.7800017442259559, "normalized_entropy_first": -0.34119774370131745, "min_margin_first": 0.1457672119140625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8557219505310059, 0.9381248950958252, 1.4178555011749268, 0.8184632658958435, 3.1662230491638184, 0.6954431533813477, 2.85178542137146, 0.6071689128875732, 1.7856132984161377, 0.9259771108627319, 0.24337121844291687, 0.012046819552779198, 0.7016907334327698, 0.5182685256004333, 0.057066403329372406, 0.002830177079886198, 9.613934525987133e-05, 0.0001521464146208018, 0.000745765573810786, 0.001390396966598928], "entropies_second": null, "final_normalized_entropy": -0.34119774370131745, "sequence_confidence_first": 0.6945369214406991, "sequence_confidence_second": null, "sequence_confidence_final": 0.6945369214406991, "token_confidences_first": [0.546128511428833, 0.5076428651809692, 0.5026459097862244, 0.768817126750946, 0.239163339138031, 0.5832067728042603, 0.3019724488258362, 0.8784087896347046, 0.39630332589149475, 0.6814324855804443, 0.954215407371521, 0.9988545179367065, 0.8367303013801575, 0.805392324924469, 0.9926490187644958, 0.99974125623703, 0.9999929666519165, 0.9999884366989136, 0.9999366998672485, 0.9998786449432373], "token_confidences_second": null, "final_mean_entropy": 0.7800017442259559, "final_min_margin": 0.1457672119140625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 24.25, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["94.6%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "used_ocr": false, "answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "answer_second": null, "raw_answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_second": null, "mean_entropy_first": 1.0260006701864768, "normalized_entropy_first": 0.8230150976745082, "min_margin_first": 0.030168533325195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1342, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1342, "total_latency_s": 1.342, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8371518850326538, 0.1718340367078781, 1.9822235107421875, 0.7082369923591614, 1.8663623332977295, 0.7145878672599792, 2.711310386657715, 0.7035772204399109, 2.8313703536987305, 1.5153939723968506, 1.8743524551391602, 0.8555509448051453, 1.5517305135726929, 1.4186594486236572, 0.12218444049358368, 0.0006851075449958444, 0.33513569831848145, 0.14349818229675293, 0.07738858461380005, 0.09877946972846985], "entropies_second": null, "final_normalized_entropy": 0.8230150976745082, "sequence_confidence_first": 0.5707545668719493, "sequence_confidence_second": null, "sequence_confidence_final": 0.5707545668719493, "token_confidences_first": [0.6110681891441345, 0.9731349349021912, 0.2605326473712921, 0.7713190317153931, 0.5669267773628235, 0.6221522092819214, 0.21078039705753326, 0.5260228514671326, 0.16474801301956177, 0.616822361946106, 0.3416540324687958, 0.5006809234619141, 0.45269882678985596, 0.43999138474464417, 0.9799617528915405, 0.9999430179595947, 0.8956270813941956, 0.9731132388114929, 0.9878726005554199, 0.9858552813529968], "token_confidences_second": null, "final_mean_entropy": 1.0260006701864768, "final_min_margin": 0.030168533325195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.166666666666668, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Two year olds."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "used_ocr": false, "answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "answer_second": null, "raw_answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_second": null, "mean_entropy_first": 1.0260006701864768, "normalized_entropy_first": 0.7556092034127445, "min_margin_first": 0.030168533325195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1342, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1343, "total_latency_s": 1.343, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8371518850326538, 0.1718340367078781, 1.9822235107421875, 0.7082369923591614, 1.8663623332977295, 0.7145878672599792, 2.711310386657715, 0.7035772204399109, 2.8313703536987305, 1.5153939723968506, 1.8743524551391602, 0.8555509448051453, 1.5517305135726929, 1.4186594486236572, 0.12218444049358368, 0.0006851075449958444, 0.33513569831848145, 0.14349818229675293, 0.07738858461380005, 0.09877946972846985], "entropies_second": null, "final_normalized_entropy": 0.7556092034127445, "sequence_confidence_first": 0.5707545668719493, "sequence_confidence_second": null, "sequence_confidence_final": 0.5707545668719493, "token_confidences_first": [0.6110681891441345, 0.9731349349021912, 0.2605326473712921, 0.7713190317153931, 0.5669267773628235, 0.6221522092819214, 0.21078039705753326, 0.5260228514671326, 0.16474801301956177, 0.616822361946106, 0.3416540324687958, 0.5006809234619141, 0.45269882678985596, 0.43999138474464417, 0.9799617528915405, 0.9999430179595947, 0.8956270813941956, 0.9731132388114929, 0.9878726005554199, 0.9858552813529968], "token_confidences_second": null, "final_mean_entropy": 1.0260006701864768, "final_min_margin": 0.030168533325195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.142857142857143, "wer": 5.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["90.6%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "used_ocr": false, "answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "answer_second": null, "raw_answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_second": null, "mean_entropy_first": 1.0260006701864768, "normalized_entropy_first": 0.6972067476925966, "min_margin_first": 0.030168533325195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1343, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1344, "total_latency_s": 1.344, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8371518850326538, 0.1718340367078781, 1.9822235107421875, 0.7082369923591614, 1.8663623332977295, 0.7145878672599792, 2.711310386657715, 0.7035772204399109, 2.8313703536987305, 1.5153939723968506, 1.8743524551391602, 0.8555509448051453, 1.5517305135726929, 1.4186594486236572, 0.12218444049358368, 0.0006851075449958444, 0.33513569831848145, 0.14349818229675293, 0.07738858461380005, 0.09877946972846985], "entropies_second": null, "final_normalized_entropy": 0.6972067476925966, "sequence_confidence_first": 0.5707545668719493, "sequence_confidence_second": null, "sequence_confidence_final": 0.5707545668719493, "token_confidences_first": [0.6110681891441345, 0.9731349349021912, 0.2605326473712921, 0.7713190317153931, 0.5669267773628235, 0.6221522092819214, 0.21078039705753326, 0.5260228514671326, 0.16474801301956177, 0.616822361946106, 0.3416540324687958, 0.5006809234619141, 0.45269882678985596, 0.43999138474464417, 0.9799617528915405, 0.9999430179595947, 0.8956270813941956, 0.9731132388114929, 0.9878726005554199, 0.9858552813529968], "token_confidences_second": null, "final_mean_entropy": 1.0260006701864768, "final_min_margin": 0.030168533325195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.166666666666668, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Five year olds."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "used_ocr": false, "answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "answer_second": null, "raw_answer": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_first": "The image is a graphic illustration that provides information on the national immunisation coverage rates for", "raw_answer_second": null, "mean_entropy_first": 1.0260006701864768, "normalized_entropy_first": 0.6459157817000923, "min_margin_first": 0.030168533325195312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1345, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1346, "total_latency_s": 1.346, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8371518850326538, 0.1718340367078781, 1.9822235107421875, 0.7082369923591614, 1.8663623332977295, 0.7145878672599792, 2.711310386657715, 0.7035772204399109, 2.8313703536987305, 1.5153939723968506, 1.8743524551391602, 0.8555509448051453, 1.5517305135726929, 1.4186594486236572, 0.12218444049358368, 0.0006851075449958444, 0.33513569831848145, 0.14349818229675293, 0.07738858461380005, 0.09877946972846985], "entropies_second": null, "final_normalized_entropy": 0.6459157817000923, "sequence_confidence_first": 0.5707545668719493, "sequence_confidence_second": null, "sequence_confidence_final": 0.5707545668719493, "token_confidences_first": [0.6110681891441345, 0.9731349349021912, 0.2605326473712921, 0.7713190317153931, 0.5669267773628235, 0.6221522092819214, 0.21078039705753326, 0.5260228514671326, 0.16474801301956177, 0.616822361946106, 0.3416540324687958, 0.5006809234619141, 0.45269882678985596, 0.43999138474464417, 0.9799617528915405, 0.9999430179595947, 0.8956270813941956, 0.9731132388114929, 0.9878726005554199, 0.9858552813529968], "token_confidences_second": null, "final_mean_entropy": 1.0260006701864768, "final_min_margin": 0.030168533325195312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.666666666666667, "wer": 5.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Nokia."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_second": null, "mean_entropy_first": 0.7021559777233051, "normalized_entropy_first": -1.075304101362432, "min_margin_first": 0.21455955505371094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1198, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1199, "total_latency_s": 1.199, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8503378629684448, 0.14283114671707153, 1.8364002704620361, 0.33744168281555176, 0.002046043984591961, 0.6153597235679626, 0.9635576009750366, 7.817341247573495e-05, 0.04376684129238129, 0.7480852603912354, 2.922473430633545, 0.7100098133087158, 0.06571037322282791, 0.04013017565011978, 2.514021873474121, 1.0137600898742676, 0.581512987613678, 0.3341432213783264, 0.2167014479637146, 0.10475153475999832], "entropies_second": null, "final_normalized_entropy": -1.075304101362432, "sequence_confidence_first": 0.7007287471408773, "sequence_confidence_second": null, "sequence_confidence_final": 0.7007287471408773, "token_confidences_first": [0.5555394291877747, 0.9785944819450378, 0.3491204082965851, 0.9164201617240906, 0.9998016953468323, 0.7513409852981567, 0.5813701152801514, 0.999995231628418, 0.9947068095207214, 0.5488496422767639, 0.25283268094062805, 0.7176525592803955, 0.9882412552833557, 0.9956894516944885, 0.20575681328773499, 0.7033035159111023, 0.8901147842407227, 0.9087565541267395, 0.9547992944717407, 0.9845190048217773], "token_confidences_second": null, "final_mean_entropy": 0.7021559777233051, "final_min_margin": 0.21455955505371094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.666666666666668, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Myspace."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_second": null, "mean_entropy_first": 0.7021559777233051, "normalized_entropy_first": -0.9658126188210868, "min_margin_first": 0.21455955505371094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1196, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1198, "total_latency_s": 1.198, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8503378629684448, 0.14283114671707153, 1.8364002704620361, 0.33744168281555176, 0.002046043984591961, 0.6153597235679626, 0.9635576009750366, 7.817341247573495e-05, 0.04376684129238129, 0.7480852603912354, 2.922473430633545, 0.7100098133087158, 0.06571037322282791, 0.04013017565011978, 2.514021873474121, 1.0137600898742676, 0.581512987613678, 0.3341432213783264, 0.2167014479637146, 0.10475153475999832], "entropies_second": null, "final_normalized_entropy": -0.9658126188210868, "sequence_confidence_first": 0.7007287471408773, "sequence_confidence_second": null, "sequence_confidence_final": 0.7007287471408773, "token_confidences_first": [0.5555394291877747, 0.9785944819450378, 0.3491204082965851, 0.9164201617240906, 0.9998016953468323, 0.7513409852981567, 0.5813701152801514, 0.999995231628418, 0.9947068095207214, 0.5488496422767639, 0.25283268094062805, 0.7176525592803955, 0.9882412552833557, 0.9956894516944885, 0.20575681328773499, 0.7033035159111023, 0.8901147842407227, 0.9087565541267395, 0.9547992944717407, 0.9845190048217773], "token_confidences_second": null, "final_mean_entropy": 0.7021559777233051, "final_min_margin": 0.21455955505371094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Social networking."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_second": null, "mean_entropy_first": 0.7021559777233051, "normalized_entropy_first": -0.8762913955103208, "min_margin_first": 0.21455955505371094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1199, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1200, "total_latency_s": 1.2, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8503378629684448, 0.14283114671707153, 1.8364002704620361, 0.33744168281555176, 0.002046043984591961, 0.6153597235679626, 0.9635576009750366, 7.817341247573495e-05, 0.04376684129238129, 0.7480852603912354, 2.922473430633545, 0.7100098133087158, 0.06571037322282791, 0.04013017565011978, 2.514021873474121, 1.0137600898742676, 0.581512987613678, 0.3341432213783264, 0.2167014479637146, 0.10475153475999832], "entropies_second": null, "final_normalized_entropy": -0.8762913955103208, "sequence_confidence_first": 0.7007287471408773, "sequence_confidence_second": null, "sequence_confidence_final": 0.7007287471408773, "token_confidences_first": [0.5555394291877747, 0.9785944819450378, 0.3491204082965851, 0.9164201617240906, 0.9998016953468323, 0.7513409852981567, 0.5813701152801514, 0.999995231628418, 0.9947068095207214, 0.5488496422767639, 0.25283268094062805, 0.7176525592803955, 0.9882412552833557, 0.9956894516944885, 0.20575681328773499, 0.7033035159111023, 0.8901147842407227, 0.9087565541267395, 0.9547992944717407, 0.9845190048217773], "token_confidences_second": null, "final_mean_entropy": 0.7021559777233051, "final_min_margin": 0.21455955505371094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.444444444444445, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Facebook."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Australian mobile internet usage", "raw_answer_second": null, "mean_entropy_first": 0.7021559777233051, "normalized_entropy_first": -0.8011328962989422, "min_margin_first": 0.21455955505371094, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1201, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1202, "total_latency_s": 1.202, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8503378629684448, 0.14283114671707153, 1.8364002704620361, 0.33744168281555176, 0.002046043984591961, 0.6153597235679626, 0.9635576009750366, 7.817341247573495e-05, 0.04376684129238129, 0.7480852603912354, 2.922473430633545, 0.7100098133087158, 0.06571037322282791, 0.04013017565011978, 2.514021873474121, 1.0137600898742676, 0.581512987613678, 0.3341432213783264, 0.2167014479637146, 0.10475153475999832], "entropies_second": null, "final_normalized_entropy": -0.8011328962989422, "sequence_confidence_first": 0.7007287471408773, "sequence_confidence_second": null, "sequence_confidence_final": 0.7007287471408773, "token_confidences_first": [0.5555394291877747, 0.9785944819450378, 0.3491204082965851, 0.9164201617240906, 0.9998016953468323, 0.7513409852981567, 0.5813701152801514, 0.999995231628418, 0.9947068095207214, 0.5488496422767639, 0.25283268094062805, 0.7176525592803955, 0.9882412552833557, 0.9956894516944885, 0.20575681328773499, 0.7033035159111023, 0.8901147842407227, 0.9087565541267395, 0.9547992944717407, 0.9845190048217773], "token_confidences_second": null, "final_mean_entropy": 0.7021559777233051, "final_min_margin": 0.21455955505371094, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.555555555555555, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["5."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a stylized map of Australia, with various statistics", "used_ocr": false, "answer_first": "The image you've provided appears to be a stylized map of Australia, with various statistics", "answer_second": null, "raw_answer": "The image you've provided appears to be a stylized map of Australia, with various statistics", "raw_answer_first": "The image you've provided appears to be a stylized map of Australia, with various statistics", "raw_answer_second": null, "mean_entropy_first": 1.101107178114762, "normalized_entropy_first": 1.4189624878212217, "min_margin_first": 0.07985115051269531, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9804674386978149, 0.025464821606874466, 1.9017958641052246, 0.34169191122055054, 0.007000292651355267, 0.6525658369064331, 0.9789620637893677, 0.00017718292656354606, 0.11711034178733826, 0.6109588146209717, 2.3902130126953125, 0.0007137979846447706, 0.004572833422571421, 2.4023659229278564, 2.219766139984131, 0.6851093769073486, 1.600191593170166, 3.3243396282196045, 0.8204185366630554, 2.9582581520080566], "entropies_second": null, "final_normalized_entropy": 1.4189624878212217, "sequence_confidence_first": 0.6225351903551871, "sequence_confidence_second": null, "sequence_confidence_final": 0.6225351903551871, "token_confidences_first": [0.5287898182868958, 0.9973680377006531, 0.26016053557395935, 0.9155790209770203, 0.9992208480834961, 0.7538257837295532, 0.6824324727058411, 0.9999887943267822, 0.9816066026687622, 0.7502235174179077, 0.45688343048095703, 0.9999436140060425, 0.9995326995849609, 0.26980167627334595, 0.47393736243247986, 0.8221721053123474, 0.46197986602783203, 0.2643953561782837, 0.8560736179351807, 0.3201788663864136], "token_confidences_second": null, "final_mean_entropy": 1.101107178114762, "final_min_margin": 0.07985115051269531, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 46.0, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["69.7%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 2.763184648603078, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1236, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1236, "total_latency_s": 1.236, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 2.763184648603078, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.166666666666668, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["5.4%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 1.9739724994305923, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1235, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1236, "total_latency_s": 1.236, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 1.9739724994305923, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.0, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Yellow."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 1.5885781441078033, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1237, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1238, "total_latency_s": 1.238, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 1.5885781441078033, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.571428571428571, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["100%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 1.3466836322470748, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1233, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1234, "total_latency_s": 1.234, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 1.3466836322470748, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.0, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["78.6%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 1.1754292654610738, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1232, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1233, "total_latency_s": 1.233, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 1.1754292654610738, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.166666666666668, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["100.1%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 1.0452386179437332, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1234, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1235, "total_latency_s": 1.235, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 1.0452386179437332, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 16.428571428571427, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["30.4%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 0.9415025871747597, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1238, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1239, "total_latency_s": 1.239, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 0.9415025871747597, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.166666666666668, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["94.7%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 0.8560518845826661, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1232, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1233, "total_latency_s": 1.233, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 0.8560518845826661, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.166666666666668, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Blue."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 0.7839066014958763, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1232, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1233, "total_latency_s": 1.233, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 0.7839066014958763, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 22.4, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Green."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "used_ocr": false, "answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "answer_second": null, "raw_answer": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_first": "The image displays a pie chart that illustrates the distribution of gender and age groups among individuals who are", "raw_answer_second": null, "mean_entropy_first": 1.3964984270744025, "normalized_entropy_first": 0.7218311342167908, "min_margin_first": 0.024694442749023438, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1234, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1234, "total_latency_s": 1.234, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6656588912010193, 0.21126198768615723, 2.087681770324707, 0.43329110741615295, 1.764589786529541, 0.014017214998602867, 2.5343542098999023, 2.347496509552002, 0.0001477990299463272, 0.34939044713974, 2.0247390270233154, 0.04214155673980713, 1.810353398323059, 1.8452380895614624, 0.021187370643019676, 2.343519687652588, 2.229323387145996, 2.245842933654785, 3.0668492317199707, 1.8928841352462769], "entropies_second": null, "final_normalized_entropy": 0.7218311342167908, "sequence_confidence_first": 0.5040460928044153, "sequence_confidence_second": null, "sequence_confidence_final": 0.5040460928044153, "token_confidences_first": [0.7701289653778076, 0.9690835475921631, 0.22178970277309418, 0.9036588072776794, 0.566703736782074, 0.9983432292938232, 0.20923343300819397, 0.22860097885131836, 0.9999898672103882, 0.9429401755332947, 0.31390050053596497, 0.9952728152275085, 0.48849931359291077, 0.5094079375267029, 0.9977586269378662, 0.35100939869880676, 0.24415554106235504, 0.30579325556755066, 0.22467245161533356, 0.6427724957466125], "token_confidences_second": null, "final_mean_entropy": 1.3964984270744025, "final_min_margin": 0.024694442749023438, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.333333333333332, "wer": 19.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["22."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1479135589441285, "normalized_entropy_first": -0.22750498615146209, "min_margin_first": 0.015872955322265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1347, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7685317993164062, 0.9946465492248535, 1.5499520301818848, 0.8047438263893127, 2.276357412338257, 0.02057468146085739, 0.923824667930603, 0.0076642814092338085, 1.737886905670166, 2.4871017932891846, 1.9312915802001953, 1.7143845558166504, 0.01900888979434967, 1.8018553256988525, 1.4160220623016357, 1.4018386602401733, 0.7127860188484192, 0.016154594719409943, 1.3508431911468506, 1.0228023529052734], "entropies_second": null, "final_normalized_entropy": -0.22750498615146209, "sequence_confidence_first": 0.5959260215916369, "sequence_confidence_second": null, "sequence_confidence_final": 0.5959260215916369, "token_confidences_first": [0.4988795518875122, 0.6305509209632874, 0.4211588501930237, 0.581581175327301, 0.4224161207675934, 0.9973708391189575, 0.7993616461753845, 0.9991282820701599, 0.5196930766105652, 0.32542598247528076, 0.34682148694992065, 0.44504085183143616, 0.9979968667030334, 0.3864423930644989, 0.7570879459381104, 0.6686365604400635, 0.6984685659408569, 0.9983202815055847, 0.49272623658180237, 0.7028056979179382], "token_confidences_second": null, "final_mean_entropy": 1.1479135589441285, "final_min_margin": 0.015872955322265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 34.666666666666664, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["21.6."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1479135589441285, "normalized_entropy_first": -0.2152737869918849, "min_margin_first": 0.015872955322265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1351, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1352, "total_latency_s": 1.352, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7685317993164062, 0.9946465492248535, 1.5499520301818848, 0.8047438263893127, 2.276357412338257, 0.02057468146085739, 0.923824667930603, 0.0076642814092338085, 1.737886905670166, 2.4871017932891846, 1.9312915802001953, 1.7143845558166504, 0.01900888979434967, 1.8018553256988525, 1.4160220623016357, 1.4018386602401733, 0.7127860188484192, 0.016154594719409943, 1.3508431911468506, 1.0228023529052734], "entropies_second": null, "final_normalized_entropy": -0.2152737869918849, "sequence_confidence_first": 0.5959260215916369, "sequence_confidence_second": null, "sequence_confidence_final": 0.5959260215916369, "token_confidences_first": [0.4988795518875122, 0.6305509209632874, 0.4211588501930237, 0.581581175327301, 0.4224161207675934, 0.9973708391189575, 0.7993616461753845, 0.9991282820701599, 0.5196930766105652, 0.32542598247528076, 0.34682148694992065, 0.44504085183143616, 0.9979968667030334, 0.3864423930644989, 0.7570879459381104, 0.6686365604400635, 0.6984685659408569, 0.9983202815055847, 0.49272623658180237, 0.7028056979179382], "token_confidences_second": null, "final_mean_entropy": 1.1479135589441285, "final_min_margin": 0.015872955322265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 20.8, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["17."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1479135589441285, "normalized_entropy_first": -0.2037550629172863, "min_margin_first": 0.015872955322265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7685317993164062, 0.9946465492248535, 1.5499520301818848, 0.8047438263893127, 2.276357412338257, 0.02057468146085739, 0.923824667930603, 0.0076642814092338085, 1.737886905670166, 2.4871017932891846, 1.9312915802001953, 1.7143845558166504, 0.01900888979434967, 1.8018553256988525, 1.4160220623016357, 1.4018386602401733, 0.7127860188484192, 0.016154594719409943, 1.3508431911468506, 1.0228023529052734], "entropies_second": null, "final_normalized_entropy": -0.2037550629172863, "sequence_confidence_first": 0.5959260215916369, "sequence_confidence_second": null, "sequence_confidence_final": 0.5959260215916369, "token_confidences_first": [0.4988795518875122, 0.6305509209632874, 0.4211588501930237, 0.581581175327301, 0.4224161207675934, 0.9973708391189575, 0.7993616461753845, 0.9991282820701599, 0.5196930766105652, 0.32542598247528076, 0.34682148694992065, 0.44504085183143616, 0.9979968667030334, 0.3864423930644989, 0.7570879459381104, 0.6686365604400635, 0.6984685659408569, 0.9983202815055847, 0.49272623658180237, 0.7028056979179382], "token_confidences_second": null, "final_mean_entropy": 1.1479135589441285, "final_min_margin": 0.015872955322265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 34.666666666666664, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["15%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1479135589441285, "normalized_entropy_first": -0.19289901882529695, "min_margin_first": 0.015872955322265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1349, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1350, "total_latency_s": 1.35, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7685317993164062, 0.9946465492248535, 1.5499520301818848, 0.8047438263893127, 2.276357412338257, 0.02057468146085739, 0.923824667930603, 0.0076642814092338085, 1.737886905670166, 2.4871017932891846, 1.9312915802001953, 1.7143845558166504, 0.01900888979434967, 1.8018553256988525, 1.4160220623016357, 1.4018386602401733, 0.7127860188484192, 0.016154594719409943, 1.3508431911468506, 1.0228023529052734], "entropies_second": null, "final_normalized_entropy": -0.19289901882529695, "sequence_confidence_first": 0.5959260215916369, "sequence_confidence_second": null, "sequence_confidence_final": 0.5959260215916369, "token_confidences_first": [0.4988795518875122, 0.6305509209632874, 0.4211588501930237, 0.581581175327301, 0.4224161207675934, 0.9973708391189575, 0.7993616461753845, 0.9991282820701599, 0.5196930766105652, 0.32542598247528076, 0.34682148694992065, 0.44504085183143616, 0.9979968667030334, 0.3864423930644989, 0.7570879459381104, 0.6686365604400635, 0.6984685659408569, 0.9983202815055847, 0.49272623658180237, 0.7028056979179382], "token_confidences_second": null, "final_mean_entropy": 1.1479135589441285, "final_min_margin": 0.015872955322265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.0, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Total christianity."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of the religious landscape in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1479135589441285, "normalized_entropy_first": -0.18266055267412915, "min_margin_first": 0.015872955322265625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7685317993164062, 0.9946465492248535, 1.5499520301818848, 0.8047438263893127, 2.276357412338257, 0.02057468146085739, 0.923824667930603, 0.0076642814092338085, 1.737886905670166, 2.4871017932891846, 1.9312915802001953, 1.7143845558166504, 0.01900888979434967, 1.8018553256988525, 1.4160220623016357, 1.4018386602401733, 0.7127860188484192, 0.016154594719409943, 1.3508431911468506, 1.0228023529052734], "entropies_second": null, "final_normalized_entropy": -0.18266055267412915, "sequence_confidence_first": 0.5959260215916369, "sequence_confidence_second": null, "sequence_confidence_final": 0.5959260215916369, "token_confidences_first": [0.4988795518875122, 0.6305509209632874, 0.4211588501930237, 0.581581175327301, 0.4224161207675934, 0.9973708391189575, 0.7993616461753845, 0.9991282820701599, 0.5196930766105652, 0.32542598247528076, 0.34682148694992065, 0.44504085183143616, 0.9979968667030334, 0.3864423930644989, 0.7570879459381104, 0.6686365604400635, 0.6984685659408569, 0.9983202815055847, 0.49272623658180237, 0.7028056979179382], "token_confidences_second": null, "final_mean_entropy": 1.1479135589441285, "final_min_margin": 0.015872955322265625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.684210526315789, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1.3 million."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is a graphic from the Clean Energy Council, which is an organization that advocates", "used_ocr": false, "answer_first": "The image is a graphic from the Clean Energy Council, which is an organization that advocates", "answer_second": null, "raw_answer": "The image is a graphic from the Clean Energy Council, which is an organization that advocates", "raw_answer_first": "The image is a graphic from the Clean Energy Council, which is an organization that advocates", "raw_answer_second": null, "mean_entropy_first": 1.1922607473563402, "normalized_entropy_first": 0.0326438989096791, "min_margin_first": 0.10525131225585938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1369, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1370, "total_latency_s": 1.37, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7596283555030823, 0.10964912176132202, 1.644982099533081, 0.7325283885002136, 2.1714231967926025, 0.3149673640727997, 2.891266107559204, 0.7556185126304626, 0.26431798934936523, 0.004972637165337801, 0.003292110515758395, 0.016208546236157417, 2.028301954269409, 2.6917967796325684, 2.2150416374206543, 1.8329393863677979, 1.564706563949585, 2.0411946773529053, 1.8017001152038574, 0.0006794033106416464], "entropies_second": null, "final_normalized_entropy": 0.0326438989096791, "sequence_confidence_first": 0.5556607156722444, "sequence_confidence_second": null, "sequence_confidence_final": 0.5556607156722444, "token_confidences_first": [0.5903760194778442, 0.9833311438560486, 0.4384070634841919, 0.5800396203994751, 0.3640032112598419, 0.9066201448440552, 0.21607422828674316, 0.8010607957839966, 0.9554806351661682, 0.9994328618049622, 0.9997125267982483, 0.9984716773033142, 0.42153412103652954, 0.27735376358032227, 0.4111592769622803, 0.4459649622440338, 0.44857466220855713, 0.28125983476638794, 0.3622001111507416, 0.9999449253082275], "token_confidences_second": null, "final_mean_entropy": 1.1922607473563402, "final_min_margin": 0.10525131225585938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.083333333333333, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["32%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or article related to the audio", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_second": null, "mean_entropy_first": 0.9841076527498445, "normalized_entropy_first": -0.9864180213674179, "min_margin_first": 0.009122848510742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1190, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1191, "total_latency_s": 1.191, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8412699103355408, 0.07099633663892746, 1.5951482057571411, 0.2780756652355194, 0.001426811097189784, 0.6205916404724121, 0.8562939167022705, 3.433204619796015e-05, 0.013080896809697151, 0.8800369501113892, 2.7582945823669434, 0.195173978805542, 1.1330077648162842, 1.7606310844421387, 1.0911613702774048, 2.385882616043091, 2.0595293045043945, 0.00020651986415032297, 1.471487045288086, 1.6698241233825684], "entropies_second": null, "final_normalized_entropy": -0.9864180213674179, "sequence_confidence_first": 0.6030519187337662, "sequence_confidence_second": null, "sequence_confidence_final": 0.6030519187337662, "token_confidences_first": [0.5913901329040527, 0.9912703037261963, 0.4204580783843994, 0.9373154044151306, 0.9998719692230225, 0.7399618625640869, 0.5036748647689819, 0.9999979734420776, 0.9984775185585022, 0.637700617313385, 0.3185123801231384, 0.9645642638206482, 0.5673877000808716, 0.5022855997085571, 0.7253470420837402, 0.189921572804451, 0.32289737462997437, 0.9999849796295166, 0.4285571277141571, 0.4422686994075775], "token_confidences_second": null, "final_mean_entropy": 0.9841076527498445, "final_min_margin": 0.009122848510742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2018, 2017, 2016."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or article related to the audio", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_second": null, "mean_entropy_first": 0.9841076527498445, "normalized_entropy_first": -0.8933448892558362, "min_margin_first": 0.009122848510742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1189, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1189, "total_latency_s": 1.189, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8412699103355408, 0.07099633663892746, 1.5951482057571411, 0.2780756652355194, 0.001426811097189784, 0.6205916404724121, 0.8562939167022705, 3.433204619796015e-05, 0.013080896809697151, 0.8800369501113892, 2.7582945823669434, 0.195173978805542, 1.1330077648162842, 1.7606310844421387, 1.0911613702774048, 2.385882616043091, 2.0595293045043945, 0.00020651986415032297, 1.471487045288086, 1.6698241233825684], "entropies_second": null, "final_normalized_entropy": -0.8933448892558362, "sequence_confidence_first": 0.6030519187337662, "sequence_confidence_second": null, "sequence_confidence_final": 0.6030519187337662, "token_confidences_first": [0.5913901329040527, 0.9912703037261963, 0.4204580783843994, 0.9373154044151306, 0.9998719692230225, 0.7399618625640869, 0.5036748647689819, 0.9999979734420776, 0.9984775185585022, 0.637700617313385, 0.3185123801231384, 0.9645642638206482, 0.5673877000808716, 0.5022855997085571, 0.7253470420837402, 0.189921572804451, 0.32289737462997437, 0.9999849796295166, 0.4285571277141571, 0.4422686994075775], "token_confidences_second": null, "final_mean_entropy": 0.9841076527498445, "final_min_margin": 0.009122848510742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.294117647058823, "wer": 6.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["6."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or article related to the audio", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_second": null, "mean_entropy_first": 0.9841076527498445, "normalized_entropy_first": -0.8155816414815449, "min_margin_first": 0.009122848510742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1194, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1195, "total_latency_s": 1.195, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8412699103355408, 0.07099633663892746, 1.5951482057571411, 0.2780756652355194, 0.001426811097189784, 0.6205916404724121, 0.8562939167022705, 3.433204619796015e-05, 0.013080896809697151, 0.8800369501113892, 2.7582945823669434, 0.195173978805542, 1.1330077648162842, 1.7606310844421387, 1.0911613702774048, 2.385882616043091, 2.0595293045043945, 0.00020651986415032297, 1.471487045288086, 1.6698241233825684], "entropies_second": null, "final_normalized_entropy": -0.8155816414815449, "sequence_confidence_first": 0.6030519187337662, "sequence_confidence_second": null, "sequence_confidence_final": 0.6030519187337662, "token_confidences_first": [0.5913901329040527, 0.9912703037261963, 0.4204580783843994, 0.9373154044151306, 0.9998719692230225, 0.7399618625640869, 0.5036748647689819, 0.9999979734420776, 0.9984775185585022, 0.637700617313385, 0.3185123801231384, 0.9645642638206482, 0.5673877000808716, 0.5022855997085571, 0.7253470420837402, 0.189921572804451, 0.32289737462997437, 0.9999849796295166, 0.4285571277141571, 0.4422686994075775], "token_confidences_second": null, "final_mean_entropy": 0.9841076527498445, "final_min_margin": 0.009122848510742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 46.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["32%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or article related to the audio", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_second": null, "mean_entropy_first": 0.9841076527498445, "normalized_entropy_first": -0.7492120183008114, "min_margin_first": 0.009122848510742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1193, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1194, "total_latency_s": 1.194, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8412699103355408, 0.07099633663892746, 1.5951482057571411, 0.2780756652355194, 0.001426811097189784, 0.6205916404724121, 0.8562939167022705, 3.433204619796015e-05, 0.013080896809697151, 0.8800369501113892, 2.7582945823669434, 0.195173978805542, 1.1330077648162842, 1.7606310844421387, 1.0911613702774048, 2.385882616043091, 2.0595293045043945, 0.00020651986415032297, 1.471487045288086, 1.6698241233825684], "entropies_second": null, "final_normalized_entropy": -0.7492120183008114, "sequence_confidence_first": 0.6030519187337662, "sequence_confidence_second": null, "sequence_confidence_final": 0.6030519187337662, "token_confidences_first": [0.5913901329040527, 0.9912703037261963, 0.4204580783843994, 0.9373154044151306, 0.9998719692230225, 0.7399618625640869, 0.5036748647689819, 0.9999979734420776, 0.9984775185585022, 0.637700617313385, 0.3185123801231384, 0.9645642638206482, 0.5673877000808716, 0.5022855997085571, 0.7253470420837402, 0.189921572804451, 0.32289737462997437, 0.9999849796295166, 0.4285571277141571, 0.4422686994075775], "token_confidences_second": null, "final_mean_entropy": 0.9841076527498445, "final_min_margin": 0.009122848510742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Podcast - bespoke branded podcasts."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a page from a report or article related to the audio", "used_ocr": false, "answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "answer_second": null, "raw_answer": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_first": "The image you've provided appears to be a page from a report or article related to the audio", "raw_answer_second": null, "mean_entropy_first": 0.9841076527498445, "normalized_entropy_first": -0.691619002364529, "min_margin_first": 0.009122848510742188, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1193, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1194, "total_latency_s": 1.194, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8412699103355408, 0.07099633663892746, 1.5951482057571411, 0.2780756652355194, 0.001426811097189784, 0.6205916404724121, 0.8562939167022705, 3.433204619796015e-05, 0.013080896809697151, 0.8800369501113892, 2.7582945823669434, 0.195173978805542, 1.1330077648162842, 1.7606310844421387, 1.0911613702774048, 2.385882616043091, 2.0595293045043945, 0.00020651986415032297, 1.471487045288086, 1.6698241233825684], "entropies_second": null, "final_normalized_entropy": -0.691619002364529, "sequence_confidence_first": 0.6030519187337662, "sequence_confidence_second": null, "sequence_confidence_final": 0.6030519187337662, "token_confidences_first": [0.5913901329040527, 0.9912703037261963, 0.4204580783843994, 0.9373154044151306, 0.9998719692230225, 0.7399618625640869, 0.5036748647689819, 0.9999979734420776, 0.9984775185585022, 0.637700617313385, 0.3185123801231384, 0.9645642638206482, 0.5673877000808716, 0.5022855997085571, 0.7253470420837402, 0.189921572804451, 0.32289737462997437, 0.9999849796295166, 0.4285571277141571, 0.4422686994075775], "token_confidences_second": null, "final_mean_entropy": 0.9841076527498445, "final_min_margin": 0.009122848510742188, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.085714285714286, "wer": 3.6, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["7."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster related to mining in Australia. It features a", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster related to mining in Australia. It features a", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster related to mining in Australia. It features a", "raw_answer_first": "This image appears to be an infographic or poster related to mining in Australia. It features a", "raw_answer_second": null, "mean_entropy_first": 0.9102035184405395, "normalized_entropy_first": -1.038478665036766, "min_margin_first": 0.17105865478515625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1431, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7522752285003662, 0.8860726356506348, 1.151932954788208, 8.538611291442066e-05, 0.027643978595733643, 0.6589179635047913, 0.9409663677215576, 0.012060396373271942, 2.4251208305358887, 1.9580289125442505, 2.519291400909424, 0.0001596271904418245, 0.8462615609169006, 0.0017726952210068703, 0.8648442625999451, 0.08469037711620331, 0.4213632643222809, 0.6905655860900879, 2.440730571746826, 1.5212863683700562], "entropies_second": null, "final_normalized_entropy": -1.038478665036766, "sequence_confidence_first": 0.638274204434621, "sequence_confidence_second": null, "sequence_confidence_final": 0.638274204434621, "token_confidences_first": [0.5883855819702148, 0.5847116708755493, 0.5239052176475525, 0.9999943971633911, 0.9969679713249207, 0.6653336882591248, 0.5759063959121704, 0.9985564351081848, 0.3736301362514496, 0.4342992305755615, 0.269375205039978, 0.9999889135360718, 0.661436140537262, 0.9998389482498169, 0.7686895728111267, 0.9875925183296204, 0.8659799098968506, 0.8133634924888611, 0.2740198075771332, 0.4324556589126587], "token_confidences_second": null, "final_mean_entropy": 0.9102035184405395, "final_min_margin": 0.17105865478515625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 47.0, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["200+ employees."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster related to mining in Australia. It features a", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster related to mining in Australia. It features a", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster related to mining in Australia. It features a", "raw_answer_first": "This image appears to be an infographic or poster related to mining in Australia. It features a", "raw_answer_second": null, "mean_entropy_first": 0.9102035184405395, "normalized_entropy_first": -0.9360080175868616, "min_margin_first": 0.17105865478515625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1431, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1432, "total_latency_s": 1.432, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7522752285003662, 0.8860726356506348, 1.151932954788208, 8.538611291442066e-05, 0.027643978595733643, 0.6589179635047913, 0.9409663677215576, 0.012060396373271942, 2.4251208305358887, 1.9580289125442505, 2.519291400909424, 0.0001596271904418245, 0.8462615609169006, 0.0017726952210068703, 0.8648442625999451, 0.08469037711620331, 0.4213632643222809, 0.6905655860900879, 2.440730571746826, 1.5212863683700562], "entropies_second": null, "final_normalized_entropy": -0.9360080175868616, "sequence_confidence_first": 0.638274204434621, "sequence_confidence_second": null, "sequence_confidence_final": 0.638274204434621, "token_confidences_first": [0.5883855819702148, 0.5847116708755493, 0.5239052176475525, 0.9999943971633911, 0.9969679713249207, 0.6653336882591248, 0.5759063959121704, 0.9985564351081848, 0.3736301362514496, 0.4342992305755615, 0.269375205039978, 0.9999889135360718, 0.661436140537262, 0.9998389482498169, 0.7686895728111267, 0.9875925183296204, 0.8659799098968506, 0.8133634924888611, 0.2740198075771332, 0.4324556589126587], "token_confidences_second": null, "final_mean_entropy": 0.9102035184405395, "final_min_margin": 0.17105865478515625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.8, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["REACHOUT.com."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic related to mental health and suicide prevention. It provides statistics and", "used_ocr": false, "answer_first": "The image is an infographic related to mental health and suicide prevention. It provides statistics and", "answer_second": null, "raw_answer": "The image is an infographic related to mental health and suicide prevention. It provides statistics and", "raw_answer_first": "The image is an infographic related to mental health and suicide prevention. It provides statistics and", "raw_answer_second": null, "mean_entropy_first": 1.087437149581092, "normalized_entropy_first": 0.11347643822471543, "min_margin_first": 0.06046104431152344, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1353, "total_latency_s": 1.353, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7538028359413147, 0.10073767602443695, 1.717666506767273, 0.7902384996414185, 1.0331238508224487, 0.012780031189322472, 2.693467617034912, 0.00017508477321825922, 1.9995827674865723, 0.009987111203372478, 1.9714142084121704, 2.0311503410339355, 0.05562584474682808, 0.3319775462150574, 0.0004569074953906238, 1.231158971786499, 0.983576774597168, 3.0128397941589355, 1.3026565313339233, 1.7163240909576416], "entropies_second": null, "final_normalized_entropy": 0.11347643822471543, "sequence_confidence_first": 0.5858614991083687, "sequence_confidence_second": null, "sequence_confidence_final": 0.5858614991083687, "token_confidences_first": [0.6482996344566345, 0.9857164025306702, 0.4349087178707123, 0.5084955096244812, 0.5172382593154907, 0.9984418749809265, 0.20968760550022125, 0.9999886751174927, 0.29209962487220764, 0.9989411234855652, 0.4366241693496704, 0.41816556453704834, 0.9911255240440369, 0.9453232884407043, 0.9999650716781616, 0.6465890407562256, 0.7220245003700256, 0.16861258447170258, 0.6743086576461792, 0.5596997737884521], "token_confidences_second": null, "final_mean_entropy": 1.087437149581092, "final_min_margin": 0.06046104431152344, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.3076923076923075, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["JPN, Japan."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -3.3865269234379736, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1993, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1994, "total_latency_s": 1.994, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -3.3865269234379736, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.454545454545454, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["IND, India."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -2.1926750921652403, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1405, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1406, "total_latency_s": 1.406, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -2.1926750921652403, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.2727272727272725, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["10.1."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -1.7094244646027263, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1407, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1408, "total_latency_s": 1.408, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -1.7094244646027263, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.2, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2.8."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -1.426606121757941, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1409, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1409, "total_latency_s": 1.409, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -1.426606121757941, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["8.3."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -1.233668828619553, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1409, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1410, "total_latency_s": 1.41, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -1.233668828619553, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["CHN, China."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -1.0903277476139814, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1405, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1406, "total_latency_s": 1.406, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -1.0903277476139814, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.454545454545454, "wer": 6.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["USA."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -0.9778816136612819, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1406, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1406, "total_latency_s": 1.406, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -0.9778816136612819, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.25, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["GBR, Great Britain."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -0.8862913688431744, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1409, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1410, "total_latency_s": 1.41, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -0.8862913688431744, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.473684210526316, "wer": 4.333333333333333, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4.4."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -0.8096128333032587, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1408, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1409, "total_latency_s": 1.409, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -0.8096128333032587, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["4.9."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_first": "The image you've provided is an infographic titled \"The Path to a Low-Carbon", "raw_answer_second": null, "mean_entropy_first": 0.47820190636848564, "normalized_entropy_first": -0.7440673931157884, "min_margin_first": 0.1909160614013672, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1405, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1407, "total_latency_s": 1.407, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7549256086349487, 0.032452065497636795, 1.5850142240524292, 0.24095170199871063, 0.0026605366729199886, 0.6927832365036011, 0.8319424390792847, 0.8000888228416443, 0.836848258972168, 0.012063326314091682, 2.217048406600952, 0.010809699073433876, 0.7143484950065613, 0.5341771245002747, 0.06305892765522003, 0.08739893138408661, 0.04911129176616669, 0.03682461753487587, 0.061430443078279495, 9.997020242735744e-05], "entropies_second": null, "final_normalized_entropy": -0.7440673931157884, "sequence_confidence_first": 0.7955435846426945, "sequence_confidence_second": null, "sequence_confidence_final": 0.7955435846426945, "token_confidences_first": [0.5668174028396606, 0.996360719203949, 0.4877907633781433, 0.9483377933502197, 0.9997342228889465, 0.6861709356307983, 0.5348781943321228, 0.5517205595970154, 0.7211722731590271, 0.9985601305961609, 0.4252451956272125, 0.9988818764686584, 0.8367900252342224, 0.7995678186416626, 0.9890819787979126, 0.986041247844696, 0.9930497407913208, 0.9942771792411804, 0.9892151951789856, 0.999993085861206], "token_confidences_second": null, "final_mean_entropy": 0.47820190636848564, "final_min_margin": 0.1909160614013672, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 19.0, "wer": 13.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Workplace bullying."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "used_ocr": false, "answer_first": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "answer_second": null, "raw_answer": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "raw_answer_first": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "raw_answer_second": null, "mean_entropy_first": 1.0137946189894138, "normalized_entropy_first": 1.1003518455310701, "min_margin_first": 0.051914215087890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1842, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1843, "total_latency_s": 1.843, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8942759037017822, 0.4724354147911072, 2.051302909851074, 0.6806724071502686, 0.40505167841911316, 0.003314684145152569, 2.3367228507995605, 2.20430588722229, 0.00014562424621544778, 1.0742989778518677, 0.0005349100101739168, 0.0029305764473974705, 3.947271761717275e-05, 1.279524564743042, 0.9891607761383057, 3.227112293243408, 3.3025665283203125, 0.0787140429019928, 1.0698413848876953, 0.20294149219989777], "entropies_second": null, "final_normalized_entropy": 1.1003518455310701, "sequence_confidence_first": 0.6336624045024787, "sequence_confidence_second": null, "sequence_confidence_final": 0.6336624045024787, "token_confidences_first": [0.6690456867218018, 0.8942201137542725, 0.3313066065311432, 0.7746843695640564, 0.8872674703598022, 0.9996575117111206, 0.2610207200050354, 0.41831326484680176, 0.999990701675415, 0.6266911625862122, 0.9999550580978394, 0.9997411370277405, 0.9999973773956299, 0.6081748008728027, 0.7458217740058899, 0.17515936493873596, 0.24687720835208893, 0.9881243109703064, 0.6324219107627869, 0.9539124965667725], "token_confidences_second": null, "final_mean_entropy": 1.0137946189894138, "final_min_margin": 0.051914215087890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 3.4210526315789473, "wer": 6.0, "precision": 0.14285714285714285, "recall": 1.0, "f1": 0.25, "rouge_l": 0.25, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "", "image_id": null, "answers": ["White."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "used_ocr": false, "answer_first": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "answer_second": null, "raw_answer": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "raw_answer_first": "The image is an infographic that discusses workplace bullying. It is titled \"What is", "raw_answer_second": null, "mean_entropy_first": 1.0137946189894138, "normalized_entropy_first": 0.9859049078229313, "min_margin_first": 0.051914215087890625, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1238, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1238, "total_latency_s": 1.238, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8942759037017822, 0.4724354147911072, 2.051302909851074, 0.6806724071502686, 0.40505167841911316, 0.003314684145152569, 2.3367228507995605, 2.20430588722229, 0.00014562424621544778, 1.0742989778518677, 0.0005349100101739168, 0.0029305764473974705, 3.947271761717275e-05, 1.279524564743042, 0.9891607761383057, 3.227112293243408, 3.3025665283203125, 0.0787140429019928, 1.0698413848876953, 0.20294149219989777], "entropies_second": null, "final_normalized_entropy": 0.9859049078229313, "sequence_confidence_first": 0.6336624045024787, "sequence_confidence_second": null, "sequence_confidence_final": 0.6336624045024787, "token_confidences_first": [0.6690456867218018, 0.8942201137542725, 0.3313066065311432, 0.7746843695640564, 0.8872674703598022, 0.9996575117111206, 0.2610207200050354, 0.41831326484680176, 0.999990701675415, 0.6266911625862122, 0.9999550580978394, 0.9997411370277405, 0.9999973773956299, 0.6081748008728027, 0.7458217740058899, 0.17515936493873596, 0.24687720835208893, 0.9881243109703064, 0.6324219107627869, 0.9539124965667725], "token_confidences_second": null, "final_mean_entropy": 1.0137946189894138, "final_min_margin": 0.051914215087890625, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.166666666666666, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["North island."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "used_ocr": false, "answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "answer_second": null, "raw_answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_second": null, "mean_entropy_first": 1.3146190719678998, "normalized_entropy_first": 1.8987274548804955, "min_margin_first": 0.13409423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1354, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7349413633346558, 0.162850022315979, 1.9209182262420654, 0.18312187492847443, 0.862703800201416, 0.6214748620986938, 0.3536972403526306, 1.5592206716537476, 2.976860523223877, 1.6422491073608398, 1.9868278503417969, 0.010194558650255203, 1.368729829788208, 0.8842310905456543, 0.19660961627960205, 1.1604431867599487, 2.063007354736328, 2.4845962524414062, 2.277324676513672, 2.842379331588745], "entropies_second": null, "final_normalized_entropy": 1.8987274548804955, "sequence_confidence_first": 0.5673132945554455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5673132945554455, "token_confidences_first": [0.5774587988853455, 0.9692534804344177, 0.27598297595977783, 0.9656708836555481, 0.811630129814148, 0.8936455845832825, 0.9068335294723511, 0.584604024887085, 0.206391841173172, 0.5076341032981873, 0.3537214398384094, 0.9986785054206848, 0.5207311511039734, 0.808785617351532, 0.9681317806243896, 0.5173583626747131, 0.4847613573074341, 0.3849559724330902, 0.365609347820282, 0.3903164863586426], "token_confidences_second": null, "final_mean_entropy": 1.3146190719678998, "final_min_margin": 0.13409423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.153846153846154, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Auralia."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "used_ocr": false, "answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "answer_second": null, "raw_answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_second": null, "mean_entropy_first": 1.3146190719678998, "normalized_entropy_first": 1.544301037384003, "min_margin_first": 0.13409423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1360, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1361, "total_latency_s": 1.361, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7349413633346558, 0.162850022315979, 1.9209182262420654, 0.18312187492847443, 0.862703800201416, 0.6214748620986938, 0.3536972403526306, 1.5592206716537476, 2.976860523223877, 1.6422491073608398, 1.9868278503417969, 0.010194558650255203, 1.368729829788208, 0.8842310905456543, 0.19660961627960205, 1.1604431867599487, 2.063007354736328, 2.4845962524414062, 2.277324676513672, 2.842379331588745], "entropies_second": null, "final_normalized_entropy": 1.544301037384003, "sequence_confidence_first": 0.5673132945554455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5673132945554455, "token_confidences_first": [0.5774587988853455, 0.9692534804344177, 0.27598297595977783, 0.9656708836555481, 0.811630129814148, 0.8936455845832825, 0.9068335294723511, 0.584604024887085, 0.206391841173172, 0.5076341032981873, 0.3537214398384094, 0.9986785054206848, 0.5207311511039734, 0.808785617351532, 0.9681317806243896, 0.5173583626747131, 0.4847613573074341, 0.3849559724330902, 0.365609347820282, 0.3903164863586426], "token_confidences_second": null, "final_mean_entropy": 1.3146190719678998, "final_min_margin": 0.13409423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["East timor."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "used_ocr": false, "answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "answer_second": null, "raw_answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_second": null, "mean_entropy_first": 1.3146190719678998, "normalized_entropy_first": 1.3164596951669738, "min_margin_first": 0.13409423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1354, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1355, "total_latency_s": 1.355, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7349413633346558, 0.162850022315979, 1.9209182262420654, 0.18312187492847443, 0.862703800201416, 0.6214748620986938, 0.3536972403526306, 1.5592206716537476, 2.976860523223877, 1.6422491073608398, 1.9868278503417969, 0.010194558650255203, 1.368729829788208, 0.8842310905456543, 0.19660961627960205, 1.1604431867599487, 2.063007354736328, 2.4845962524414062, 2.277324676513672, 2.842379331588745], "entropies_second": null, "final_normalized_entropy": 1.3164596951669738, "sequence_confidence_first": 0.5673132945554455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5673132945554455, "token_confidences_first": [0.5774587988853455, 0.9692534804344177, 0.27598297595977783, 0.9656708836555481, 0.811630129814148, 0.8936455845832825, 0.9068335294723511, 0.584604024887085, 0.206391841173172, 0.5076341032981873, 0.3537214398384094, 0.9986785054206848, 0.5207311511039734, 0.808785617351532, 0.9681317806243896, 0.5173583626747131, 0.4847613573074341, 0.3849559724330902, 0.365609347820282, 0.3903164863586426], "token_confidences_second": null, "final_mean_entropy": 1.3146190719678998, "final_min_margin": 0.13409423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.545454545454545, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Red."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "used_ocr": false, "answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "answer_second": null, "raw_answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_second": null, "mean_entropy_first": 1.3146190719678998, "normalized_entropy_first": 1.1529834216347274, "min_margin_first": 0.13409423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1356, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1357, "total_latency_s": 1.357, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7349413633346558, 0.162850022315979, 1.9209182262420654, 0.18312187492847443, 0.862703800201416, 0.6214748620986938, 0.3536972403526306, 1.5592206716537476, 2.976860523223877, 1.6422491073608398, 1.9868278503417969, 0.010194558650255203, 1.368729829788208, 0.8842310905456543, 0.19660961627960205, 1.1604431867599487, 2.063007354736328, 2.4845962524414062, 2.277324676513672, 2.842379331588745], "entropies_second": null, "final_normalized_entropy": 1.1529834216347274, "sequence_confidence_first": 0.5673132945554455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5673132945554455, "token_confidences_first": [0.5774587988853455, 0.9692534804344177, 0.27598297595977783, 0.9656708836555481, 0.811630129814148, 0.8936455845832825, 0.9068335294723511, 0.584604024887085, 0.206391841173172, 0.5076341032981873, 0.3537214398384094, 0.9986785054206848, 0.5207311511039734, 0.808785617351532, 0.9681317806243896, 0.5173583626747131, 0.4847613573074341, 0.3849559724330902, 0.365609347820282, 0.3903164863586426], "token_confidences_second": null, "final_mean_entropy": 1.3146190719678998, "final_min_margin": 0.13409423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 25.0, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Central australia."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "used_ocr": false, "answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "answer_second": null, "raw_answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_second": null, "mean_entropy_first": 1.3146190719678998, "normalized_entropy_first": 1.0276409986438961, "min_margin_first": 0.13409423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1355, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1356, "total_latency_s": 1.356, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7349413633346558, 0.162850022315979, 1.9209182262420654, 0.18312187492847443, 0.862703800201416, 0.6214748620986938, 0.3536972403526306, 1.5592206716537476, 2.976860523223877, 1.6422491073608398, 1.9868278503417969, 0.010194558650255203, 1.368729829788208, 0.8842310905456543, 0.19660961627960205, 1.1604431867599487, 2.063007354736328, 2.4845962524414062, 2.277324676513672, 2.842379331588745], "entropies_second": null, "final_normalized_entropy": 1.0276409986438961, "sequence_confidence_first": 0.5673132945554455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5673132945554455, "token_confidences_first": [0.5774587988853455, 0.9692534804344177, 0.27598297595977783, 0.9656708836555481, 0.811630129814148, 0.8936455845832825, 0.9068335294723511, 0.584604024887085, 0.206391841173172, 0.5076341032981873, 0.3537214398384094, 0.9986785054206848, 0.5207311511039734, 0.808785617351532, 0.9681317806243896, 0.5173583626747131, 0.4847613573074341, 0.3849559724330902, 0.365609347820282, 0.3903164863586426], "token_confidences_second": null, "final_mean_entropy": 1.3146190719678998, "final_min_margin": 0.13409423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.055555555555555, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["New guinea."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "used_ocr": false, "answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "answer_second": null, "raw_answer": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_first": "The image shows a map of Australia with various states highlighted in different colors. The states are l", "raw_answer_second": null, "mean_entropy_first": 1.3146190719678998, "normalized_entropy_first": 0.927177231706153, "min_margin_first": 0.13409423828125, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1353, "total_latency_s": 1.353, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7349413633346558, 0.162850022315979, 1.9209182262420654, 0.18312187492847443, 0.862703800201416, 0.6214748620986938, 0.3536972403526306, 1.5592206716537476, 2.976860523223877, 1.6422491073608398, 1.9868278503417969, 0.010194558650255203, 1.368729829788208, 0.8842310905456543, 0.19660961627960205, 1.1604431867599487, 2.063007354736328, 2.4845962524414062, 2.277324676513672, 2.842379331588745], "entropies_second": null, "final_normalized_entropy": 0.927177231706153, "sequence_confidence_first": 0.5673132945554455, "sequence_confidence_second": null, "sequence_confidence_final": 0.5673132945554455, "token_confidences_first": [0.5774587988853455, 0.9692534804344177, 0.27598297595977783, 0.9656708836555481, 0.811630129814148, 0.8936455845832825, 0.9068335294723511, 0.584604024887085, 0.206391841173172, 0.5076341032981873, 0.3537214398384094, 0.9986785054206848, 0.5207311511039734, 0.808785617351532, 0.9681317806243896, 0.5173583626747131, 0.4847613573074341, 0.3849559724330902, 0.365609347820282, 0.3903164863586426], "token_confidences_second": null, "final_mean_entropy": 1.3146190719678998, "final_min_margin": 0.13409423828125, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.727272727272727, "wer": 9.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Myth."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "used_ocr": false, "answer_first": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "answer_second": null, "raw_answer": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "raw_answer_first": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "raw_answer_second": null, "mean_entropy_first": 0.8686588667565956, "normalized_entropy_first": -0.4031905742033764, "min_margin_first": 0.15727806091308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1351, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1352, "total_latency_s": 1.352, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8142921924591064, 0.05126446485519409, 1.7283592224121094, 0.25974762439727783, 0.002156245056539774, 0.6620790362358093, 0.720895528793335, 0.9783527255058289, 2.80318546295166, 0.41164031624794006, 3.0711052417755127, 1.8559670448303223, 1.491699457168579, 0.04302974417805672, 0.7913778424263, 0.6932796835899353, 0.06228502839803696, 0.05755957216024399, 0.002544248476624489, 0.872356653213501], "entropies_second": null, "final_normalized_entropy": -0.4031905742033764, "sequence_confidence_first": 0.6587577181120329, "sequence_confidence_second": null, "sequence_confidence_final": 0.6587577181120329, "token_confidences_first": [0.592969536781311, 0.9938936829566956, 0.3642406165599823, 0.9421387910842896, 0.9997904896736145, 0.7259626984596252, 0.6477243900299072, 0.6943817138671875, 0.25649726390838623, 0.8577451109886169, 0.16681939363479614, 0.5798351168632507, 0.6755596399307251, 0.994355320930481, 0.7160751223564148, 0.5541175007820129, 0.9900354743003845, 0.9919856786727905, 0.9997491240501404, 0.6442018151283264], "token_confidences_second": null, "final_mean_entropy": 0.8686588667565956, "final_min_margin": 0.15727806091308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.4, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Extended bushfire season."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "used_ocr": false, "answer_first": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "answer_second": null, "raw_answer": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "raw_answer_first": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "raw_answer_second": null, "mean_entropy_first": 0.8686588667565956, "normalized_entropy_first": -0.37942855125659636, "min_margin_first": 0.15727806091308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1351, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1352, "total_latency_s": 1.352, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8142921924591064, 0.05126446485519409, 1.7283592224121094, 0.25974762439727783, 0.002156245056539774, 0.6620790362358093, 0.720895528793335, 0.9783527255058289, 2.80318546295166, 0.41164031624794006, 3.0711052417755127, 1.8559670448303223, 1.491699457168579, 0.04302974417805672, 0.7913778424263, 0.6932796835899353, 0.06228502839803696, 0.05755957216024399, 0.002544248476624489, 0.872356653213501], "entropies_second": null, "final_normalized_entropy": -0.37942855125659636, "sequence_confidence_first": 0.6587577181120329, "sequence_confidence_second": null, "sequence_confidence_final": 0.6587577181120329, "token_confidences_first": [0.592969536781311, 0.9938936829566956, 0.3642406165599823, 0.9421387910842896, 0.9997904896736145, 0.7259626984596252, 0.6477243900299072, 0.6943817138671875, 0.25649726390838623, 0.8577451109886169, 0.16681939363479614, 0.5798351168632507, 0.6755596399307251, 0.994355320930481, 0.7160751223564148, 0.5541175007820129, 0.9900354743003845, 0.9919856786727905, 0.9997491240501404, 0.6442018151283264], "token_confidences_second": null, "final_mean_entropy": 0.8686588667565956, "final_min_margin": 0.15727806091308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.16, "wer": 3.6666666666666665, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Climate change."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "used_ocr": false, "answer_first": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "answer_second": null, "raw_answer": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "raw_answer_first": "The image you've provided is a graphic or poster titled \"BUSHFIRES", "raw_answer_second": null, "mean_entropy_first": 0.8686588667565956, "normalized_entropy_first": -0.35739409225729496, "min_margin_first": 0.15727806091308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1353, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1354, "total_latency_s": 1.354, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8142921924591064, 0.05126446485519409, 1.7283592224121094, 0.25974762439727783, 0.002156245056539774, 0.6620790362358093, 0.720895528793335, 0.9783527255058289, 2.80318546295166, 0.41164031624794006, 3.0711052417755127, 1.8559670448303223, 1.491699457168579, 0.04302974417805672, 0.7913778424263, 0.6932796835899353, 0.06228502839803696, 0.05755957216024399, 0.002544248476624489, 0.872356653213501], "entropies_second": null, "final_normalized_entropy": -0.35739409225729496, "sequence_confidence_first": 0.6587577181120329, "sequence_confidence_second": null, "sequence_confidence_final": 0.6587577181120329, "token_confidences_first": [0.592969536781311, 0.9938936829566956, 0.3642406165599823, 0.9421387910842896, 0.9997904896736145, 0.7259626984596252, 0.6477243900299072, 0.6943817138671875, 0.25649726390838623, 0.8577451109886169, 0.16681939363479614, 0.5798351168632507, 0.6755596399307251, 0.994355320930481, 0.7160751223564148, 0.5541175007820129, 0.9900354743003845, 0.9919856786727905, 0.9997491240501404, 0.6442018151283264], "token_confidences_second": null, "final_mean_entropy": 0.8686588667565956, "final_min_margin": 0.15727806091308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 3.8666666666666667, "wer": 5.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Yellow."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "used_ocr": false, "answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "answer_second": null, "raw_answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_second": null, "mean_entropy_first": 0.8298927700030617, "normalized_entropy_first": -0.46118462596934234, "min_margin_first": 0.024763107299804688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1764, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1765, "total_latency_s": 1.765, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.001860499382019, 0.06256937235593796, 1.8200347423553467, 0.28876346349716187, 0.0018439625855535269, 0.7230926752090454, 1.0915381908416748, 0.8363824486732483, 1.052396297454834, 0.5430541634559631, 1.1601608991622925, 0.017886295914649963, 1.645620584487915, 0.841214656829834, 0.21092823147773743, 1.3058236837387085, 0.9993787407875061, 1.5597383975982666, 0.27231478691101074, 1.1632533073425293], "entropies_second": null, "final_normalized_entropy": -0.46118462596934234, "sequence_confidence_first": 0.6854435659321726, "sequence_confidence_second": null, "sequence_confidence_final": 0.6854435659321726, "token_confidences_first": [0.5293982625007629, 0.9928747415542603, 0.4035109281539917, 0.9342373609542847, 0.9998247027397156, 0.6302323937416077, 0.4651070833206177, 0.5416966676712036, 0.6284776926040649, 0.7706218361854553, 0.6551491618156433, 0.9975540041923523, 0.6606760025024414, 0.7566491961479187, 0.9552643299102783, 0.6273900866508484, 0.5463742017745972, 0.5951281785964966, 0.940872848033905, 0.574284553527832], "token_confidences_second": null, "final_mean_entropy": 0.8298927700030617, "final_min_margin": 0.024763107299804688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.142857142857142, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["New South Wales."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "used_ocr": false, "answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "answer_second": null, "raw_answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_second": null, "mean_entropy_first": 0.8298927700030617, "normalized_entropy_first": -0.4329382671212804, "min_margin_first": 0.024763107299804688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1185, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1186, "total_latency_s": 1.186, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.001860499382019, 0.06256937235593796, 1.8200347423553467, 0.28876346349716187, 0.0018439625855535269, 0.7230926752090454, 1.0915381908416748, 0.8363824486732483, 1.052396297454834, 0.5430541634559631, 1.1601608991622925, 0.017886295914649963, 1.645620584487915, 0.841214656829834, 0.21092823147773743, 1.3058236837387085, 0.9993787407875061, 1.5597383975982666, 0.27231478691101074, 1.1632533073425293], "entropies_second": null, "final_normalized_entropy": -0.4329382671212804, "sequence_confidence_first": 0.6854435659321726, "sequence_confidence_second": null, "sequence_confidence_final": 0.6854435659321726, "token_confidences_first": [0.5293982625007629, 0.9928747415542603, 0.4035109281539917, 0.9342373609542847, 0.9998247027397156, 0.6302323937416077, 0.4651070833206177, 0.5416966676712036, 0.6284776926040649, 0.7706218361854553, 0.6551491618156433, 0.9975540041923523, 0.6606760025024414, 0.7566491961479187, 0.9552643299102783, 0.6273900866508484, 0.5463742017745972, 0.5951281785964966, 0.940872848033905, 0.574284553527832], "token_confidences_second": null, "final_mean_entropy": 0.8298927700030617, "final_min_margin": 0.024763107299804688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.3125, "wer": 5.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Green."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "used_ocr": false, "answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "answer_second": null, "raw_answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_second": null, "mean_entropy_first": 0.8298927700030617, "normalized_entropy_first": -0.40692539362839814, "min_margin_first": 0.024763107299804688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1185, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1186, "total_latency_s": 1.186, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.001860499382019, 0.06256937235593796, 1.8200347423553467, 0.28876346349716187, 0.0018439625855535269, 0.7230926752090454, 1.0915381908416748, 0.8363824486732483, 1.052396297454834, 0.5430541634559631, 1.1601608991622925, 0.017886295914649963, 1.645620584487915, 0.841214656829834, 0.21092823147773743, 1.3058236837387085, 0.9993787407875061, 1.5597383975982666, 0.27231478691101074, 1.1632533073425293], "entropies_second": null, "final_normalized_entropy": -0.40692539362839814, "sequence_confidence_first": 0.6854435659321726, "sequence_confidence_second": null, "sequence_confidence_final": 0.6854435659321726, "token_confidences_first": [0.5293982625007629, 0.9928747415542603, 0.4035109281539917, 0.9342373609542847, 0.9998247027397156, 0.6302323937416077, 0.4651070833206177, 0.5416966676712036, 0.6284776926040649, 0.7706218361854553, 0.6551491618156433, 0.9975540041923523, 0.6606760025024414, 0.7566491961479187, 0.9552643299102783, 0.6273900866508484, 0.5463742017745972, 0.5951281785964966, 0.940872848033905, 0.574284553527832], "token_confidences_second": null, "final_mean_entropy": 0.8298927700030617, "final_min_margin": 0.024763107299804688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 15.166666666666666, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Tasmania."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "used_ocr": false, "answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "answer_second": null, "raw_answer": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_first": "The image you've provided is an informative graphic about Australian wine facts and regions. Here", "raw_answer_second": null, "mean_entropy_first": 0.8298927700030617, "normalized_entropy_first": -0.3828862662872733, "min_margin_first": 0.024763107299804688, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1186, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1187, "total_latency_s": 1.187, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.001860499382019, 0.06256937235593796, 1.8200347423553467, 0.28876346349716187, 0.0018439625855535269, 0.7230926752090454, 1.0915381908416748, 0.8363824486732483, 1.052396297454834, 0.5430541634559631, 1.1601608991622925, 0.017886295914649963, 1.645620584487915, 0.841214656829834, 0.21092823147773743, 1.3058236837387085, 0.9993787407875061, 1.5597383975982666, 0.27231478691101074, 1.1632533073425293], "entropies_second": null, "final_normalized_entropy": -0.3828862662872733, "sequence_confidence_first": 0.6854435659321726, "sequence_confidence_second": null, "sequence_confidence_final": 0.6854435659321726, "token_confidences_first": [0.5293982625007629, 0.9928747415542603, 0.4035109281539917, 0.9342373609542847, 0.9998247027397156, 0.6302323937416077, 0.4651070833206177, 0.5416966676712036, 0.6284776926040649, 0.7706218361854553, 0.6551491618156433, 0.9975540041923523, 0.6606760025024414, 0.7566491961479187, 0.9552643299102783, 0.6273900866508484, 0.5463742017745972, 0.5951281785964966, 0.940872848033905, 0.574284553527832], "token_confidences_second": null, "final_mean_entropy": 0.8298927700030617, "final_min_margin": 0.024763107299804688, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.777777777777779, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["3."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "used_ocr": false, "answer_first": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "answer_second": null, "raw_answer": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "raw_answer_first": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "raw_answer_second": null, "mean_entropy_first": 1.0141244668047875, "normalized_entropy_first": 0.34325256714016317, "min_margin_first": 0.026973724365234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1131, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1131, "total_latency_s": 1.131, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.5531728267669678, 0.10949474573135376, 2.0258028507232666, 0.2932913303375244, 0.0038751615211367607, 0.820647656917572, 1.2385015487670898, 1.037757396697998, 2.730821132659912, 1.2637410163879395, 1.952164649963379, 0.011304784566164017, 0.7706395387649536, 0.05010475963354111, 0.582761824131012, 0.9268194437026978, 0.8173073530197144, 0.2063753604888916, 2.127666473388672, 1.7602394819259644], "entropies_second": null, "final_normalized_entropy": 0.34325256714016317, "sequence_confidence_first": 0.6533547110695123, "sequence_confidence_second": null, "sequence_confidence_final": 0.6533547110695123, "token_confidences_first": [0.4206782579421997, 0.9873577356338501, 0.2946341931819916, 0.9358164072036743, 0.9995893836021423, 0.5188220739364624, 0.5480608940124512, 0.6454442739486694, 0.36038434505462646, 0.6082189679145813, 0.5780903100967407, 0.9986897110939026, 0.8607754707336426, 0.9943457245826721, 0.8439458012580872, 0.7545124292373657, 0.6919086575508118, 0.9666356444358826, 0.3580183684825897, 0.5789982080459595], "token_confidences_second": null, "final_mean_entropy": 1.0141244668047875, "final_min_margin": 0.026973724365234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 47.0, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Who is Jesus."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "used_ocr": false, "answer_first": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "answer_second": null, "raw_answer": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "raw_answer_first": "The image you've provided is a fun facts infographic about Australia. Here are the facts listed", "raw_answer_second": null, "mean_entropy_first": 1.0141244668047875, "normalized_entropy_first": 0.3237363935561532, "min_margin_first": 0.026973724365234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1132, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1133, "total_latency_s": 1.133, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.5531728267669678, 0.10949474573135376, 2.0258028507232666, 0.2932913303375244, 0.0038751615211367607, 0.820647656917572, 1.2385015487670898, 1.037757396697998, 2.730821132659912, 1.2637410163879395, 1.952164649963379, 0.011304784566164017, 0.7706395387649536, 0.05010475963354111, 0.582761824131012, 0.9268194437026978, 0.8173073530197144, 0.2063753604888916, 2.127666473388672, 1.7602394819259644], "entropies_second": null, "final_normalized_entropy": 0.3237363935561532, "sequence_confidence_first": 0.6533547110695123, "sequence_confidence_second": null, "sequence_confidence_final": 0.6533547110695123, "token_confidences_first": [0.4206782579421997, 0.9873577356338501, 0.2946341931819916, 0.9358164072036743, 0.9995893836021423, 0.5188220739364624, 0.5480608940124512, 0.6454442739486694, 0.36038434505462646, 0.6082189679145813, 0.5780903100967407, 0.9986897110939026, 0.8607754707336426, 0.9943457245826721, 0.8439458012580872, 0.7545124292373657, 0.6919086575508118, 0.9666356444358826, 0.3580183684825897, 0.5789982080459595], "token_confidences_second": null, "final_mean_entropy": 1.0141244668047875, "final_min_margin": 0.026973724365234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.538461538461538, "wer": 5.0, "precision": 0.0625, "recall": 0.3333333333333333, "f1": 0.10526315789473684, "rouge_l": 0.10526315789473684, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["SA."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or informational graphic related to energy", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or informational graphic related to energy", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or informational graphic related to energy", "raw_answer_first": "The image you've provided appears to be an infographic or informational graphic related to energy", "raw_answer_second": null, "mean_entropy_first": 0.8669331435179629, "normalized_entropy_first": -0.3124218195710842, "min_margin_first": 0.044795989990234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1189, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1190, "total_latency_s": 1.19, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9570178985595703, 0.09549200534820557, 1.843093991279602, 0.25972190499305725, 0.0031696909572929144, 0.7562444806098938, 1.2510576248168945, 0.00013827432121615857, 0.05728522315621376, 0.6510434746742249, 1.0712045431137085, 0.010345036163926125, 2.6181201934814453, 2.266580581665039, 0.609995424747467, 0.9275761842727661, 0.48092567920684814, 2.369964599609375, 0.0002870983153115958, 1.1093989610671997], "entropies_second": null, "final_normalized_entropy": -0.3124218195710842, "sequence_confidence_first": 0.6325522701973235, "sequence_confidence_second": null, "sequence_confidence_final": 0.6325522701973235, "token_confidences_first": [0.5147491693496704, 0.9869340658187866, 0.36079737544059753, 0.94316166639328, 0.9996809959411621, 0.609290361404419, 0.45062872767448425, 0.9999911785125732, 0.9927050471305847, 0.7134047150611877, 0.53598552942276, 0.9988407492637634, 0.3458375632762909, 0.1954226940870285, 0.7106025218963623, 0.6386085152626038, 0.9242627620697021, 0.2826231122016907, 0.9999804496765137, 0.7297691702842712], "token_confidences_second": null, "final_mean_entropy": 0.8669331435179629, "final_min_margin": 0.044795989990234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 31.666666666666668, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["5."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or informational graphic related to energy", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or informational graphic related to energy", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or informational graphic related to energy", "raw_answer_first": "The image you've provided appears to be an infographic or informational graphic related to energy", "raw_answer_second": null, "mean_entropy_first": 0.8669331435179629, "normalized_entropy_first": -0.2949533762250988, "min_margin_first": 0.044795989990234375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1189, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1190, "total_latency_s": 1.19, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9570178985595703, 0.09549200534820557, 1.843093991279602, 0.25972190499305725, 0.0031696909572929144, 0.7562444806098938, 1.2510576248168945, 0.00013827432121615857, 0.05728522315621376, 0.6510434746742249, 1.0712045431137085, 0.010345036163926125, 2.6181201934814453, 2.266580581665039, 0.609995424747467, 0.9275761842727661, 0.48092567920684814, 2.369964599609375, 0.0002870983153115958, 1.1093989610671997], "entropies_second": null, "final_normalized_entropy": -0.2949533762250988, "sequence_confidence_first": 0.6325522701973235, "sequence_confidence_second": null, "sequence_confidence_final": 0.6325522701973235, "token_confidences_first": [0.5147491693496704, 0.9869340658187866, 0.36079737544059753, 0.94316166639328, 0.9996809959411621, 0.609290361404419, 0.45062872767448425, 0.9999911785125732, 0.9927050471305847, 0.7134047150611877, 0.53598552942276, 0.9988407492637634, 0.3458375632762909, 0.1954226940870285, 0.7106025218963623, 0.6386085152626038, 0.9242627620697021, 0.2826231122016907, 0.9999804496765137, 0.7297691702842712], "token_confidences_second": null, "final_mean_entropy": 0.8669331435179629, "final_min_margin": 0.044795989990234375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 48.5, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["270 km."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_second": null, "mean_entropy_first": 0.5474137754470576, "normalized_entropy_first": -1.7554512593298601, "min_margin_first": 0.040019989013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1382, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1382, "total_latency_s": 1.382, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8068023324012756, 0.0987272709608078, 1.4951372146606445, 0.26202985644340515, 0.003455757861956954, 0.6334844827651978, 0.819364607334137, 0.8111054301261902, 0.6671940088272095, 0.012493744492530823, 1.2955360412597656, 0.0077324677258729935, 0.8641110062599182, 1.078228235244751, 0.09413132071495056, 0.007246444933116436, 0.009186366572976112, 0.00027066085021942854, 1.9140779972076416, 0.06796026229858398], "entropies_second": null, "final_normalized_entropy": -1.7554512593298601, "sequence_confidence_first": 0.7689050820569536, "sequence_confidence_second": null, "sequence_confidence_final": 0.7689050820569536, "token_confidences_first": [0.5388403534889221, 0.9852651953697205, 0.47945013642311096, 0.9420515298843384, 0.9996501207351685, 0.7242646813392639, 0.5251955986022949, 0.6165732145309448, 0.7643243074417114, 0.9985243678092957, 0.7374299168586731, 0.9992478489875793, 0.8408684730529785, 0.47638899087905884, 0.9882146120071411, 0.9992737174034119, 0.999190628528595, 0.9999798536300659, 0.42221227288246155, 0.9888852834701538], "token_confidences_second": null, "final_mean_entropy": 0.5474137754470576, "final_min_margin": 0.040019989013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.714285714285714, "wer": 5.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["13 km."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_second": null, "mean_entropy_first": 0.5474137754470576, "normalized_entropy_first": -1.4560605861484917, "min_margin_first": 0.040019989013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1390, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1390, "total_latency_s": 1.39, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8068023324012756, 0.0987272709608078, 1.4951372146606445, 0.26202985644340515, 0.003455757861956954, 0.6334844827651978, 0.819364607334137, 0.8111054301261902, 0.6671940088272095, 0.012493744492530823, 1.2955360412597656, 0.0077324677258729935, 0.8641110062599182, 1.078228235244751, 0.09413132071495056, 0.007246444933116436, 0.009186366572976112, 0.00027066085021942854, 1.9140779972076416, 0.06796026229858398], "entropies_second": null, "final_normalized_entropy": -1.4560605861484917, "sequence_confidence_first": 0.7689050820569536, "sequence_confidence_second": null, "sequence_confidence_final": 0.7689050820569536, "token_confidences_first": [0.5388403534889221, 0.9852651953697205, 0.47945013642311096, 0.9420515298843384, 0.9996501207351685, 0.7242646813392639, 0.5251955986022949, 0.6165732145309448, 0.7643243074417114, 0.9985243678092957, 0.7374299168586731, 0.9992478489875793, 0.8408684730529785, 0.47638899087905884, 0.9882146120071411, 0.9992737174034119, 0.999190628528595, 0.9999798536300659, 0.42221227288246155, 0.9888852834701538], "token_confidences_second": null, "final_mean_entropy": 0.5474137754470576, "final_min_margin": 0.040019989013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 11.333333333333334, "wer": 5.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["3500 km2."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_second": null, "mean_entropy_first": 0.5474137754470576, "normalized_entropy_first": -1.254721606416914, "min_margin_first": 0.040019989013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1384, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1385, "total_latency_s": 1.385, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8068023324012756, 0.0987272709608078, 1.4951372146606445, 0.26202985644340515, 0.003455757861956954, 0.6334844827651978, 0.819364607334137, 0.8111054301261902, 0.6671940088272095, 0.012493744492530823, 1.2955360412597656, 0.0077324677258729935, 0.8641110062599182, 1.078228235244751, 0.09413132071495056, 0.007246444933116436, 0.009186366572976112, 0.00027066085021942854, 1.9140779972076416, 0.06796026229858398], "entropies_second": null, "final_normalized_entropy": -1.254721606416914, "sequence_confidence_first": 0.7689050820569536, "sequence_confidence_second": null, "sequence_confidence_final": 0.7689050820569536, "token_confidences_first": [0.5388403534889221, 0.9852651953697205, 0.47945013642311096, 0.9420515298843384, 0.9996501207351685, 0.7242646813392639, 0.5251955986022949, 0.6165732145309448, 0.7643243074417114, 0.9985243678092957, 0.7374299168586731, 0.9992478489875793, 0.8408684730529785, 0.47638899087905884, 0.9882146120071411, 0.9992737174034119, 0.999190628528595, 0.9999798536300659, 0.42221227288246155, 0.9888852834701538], "token_confidences_second": null, "final_mean_entropy": 0.5474137754470576, "final_min_margin": 0.040019989013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 7.555555555555555, "wer": 5.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1,354 BILLION LITRES."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_second": null, "mean_entropy_first": 0.5474137754470576, "normalized_entropy_first": -1.1064219176441357, "min_margin_first": 0.040019989013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1380, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1382, "total_latency_s": 1.382, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8068023324012756, 0.0987272709608078, 1.4951372146606445, 0.26202985644340515, 0.003455757861956954, 0.6334844827651978, 0.819364607334137, 0.8111054301261902, 0.6671940088272095, 0.012493744492530823, 1.2955360412597656, 0.0077324677258729935, 0.8641110062599182, 1.078228235244751, 0.09413132071495056, 0.007246444933116436, 0.009186366572976112, 0.00027066085021942854, 1.9140779972076416, 0.06796026229858398], "entropies_second": null, "final_normalized_entropy": -1.1064219176441357, "sequence_confidence_first": 0.7689050820569536, "sequence_confidence_second": null, "sequence_confidence_final": 0.7689050820569536, "token_confidences_first": [0.5388403534889221, 0.9852651953697205, 0.47945013642311096, 0.9420515298843384, 0.9996501207351685, 0.7242646813392639, 0.5251955986022949, 0.6165732145309448, 0.7643243074417114, 0.9985243678092957, 0.7374299168586731, 0.9992478489875793, 0.8408684730529785, 0.47638899087905884, 0.9882146120071411, 0.9992737174034119, 0.999190628528595, 0.9999798536300659, 0.42221227288246155, 0.9888852834701538], "token_confidences_second": null, "final_mean_entropy": 0.5474137754470576, "final_min_margin": 0.040019989013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.9047619047619047, "wer": 3.6666666666666665, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Cattle."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_second": null, "mean_entropy_first": 0.5474137754470576, "normalized_entropy_first": -0.9907519084709154, "min_margin_first": 0.040019989013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1381, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1383, "total_latency_s": 1.383, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8068023324012756, 0.0987272709608078, 1.4951372146606445, 0.26202985644340515, 0.003455757861956954, 0.6334844827651978, 0.819364607334137, 0.8111054301261902, 0.6671940088272095, 0.012493744492530823, 1.2955360412597656, 0.0077324677258729935, 0.8641110062599182, 1.078228235244751, 0.09413132071495056, 0.007246444933116436, 0.009186366572976112, 0.00027066085021942854, 1.9140779972076416, 0.06796026229858398], "entropies_second": null, "final_normalized_entropy": -0.9907519084709154, "sequence_confidence_first": 0.7689050820569536, "sequence_confidence_second": null, "sequence_confidence_final": 0.7689050820569536, "token_confidences_first": [0.5388403534889221, 0.9852651953697205, 0.47945013642311096, 0.9420515298843384, 0.9996501207351685, 0.7242646813392639, 0.5251955986022949, 0.6165732145309448, 0.7643243074417114, 0.9985243678092957, 0.7374299168586731, 0.9992478489875793, 0.8408684730529785, 0.47638899087905884, 0.9882146120071411, 0.9992737174034119, 0.999190628528595, 0.9999798536300659, 0.42221227288246155, 0.9888852834701538], "token_confidences_second": null, "final_mean_entropy": 0.5474137754470576, "final_min_margin": 0.040019989013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.285714285714286, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["9."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_first": "The image you've provided is an infographic titled \"Galilee Basin Mega", "raw_answer_second": null, "mean_entropy_first": 0.5474137754470576, "normalized_entropy_first": -0.8969197199924022, "min_margin_first": 0.040019989013671875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1380, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1381, "total_latency_s": 1.381, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8068023324012756, 0.0987272709608078, 1.4951372146606445, 0.26202985644340515, 0.003455757861956954, 0.6334844827651978, 0.819364607334137, 0.8111054301261902, 0.6671940088272095, 0.012493744492530823, 1.2955360412597656, 0.0077324677258729935, 0.8641110062599182, 1.078228235244751, 0.09413132071495056, 0.007246444933116436, 0.009186366572976112, 0.00027066085021942854, 1.9140779972076416, 0.06796026229858398], "entropies_second": null, "final_normalized_entropy": -0.8969197199924022, "sequence_confidence_first": 0.7689050820569536, "sequence_confidence_second": null, "sequence_confidence_final": 0.7689050820569536, "token_confidences_first": [0.5388403534889221, 0.9852651953697205, 0.47945013642311096, 0.9420515298843384, 0.9996501207351685, 0.7242646813392639, 0.5251955986022949, 0.6165732145309448, 0.7643243074417114, 0.9985243678092957, 0.7374299168586731, 0.9992478489875793, 0.8408684730529785, 0.47638899087905884, 0.9882146120071411, 0.9992737174034119, 0.999190628528595, 0.9999798536300659, 0.42221227288246155, 0.9888852834701538], "token_confidences_second": null, "final_mean_entropy": 0.5474137754470576, "final_min_margin": 0.040019989013671875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 35.0, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["23%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a graphic illustration that presents", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a graphic illustration that presents", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a graphic illustration that presents", "raw_answer_first": "The image you've provided appears to be an infographic or a graphic illustration that presents", "raw_answer_second": null, "mean_entropy_first": 1.303466254088562, "normalized_entropy_first": 2.2477245163767288, "min_margin_first": 0.009426116943359375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1195, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1196, "total_latency_s": 1.196, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6544710993766785, 0.15060561895370483, 2.0230040550231934, 0.3357173502445221, 0.0029523654375225306, 0.669367253780365, 1.3558452129364014, 0.0002582636661827564, 0.10882924497127533, 0.753300130367279, 0.6894080638885498, 0.007318270392715931, 2.6705493927001953, 2.437006950378418, 3.252044200897217, 1.8461633920669556, 3.252434492111206, 0.5054879188537598, 2.8687386512756348, 2.485823154449463], "entropies_second": null, "final_normalized_entropy": 2.2477245163767288, "sequence_confidence_first": 0.5064227713250882, "sequence_confidence_second": null, "sequence_confidence_final": 0.5064227713250882, "token_confidences_first": [0.7667481303215027, 0.9755673408508301, 0.25258558988571167, 0.917538046836853, 0.9997027516365051, 0.7161862254142761, 0.4108809530735016, 0.9999823570251465, 0.9855653643608093, 0.5885053873062134, 0.7761658430099487, 0.9991818070411682, 0.26346254348754883, 0.27900248765945435, 0.24432729184627533, 0.3716854751110077, 0.1549377292394638, 0.8097168207168579, 0.24621263146400452, 0.2602594792842865], "token_confidences_second": null, "final_mean_entropy": 1.303466254088562, "final_min_margin": 0.009426116943359375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.5, "wer": 15.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["8.5 - 9.3 YEARS."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a graphic illustration that presents", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a graphic illustration that presents", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a graphic illustration that presents", "raw_answer_first": "The image you've provided appears to be an infographic or a graphic illustration that presents", "raw_answer_second": null, "mean_entropy_first": 1.303466254088562, "normalized_entropy_first": 1.7380545470941562, "min_margin_first": 0.009426116943359375, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1194, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1195, "total_latency_s": 1.195, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.6544710993766785, 0.15060561895370483, 2.0230040550231934, 0.3357173502445221, 0.0029523654375225306, 0.669367253780365, 1.3558452129364014, 0.0002582636661827564, 0.10882924497127533, 0.753300130367279, 0.6894080638885498, 0.007318270392715931, 2.6705493927001953, 2.437006950378418, 3.252044200897217, 1.8461633920669556, 3.252434492111206, 0.5054879188537598, 2.8687386512756348, 2.485823154449463], "entropies_second": null, "final_normalized_entropy": 1.7380545470941562, "sequence_confidence_first": 0.5064227713250882, "sequence_confidence_second": null, "sequence_confidence_final": 0.5064227713250882, "token_confidences_first": [0.7667481303215027, 0.9755673408508301, 0.25258558988571167, 0.917538046836853, 0.9997027516365051, 0.7161862254142761, 0.4108809530735016, 0.9999823570251465, 0.9855653643608093, 0.5885053873062134, 0.7761658430099487, 0.9991818070411682, 0.26346254348754883, 0.27900248765945435, 0.24432729184627533, 0.3716854751110077, 0.1549377292394638, 0.8097168207168579, 0.24621263146400452, 0.2602594792842865], "token_confidences_second": null, "final_mean_entropy": 1.303466254088562, "final_min_margin": 0.009426116943359375, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.4375, "wer": 3.75, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Buddhism."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "raw_answer_second": null, "mean_entropy_first": 1.3441801791079342, "normalized_entropy_first": 1.5760443180616612, "min_margin_first": 0.06881141662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1347, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7661912441253662, 1.2176121473312378, 1.1210688352584839, 0.6469526886940002, 2.6778738498687744, 0.013664239086210728, 0.9819353818893433, 0.010955474339425564, 1.983842134475708, 1.2057815790176392, 1.8845341205596924, 1.5813360214233398, 0.010489160194993019, 1.7202746868133545, 2.0530638694763184, 1.072379231452942, 1.2961636781692505, 2.8923325538635254, 1.6024774312973022, 2.1446752548217773], "entropies_second": null, "final_normalized_entropy": 1.5760443180616612, "sequence_confidence_first": 0.5632876146236061, "sequence_confidence_second": null, "sequence_confidence_final": 0.5632876146236061, "token_confidences_first": [0.6104896068572998, 0.42748120427131653, 0.5058954954147339, 0.7134303450584412, 0.3974612057209015, 0.9984007477760315, 0.73005610704422, 0.9987119436264038, 0.5757753849029541, 0.6931533217430115, 0.3671095073223114, 0.6755707859992981, 0.9989976286888123, 0.5823500752449036, 0.27102574706077576, 0.705834150314331, 0.6161009669303894, 0.219131201505661, 0.4896831214427948, 0.5210966467857361], "token_confidences_second": null, "final_mean_entropy": 1.3441801791079342, "final_min_margin": 0.06881141662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.222222222222221, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["91."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "raw_answer_second": null, "mean_entropy_first": 1.3441801791079342, "normalized_entropy_first": 1.3381791742489477, "min_margin_first": 0.06881141662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1346, "total_latency_s": 1.346, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7661912441253662, 1.2176121473312378, 1.1210688352584839, 0.6469526886940002, 2.6778738498687744, 0.013664239086210728, 0.9819353818893433, 0.010955474339425564, 1.983842134475708, 1.2057815790176392, 1.8845341205596924, 1.5813360214233398, 0.010489160194993019, 1.7202746868133545, 2.0530638694763184, 1.072379231452942, 1.2961636781692505, 2.8923325538635254, 1.6024774312973022, 2.1446752548217773], "entropies_second": null, "final_normalized_entropy": 1.3381791742489477, "sequence_confidence_first": 0.5632876146236061, "sequence_confidence_second": null, "sequence_confidence_final": 0.5632876146236061, "token_confidences_first": [0.6104896068572998, 0.42748120427131653, 0.5058954954147339, 0.7134303450584412, 0.3974612057209015, 0.9984007477760315, 0.73005610704422, 0.9987119436264038, 0.5757753849029541, 0.6931533217430115, 0.3671095073223114, 0.6755707859992981, 0.9989976286888123, 0.5823500752449036, 0.27102574706077576, 0.705834150314331, 0.6161009669303894, 0.219131201505661, 0.4896831214427948, 0.5210966467857361], "token_confidences_second": null, "final_mean_entropy": 1.3441801791079342, "final_min_margin": 0.06881141662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 38.666666666666664, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["38.9."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "used_ocr": false, "answer_first": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "answer_second": null, "raw_answer": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "raw_answer_first": "This image is a colorful infographic that provides a snapshot of various life stages and their associated statistics", "raw_answer_second": null, "mean_entropy_first": 1.3441801791079342, "normalized_entropy_first": 1.1691367011457134, "min_margin_first": 0.06881141662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1346, "total_latency_s": 1.346, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7661912441253662, 1.2176121473312378, 1.1210688352584839, 0.6469526886940002, 2.6778738498687744, 0.013664239086210728, 0.9819353818893433, 0.010955474339425564, 1.983842134475708, 1.2057815790176392, 1.8845341205596924, 1.5813360214233398, 0.010489160194993019, 1.7202746868133545, 2.0530638694763184, 1.072379231452942, 1.2961636781692505, 2.8923325538635254, 1.6024774312973022, 2.1446752548217773], "entropies_second": null, "final_normalized_entropy": 1.1691367011457134, "sequence_confidence_first": 0.5632876146236061, "sequence_confidence_second": null, "sequence_confidence_final": 0.5632876146236061, "token_confidences_first": [0.6104896068572998, 0.42748120427131653, 0.5058954954147339, 0.7134303450584412, 0.3974612057209015, 0.9984007477760315, 0.73005610704422, 0.9987119436264038, 0.5757753849029541, 0.6931533217430115, 0.3671095073223114, 0.6755707859992981, 0.9989976286888123, 0.5823500752449036, 0.27102574706077576, 0.705834150314331, 0.6161009669303894, 0.219131201505661, 0.4896831214427948, 0.5210966467857361], "token_confidences_second": null, "final_mean_entropy": 1.3441801791079342, "final_min_margin": 0.06881141662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.2, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["31%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_second": null, "mean_entropy_first": 0.44776840738122703, "normalized_entropy_first": -1.5723324232530425, "min_margin_first": 0.04045677185058594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1228, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1229, "total_latency_s": 1.229, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8204617500305176, 0.12248177826404572, 1.8972043991088867, 0.24820363521575928, 0.0021444237791001797, 0.720757007598877, 0.9273355007171631, 0.7868642210960388, 0.268100768327713, 0.008893672376871109, 2.0570082664489746, 0.008071769028902054, 0.7773869037628174, 0.003698278684169054, 0.00016276625683531165, 0.00353431049734354, 0.0004429254331625998, 0.00018606327648740262, 0.2766133248806, 0.025816382840275764], "entropies_second": null, "final_normalized_entropy": -1.5723324232530425, "sequence_confidence_first": 0.7710618957587781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7710618957587781, "token_confidences_first": [0.5329687595367432, 0.9810047745704651, 0.2812984585762024, 0.9459823369979858, 0.9997952580451965, 0.6430691480636597, 0.4965868294239044, 0.5289799571037292, 0.9438300132751465, 0.9989691972732544, 0.4275483787059784, 0.9991838335990906, 0.6220735907554626, 0.9996532201766968, 0.99998939037323, 0.9996871948242188, 0.9999699592590332, 0.9999860525131226, 0.9410784840583801, 0.9966082572937012], "token_confidences_second": null, "final_mean_entropy": 0.44776840738122703, "final_min_margin": 0.04045677185058594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 16.75, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["58%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_second": null, "mean_entropy_first": 0.44776840738122703, "normalized_entropy_first": -1.3356528107444823, "min_margin_first": 0.04045677185058594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1229, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1230, "total_latency_s": 1.23, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8204617500305176, 0.12248177826404572, 1.8972043991088867, 0.24820363521575928, 0.0021444237791001797, 0.720757007598877, 0.9273355007171631, 0.7868642210960388, 0.268100768327713, 0.008893672376871109, 2.0570082664489746, 0.008071769028902054, 0.7773869037628174, 0.003698278684169054, 0.00016276625683531165, 0.00353431049734354, 0.0004429254331625998, 0.00018606327648740262, 0.2766133248806, 0.025816382840275764], "entropies_second": null, "final_normalized_entropy": -1.3356528107444823, "sequence_confidence_first": 0.7710618957587781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7710618957587781, "token_confidences_first": [0.5329687595367432, 0.9810047745704651, 0.2812984585762024, 0.9459823369979858, 0.9997952580451965, 0.6430691480636597, 0.4965868294239044, 0.5289799571037292, 0.9438300132751465, 0.9989691972732544, 0.4275483787059784, 0.9991838335990906, 0.6220735907554626, 0.9996532201766968, 0.99998939037323, 0.9996871948242188, 0.9999699592590332, 0.9999860525131226, 0.9410784840583801, 0.9966082572937012], "token_confidences_second": null, "final_mean_entropy": 0.44776840738122703, "final_min_margin": 0.04045677185058594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 16.75, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2014."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_second": null, "mean_entropy_first": 0.44776840738122703, "normalized_entropy_first": -1.1672638936931428, "min_margin_first": 0.04045677185058594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1227, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1227, "total_latency_s": 1.227, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8204617500305176, 0.12248177826404572, 1.8972043991088867, 0.24820363521575928, 0.0021444237791001797, 0.720757007598877, 0.9273355007171631, 0.7868642210960388, 0.268100768327713, 0.008893672376871109, 2.0570082664489746, 0.008071769028902054, 0.7773869037628174, 0.003698278684169054, 0.00016276625683531165, 0.00353431049734354, 0.0004429254331625998, 0.00018606327648740262, 0.2766133248806, 0.025816382840275764], "entropies_second": null, "final_normalized_entropy": -1.1672638936931428, "sequence_confidence_first": 0.7710618957587781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7710618957587781, "token_confidences_first": [0.5329687595367432, 0.9810047745704651, 0.2812984585762024, 0.9459823369979858, 0.9997952580451965, 0.6430691480636597, 0.4965868294239044, 0.5289799571037292, 0.9438300132751465, 0.9989691972732544, 0.4275483787059784, 0.9991838335990906, 0.6220735907554626, 0.9996532201766968, 0.99998939037323, 0.9996871948242188, 0.9999699592590332, 0.9999860525131226, 0.9410784840583801, 0.9966082572937012], "token_confidences_second": null, "final_mean_entropy": 0.44776840738122703, "final_min_margin": 0.04045677185058594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.4, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["21%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_second": null, "mean_entropy_first": 0.44776840738122703, "normalized_entropy_first": -1.0388510020973067, "min_margin_first": 0.04045677185058594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1228, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1229, "total_latency_s": 1.229, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8204617500305176, 0.12248177826404572, 1.8972043991088867, 0.24820363521575928, 0.0021444237791001797, 0.720757007598877, 0.9273355007171631, 0.7868642210960388, 0.268100768327713, 0.008893672376871109, 2.0570082664489746, 0.008071769028902054, 0.7773869037628174, 0.003698278684169054, 0.00016276625683531165, 0.00353431049734354, 0.0004429254331625998, 0.00018606327648740262, 0.2766133248806, 0.025816382840275764], "entropies_second": null, "final_normalized_entropy": -1.0388510020973067, "sequence_confidence_first": 0.7710618957587781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7710618957587781, "token_confidences_first": [0.5329687595367432, 0.9810047745704651, 0.2812984585762024, 0.9459823369979858, 0.9997952580451965, 0.6430691480636597, 0.4965868294239044, 0.5289799571037292, 0.9438300132751465, 0.9989691972732544, 0.4275483787059784, 0.9991838335990906, 0.6220735907554626, 0.9996532201766968, 0.99998939037323, 0.9996871948242188, 0.9999699592590332, 0.9999860525131226, 0.9410784840583801, 0.9966082572937012], "token_confidences_second": null, "final_mean_entropy": 0.44776840738122703, "final_min_margin": 0.04045677185058594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 16.75, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["2012."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_second": null, "mean_entropy_first": 0.44776840738122703, "normalized_entropy_first": -0.9363109299422959, "min_margin_first": 0.04045677185058594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1228, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1229, "total_latency_s": 1.229, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8204617500305176, 0.12248177826404572, 1.8972043991088867, 0.24820363521575928, 0.0021444237791001797, 0.720757007598877, 0.9273355007171631, 0.7868642210960388, 0.268100768327713, 0.008893672376871109, 2.0570082664489746, 0.008071769028902054, 0.7773869037628174, 0.003698278684169054, 0.00016276625683531165, 0.00353431049734354, 0.0004429254331625998, 0.00018606327648740262, 0.2766133248806, 0.025816382840275764], "entropies_second": null, "final_normalized_entropy": -0.9363109299422959, "sequence_confidence_first": 0.7710618957587781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7710618957587781, "token_confidences_first": [0.5329687595367432, 0.9810047745704651, 0.2812984585762024, 0.9459823369979858, 0.9997952580451965, 0.6430691480636597, 0.4965868294239044, 0.5289799571037292, 0.9438300132751465, 0.9989691972732544, 0.4275483787059784, 0.9991838335990906, 0.6220735907554626, 0.9996532201766968, 0.99998939037323, 0.9996871948242188, 0.9999699592590332, 0.9999860525131226, 0.9410784840583801, 0.9966082572937012], "token_confidences_second": null, "final_mean_entropy": 0.44776840738122703, "final_min_margin": 0.04045677185058594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 13.4, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["15-16 MILLION."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_first": "The image you've provided is an infographic titled \"SOCIAL MEDIA AU", "raw_answer_second": null, "mean_entropy_first": 0.44776840738122703, "normalized_entropy_first": -0.8517128682129298, "min_margin_first": 0.04045677185058594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1228, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1229, "total_latency_s": 1.229, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8204617500305176, 0.12248177826404572, 1.8972043991088867, 0.24820363521575928, 0.0021444237791001797, 0.720757007598877, 0.9273355007171631, 0.7868642210960388, 0.268100768327713, 0.008893672376871109, 2.0570082664489746, 0.008071769028902054, 0.7773869037628174, 0.003698278684169054, 0.00016276625683531165, 0.00353431049734354, 0.0004429254331625998, 0.00018606327648740262, 0.2766133248806, 0.025816382840275764], "entropies_second": null, "final_normalized_entropy": -0.8517128682129298, "sequence_confidence_first": 0.7710618957587781, "sequence_confidence_second": null, "sequence_confidence_final": 0.7710618957587781, "token_confidences_first": [0.5329687595367432, 0.9810047745704651, 0.2812984585762024, 0.9459823369979858, 0.9997952580451965, 0.6430691480636597, 0.4965868294239044, 0.5289799571037292, 0.9438300132751465, 0.9989691972732544, 0.4275483787059784, 0.9991838335990906, 0.6220735907554626, 0.9996532201766968, 0.99998939037323, 0.9996871948242188, 0.9999699592590332, 0.9999860525131226, 0.9410784840583801, 0.9966082572937012], "token_confidences_second": null, "final_mean_entropy": 0.44776840738122703, "final_min_margin": 0.04045677185058594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.428571428571429, "wer": 5.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Red."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "used_ocr": false, "answer_first": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "answer_second": null, "raw_answer": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "raw_answer_first": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "raw_answer_second": null, "mean_entropy_first": 1.2808516447905276, "normalized_entropy_first": 1.4868940998824227, "min_margin_first": 0.07235336303710938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1513, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1514, "total_latency_s": 1.514, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7572641372680664, 0.07664631307125092, 2.0042147636413574, 0.0003460928564891219, 0.18463657796382904, 0.807296872138977, 2.7253928184509277, 0.5452640056610107, 2.368842601776123, 2.3458385467529297, 0.02034369297325611, 2.326143264770508, 0.00018278755305800587, 1.8470065593719482, 2.5978610515594482, 1.8397074937820435, 0.7423555850982666, 0.8179759383201599, 0.7397339344024658, 2.8699798583984375], "entropies_second": null, "final_normalized_entropy": 1.4868940998824227, "sequence_confidence_first": 0.5337272838260366, "sequence_confidence_second": null, "sequence_confidence_final": 0.5337272838260366, "token_confidences_first": [0.6115046739578247, 0.9897464513778687, 0.23568430542945862, 0.9999755620956421, 0.9716190099716187, 0.5423452854156494, 0.3841511607170105, 0.848820686340332, 0.3769705891609192, 0.34302574396133423, 0.9973601698875427, 0.4370000660419464, 0.9999885559082031, 0.5936279892921448, 0.23589321970939636, 0.2782232463359833, 0.7349984049797058, 0.666025698184967, 0.8284659385681152, 0.1612190306186676], "token_confidences_second": null, "final_mean_entropy": 1.2808516447905276, "final_min_margin": 0.07235336303710938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 22.75, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Supported accommodation for the homeless."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "used_ocr": false, "answer_first": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "answer_second": null, "raw_answer": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "raw_answer_first": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "raw_answer_second": null, "mean_entropy_first": 1.2808516447905276, "normalized_entropy_first": 1.276521907239592, "min_margin_first": 0.07235336303710938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1509, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1511, "total_latency_s": 1.511, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7572641372680664, 0.07664631307125092, 2.0042147636413574, 0.0003460928564891219, 0.18463657796382904, 0.807296872138977, 2.7253928184509277, 0.5452640056610107, 2.368842601776123, 2.3458385467529297, 0.02034369297325611, 2.326143264770508, 0.00018278755305800587, 1.8470065593719482, 2.5978610515594482, 1.8397074937820435, 0.7423555850982666, 0.8179759383201599, 0.7397339344024658, 2.8699798583984375], "entropies_second": null, "final_normalized_entropy": 1.276521907239592, "sequence_confidence_first": 0.5337272838260366, "sequence_confidence_second": null, "sequence_confidence_final": 0.5337272838260366, "token_confidences_first": [0.6115046739578247, 0.9897464513778687, 0.23568430542945862, 0.9999755620956421, 0.9716190099716187, 0.5423452854156494, 0.3841511607170105, 0.848820686340332, 0.3769705891609192, 0.34302574396133423, 0.9973601698875427, 0.4370000660419464, 0.9999885559082031, 0.5936279892921448, 0.23589321970939636, 0.2782232463359833, 0.7349984049797058, 0.666025698184967, 0.8284659385681152, 0.1612190306186676], "token_confidences_second": null, "final_mean_entropy": 1.2808516447905276, "final_min_margin": 0.07235336303710938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.853658536585366, "wer": 3.4, "precision": 0.058823529411764705, "recall": 0.2, "f1": 0.0909090909090909, "rouge_l": 0.0909090909090909, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Boarding houses."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "used_ocr": false, "answer_first": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "answer_second": null, "raw_answer": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "raw_answer_first": "The image appears to be a graphic or infographic related to housing options in Australia. It is", "raw_answer_second": null, "mean_entropy_first": 1.2808516447905276, "normalized_entropy_first": 1.1229718063972414, "min_margin_first": 0.07235336303710938, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1510, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1511, "total_latency_s": 1.511, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7572641372680664, 0.07664631307125092, 2.0042147636413574, 0.0003460928564891219, 0.18463657796382904, 0.807296872138977, 2.7253928184509277, 0.5452640056610107, 2.368842601776123, 2.3458385467529297, 0.02034369297325611, 2.326143264770508, 0.00018278755305800587, 1.8470065593719482, 2.5978610515594482, 1.8397074937820435, 0.7423555850982666, 0.8179759383201599, 0.7397339344024658, 2.8699798583984375], "entropies_second": null, "final_normalized_entropy": 1.1229718063972414, "sequence_confidence_first": 0.5337272838260366, "sequence_confidence_second": null, "sequence_confidence_final": 0.5337272838260366, "token_confidences_first": [0.6115046739578247, 0.9897464513778687, 0.23568430542945862, 0.9999755620956421, 0.9716190099716187, 0.5423452854156494, 0.3841511607170105, 0.848820686340332, 0.3769705891609192, 0.34302574396133423, 0.9973601698875427, 0.4370000660419464, 0.9999885559082031, 0.5936279892921448, 0.23589321970939636, 0.2782232463359833, 0.7349984049797058, 0.666025698184967, 0.8284659385681152, 0.1612190306186676], "token_confidences_second": null, "final_mean_entropy": 1.2808516447905276, "final_min_margin": 0.07235336303710938, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.125, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["PrimeSafe."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_second": null, "mean_entropy_first": 0.6577305330005402, "normalized_entropy_first": -0.5666095588370066, "min_margin_first": 0.10323524475097656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1953, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1953, "total_latency_s": 1.953, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8606623411178589, 0.011408441700041294, 1.361497402191162, 0.2836044430732727, 0.0019706962630152702, 0.6141867637634277, 0.7486268281936646, 2.9790935514029115e-05, 0.018546508625149727, 0.7388899326324463, 3.5332231521606445, 0.6704115271568298, 0.06996554881334305, 0.07769584655761719, 2.2907302379608154, 0.07521570473909378, 1.014546513557434, 4.575300408760086e-06, 0.7833409905433655, 5.3414725698530674e-05], "entropies_second": null, "final_normalized_entropy": -0.5666095588370066, "sequence_confidence_first": 0.6995982589353082, "sequence_confidence_second": null, "sequence_confidence_final": 0.6995982589353082, "token_confidences_first": [0.5342865586280823, 0.9989284873008728, 0.5432342290878296, 0.9354031085968018, 0.9998100399971008, 0.7416055202484131, 0.6935089230537415, 0.9999983310699463, 0.997677743434906, 0.6747705936431885, 0.13815124332904816, 0.7205229997634888, 0.9871863126754761, 0.9902971386909485, 0.19228805601596832, 0.9894234538078308, 0.7163853645324707, 0.9999997615814209, 0.6334973573684692, 0.9999963045120239], "token_confidences_second": null, "final_mean_entropy": 0.6577305330005402, "final_min_margin": 0.10323524475097656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.5, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Rock Lobster."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_second": null, "mean_entropy_first": 0.6577305330005402, "normalized_entropy_first": -0.5291067311372963, "min_margin_first": 0.10323524475097656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1346, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8606623411178589, 0.011408441700041294, 1.361497402191162, 0.2836044430732727, 0.0019706962630152702, 0.6141867637634277, 0.7486268281936646, 2.9790935514029115e-05, 0.018546508625149727, 0.7388899326324463, 3.5332231521606445, 0.6704115271568298, 0.06996554881334305, 0.07769584655761719, 2.2907302379608154, 0.07521570473909378, 1.014546513557434, 4.575300408760086e-06, 0.7833409905433655, 5.3414725698530674e-05], "entropies_second": null, "final_normalized_entropy": -0.5291067311372963, "sequence_confidence_first": 0.6995982589353082, "sequence_confidence_second": null, "sequence_confidence_final": 0.6995982589353082, "token_confidences_first": [0.5342865586280823, 0.9989284873008728, 0.5432342290878296, 0.9354031085968018, 0.9998100399971008, 0.7416055202484131, 0.6935089230537415, 0.9999983310699463, 0.997677743434906, 0.6747705936431885, 0.13815124332904816, 0.7205229997634888, 0.9871863126754761, 0.9902971386909485, 0.19228805601596832, 0.9894234538078308, 0.7163853645324707, 0.9999997615814209, 0.6334973573684692, 0.9999963045120239], "token_confidences_second": null, "final_mean_entropy": 0.6577305330005402, "final_min_margin": 0.10323524475097656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.461538461538462, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["A$1m."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_second": null, "mean_entropy_first": 0.6577305330005402, "normalized_entropy_first": -0.49507267549172307, "min_margin_first": 0.10323524475097656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1349, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1349, "total_latency_s": 1.349, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8606623411178589, 0.011408441700041294, 1.361497402191162, 0.2836044430732727, 0.0019706962630152702, 0.6141867637634277, 0.7486268281936646, 2.9790935514029115e-05, 0.018546508625149727, 0.7388899326324463, 3.5332231521606445, 0.6704115271568298, 0.06996554881334305, 0.07769584655761719, 2.2907302379608154, 0.07521570473909378, 1.014546513557434, 4.575300408760086e-06, 0.7833409905433655, 5.3414725698530674e-05], "entropies_second": null, "final_normalized_entropy": -0.49507267549172307, "sequence_confidence_first": 0.6995982589353082, "sequence_confidence_second": null, "sequence_confidence_final": 0.6995982589353082, "token_confidences_first": [0.5342865586280823, 0.9989284873008728, 0.5432342290878296, 0.9354031085968018, 0.9998100399971008, 0.7416055202484131, 0.6935089230537415, 0.9999983310699463, 0.997677743434906, 0.6747705936431885, 0.13815124332904816, 0.7205229997634888, 0.9871863126754761, 0.9902971386909485, 0.19228805601596832, 0.9894234538078308, 0.7163853645324707, 0.9999997615814209, 0.6334973573684692, 0.9999963045120239], "token_confidences_second": null, "final_mean_entropy": 0.6577305330005402, "final_min_margin": 0.10323524475097656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 18.2, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Vietnam."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_second": null, "mean_entropy_first": 0.6577305330005402, "normalized_entropy_first": -0.4640151660012282, "min_margin_first": 0.10323524475097656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1349, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1350, "total_latency_s": 1.35, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8606623411178589, 0.011408441700041294, 1.361497402191162, 0.2836044430732727, 0.0019706962630152702, 0.6141867637634277, 0.7486268281936646, 2.9790935514029115e-05, 0.018546508625149727, 0.7388899326324463, 3.5332231521606445, 0.6704115271568298, 0.06996554881334305, 0.07769584655761719, 2.2907302379608154, 0.07521570473909378, 1.014546513557434, 4.575300408760086e-06, 0.7833409905433655, 5.3414725698530674e-05], "entropies_second": null, "final_normalized_entropy": -0.4640151660012282, "sequence_confidence_first": 0.6995982589353082, "sequence_confidence_second": null, "sequence_confidence_final": 0.6995982589353082, "token_confidences_first": [0.5342865586280823, 0.9989284873008728, 0.5432342290878296, 0.9354031085968018, 0.9998100399971008, 0.7416055202484131, 0.6935089230537415, 0.9999983310699463, 0.997677743434906, 0.6747705936431885, 0.13815124332904816, 0.7205229997634888, 0.9871863126754761, 0.9902971386909485, 0.19228805601596832, 0.9894234538078308, 0.7163853645324707, 0.9999997615814209, 0.6334973573684692, 0.9999963045120239], "token_confidences_second": null, "final_mean_entropy": 0.6577305330005402, "final_min_margin": 0.10323524475097656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 10.875, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Singapore."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "used_ocr": false, "answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "answer_second": null, "raw_answer": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_first": "The image you've provided appears to be a slide from a presentation about the Victorian seafo", "raw_answer_second": null, "mean_entropy_first": 0.6577305330005402, "normalized_entropy_first": -0.4355396053480536, "min_margin_first": 0.10323524475097656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1354, "total_latency_s": 1.354, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8606623411178589, 0.011408441700041294, 1.361497402191162, 0.2836044430732727, 0.0019706962630152702, 0.6141867637634277, 0.7486268281936646, 2.9790935514029115e-05, 0.018546508625149727, 0.7388899326324463, 3.5332231521606445, 0.6704115271568298, 0.06996554881334305, 0.07769584655761719, 2.2907302379608154, 0.07521570473909378, 1.014546513557434, 4.575300408760086e-06, 0.7833409905433655, 5.3414725698530674e-05], "entropies_second": null, "final_normalized_entropy": -0.4355396053480536, "sequence_confidence_first": 0.6995982589353082, "sequence_confidence_second": null, "sequence_confidence_final": 0.6995982589353082, "token_confidences_first": [0.5342865586280823, 0.9989284873008728, 0.5432342290878296, 0.9354031085968018, 0.9998100399971008, 0.7416055202484131, 0.6935089230537415, 0.9999983310699463, 0.997677743434906, 0.6747705936431885, 0.13815124332904816, 0.7205229997634888, 0.9871863126754761, 0.9902971386909485, 0.19228805601596832, 0.9894234538078308, 0.7163853645324707, 0.9999997615814209, 0.6334973573684692, 0.9999963045120239], "token_confidences_second": null, "final_mean_entropy": 0.6577305330005402, "final_min_margin": 0.10323524475097656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.6, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Male."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1998182053990605, "normalized_entropy_first": 1.2622114337651635, "min_margin_first": 0.06987571716308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1393, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1394, "total_latency_s": 1.394, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7903447151184082, 0.9506818056106567, 1.0393434762954712, 0.0001423861103830859, 0.05566253513097763, 0.7497702836990356, 2.894050121307373, 0.6077054738998413, 2.27553129196167, 1.8170417547225952, 2.6723203659057617, 0.0001752085518091917, 2.1121912002563477, 2.7220332622528076, 1.9871280193328857, 0.040900714695453644, 0.5183296799659729, 0.47656118869781494, 1.0287058353424072, 1.2577447891235352], "entropies_second": null, "final_normalized_entropy": 1.2622114337651635, "sequence_confidence_first": 0.5783784778852331, "sequence_confidence_second": null, "sequence_confidence_final": 0.5783784778852331, "token_confidences_first": [0.5105436444282532, 0.6849386692047119, 0.6500778198242188, 0.9999899864196777, 0.9934479594230652, 0.5868270993232727, 0.3489132821559906, 0.7470560073852539, 0.4454226791858673, 0.38782790303230286, 0.2784481942653656, 0.9999890327453613, 0.3629256784915924, 0.2459099441766739, 0.4307329058647156, 0.9942995309829712, 0.91021728515625, 0.881434977054596, 0.625298798084259, 0.5507797002792358], "token_confidences_second": null, "final_mean_entropy": 1.1998182053990605, "final_min_margin": 0.06987571716308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.2, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Female."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1998182053990605, "normalized_entropy_first": 1.1121211962802116, "min_margin_first": 0.06987571716308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1393, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1394, "total_latency_s": 1.394, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7903447151184082, 0.9506818056106567, 1.0393434762954712, 0.0001423861103830859, 0.05566253513097763, 0.7497702836990356, 2.894050121307373, 0.6077054738998413, 2.27553129196167, 1.8170417547225952, 2.6723203659057617, 0.0001752085518091917, 2.1121912002563477, 2.7220332622528076, 1.9871280193328857, 0.040900714695453644, 0.5183296799659729, 0.47656118869781494, 1.0287058353424072, 1.2577447891235352], "entropies_second": null, "final_normalized_entropy": 1.1121211962802116, "sequence_confidence_first": 0.5783784778852331, "sequence_confidence_second": null, "sequence_confidence_final": 0.5783784778852331, "token_confidences_first": [0.5105436444282532, 0.6849386692047119, 0.6500778198242188, 0.9999899864196777, 0.9934479594230652, 0.5868270993232727, 0.3489132821559906, 0.7470560073852539, 0.4454226791858673, 0.38782790303230286, 0.2784481942653656, 0.9999890327453613, 0.3629256784915924, 0.2459099441766739, 0.4307329058647156, 0.9942995309829712, 0.91021728515625, 0.881434977054596, 0.625298798084259, 0.5507797002792358], "token_confidences_second": null, "final_mean_entropy": 1.1998182053990605, "final_min_margin": 0.06987571716308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.285714285714286, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Male."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1998182053990605, "normalized_entropy_first": 0.9952949153991559, "min_margin_first": 0.06987571716308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1391, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1392, "total_latency_s": 1.392, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7903447151184082, 0.9506818056106567, 1.0393434762954712, 0.0001423861103830859, 0.05566253513097763, 0.7497702836990356, 2.894050121307373, 0.6077054738998413, 2.27553129196167, 1.8170417547225952, 2.6723203659057617, 0.0001752085518091917, 2.1121912002563477, 2.7220332622528076, 1.9871280193328857, 0.040900714695453644, 0.5183296799659729, 0.47656118869781494, 1.0287058353424072, 1.2577447891235352], "entropies_second": null, "final_normalized_entropy": 0.9952949153991559, "sequence_confidence_first": 0.5783784778852331, "sequence_confidence_second": null, "sequence_confidence_final": 0.5783784778852331, "token_confidences_first": [0.5105436444282532, 0.6849386692047119, 0.6500778198242188, 0.9999899864196777, 0.9934479594230652, 0.5868270993232727, 0.3489132821559906, 0.7470560073852539, 0.4454226791858673, 0.38782790303230286, 0.2784481942653656, 0.9999890327453613, 0.3629256784915924, 0.2459099441766739, 0.4307329058647156, 0.9942995309829712, 0.91021728515625, 0.881434977054596, 0.625298798084259, 0.5507797002792358], "token_confidences_second": null, "final_mean_entropy": 1.1998182053990605, "final_min_margin": 0.06987571716308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.2, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Female."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1998182053990605, "normalized_entropy_first": 0.9006625451756376, "min_margin_first": 0.06987571716308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1388, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1389, "total_latency_s": 1.389, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7903447151184082, 0.9506818056106567, 1.0393434762954712, 0.0001423861103830859, 0.05566253513097763, 0.7497702836990356, 2.894050121307373, 0.6077054738998413, 2.27553129196167, 1.8170417547225952, 2.6723203659057617, 0.0001752085518091917, 2.1121912002563477, 2.7220332622528076, 1.9871280193328857, 0.040900714695453644, 0.5183296799659729, 0.47656118869781494, 1.0287058353424072, 1.2577447891235352], "entropies_second": null, "final_normalized_entropy": 0.9006625451756376, "sequence_confidence_first": 0.5783784778852331, "sequence_confidence_second": null, "sequence_confidence_final": 0.5783784778852331, "token_confidences_first": [0.5105436444282532, 0.6849386692047119, 0.6500778198242188, 0.9999899864196777, 0.9934479594230652, 0.5868270993232727, 0.3489132821559906, 0.7470560073852539, 0.4454226791858673, 0.38782790303230286, 0.2784481942653656, 0.9999890327453613, 0.3629256784915924, 0.2459099441766739, 0.4307329058647156, 0.9942995309829712, 0.91021728515625, 0.881434977054596, 0.625298798084259, 0.5507797002792358], "token_confidences_second": null, "final_mean_entropy": 1.1998182053990605, "final_min_margin": 0.06987571716308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.285714285714286, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Female."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1998182053990605, "normalized_entropy_first": 0.821762931006133, "min_margin_first": 0.06987571716308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1388, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1389, "total_latency_s": 1.389, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7903447151184082, 0.9506818056106567, 1.0393434762954712, 0.0001423861103830859, 0.05566253513097763, 0.7497702836990356, 2.894050121307373, 0.6077054738998413, 2.27553129196167, 1.8170417547225952, 2.6723203659057617, 0.0001752085518091917, 2.1121912002563477, 2.7220332622528076, 1.9871280193328857, 0.040900714695453644, 0.5183296799659729, 0.47656118869781494, 1.0287058353424072, 1.2577447891235352], "entropies_second": null, "final_normalized_entropy": 0.821762931006133, "sequence_confidence_first": 0.5783784778852331, "sequence_confidence_second": null, "sequence_confidence_final": 0.5783784778852331, "token_confidences_first": [0.5105436444282532, 0.6849386692047119, 0.6500778198242188, 0.9999899864196777, 0.9934479594230652, 0.5868270993232727, 0.3489132821559906, 0.7470560073852539, 0.4454226791858673, 0.38782790303230286, 0.2784481942653656, 0.9999890327453613, 0.3629256784915924, 0.2459099441766739, 0.4307329058647156, 0.9942995309829712, 0.91021728515625, 0.881434977054596, 0.625298798084259, 0.5507797002792358], "token_confidences_second": null, "final_mean_entropy": 1.1998182053990605, "final_min_margin": 0.06987571716308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 12.285714285714286, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Male."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "used_ocr": false, "answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "answer_second": null, "raw_answer": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_first": "This image appears to be a graphic or poster related to the gender pay gap in Australia. It", "raw_answer_second": null, "mean_entropy_first": 1.1998182053990605, "normalized_entropy_first": 0.7545323643792542, "min_margin_first": 0.06987571716308594, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1389, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1391, "total_latency_s": 1.391, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7903447151184082, 0.9506818056106567, 1.0393434762954712, 0.0001423861103830859, 0.05566253513097763, 0.7497702836990356, 2.894050121307373, 0.6077054738998413, 2.27553129196167, 1.8170417547225952, 2.6723203659057617, 0.0001752085518091917, 2.1121912002563477, 2.7220332622528076, 1.9871280193328857, 0.040900714695453644, 0.5183296799659729, 0.47656118869781494, 1.0287058353424072, 1.2577447891235352], "entropies_second": null, "final_normalized_entropy": 0.7545323643792542, "sequence_confidence_first": 0.5783784778852331, "sequence_confidence_second": null, "sequence_confidence_final": 0.5783784778852331, "token_confidences_first": [0.5105436444282532, 0.6849386692047119, 0.6500778198242188, 0.9999899864196777, 0.9934479594230652, 0.5868270993232727, 0.3489132821559906, 0.7470560073852539, 0.4454226791858673, 0.38782790303230286, 0.2784481942653656, 0.9999890327453613, 0.3629256784915924, 0.2459099441766739, 0.4307329058647156, 0.9942995309829712, 0.91021728515625, 0.881434977054596, 0.625298798084259, 0.5507797002792358], "token_confidences_second": null, "final_mean_entropy": 1.1998182053990605, "final_min_margin": 0.06987571716308594, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.2, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["20."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "used_ocr": false, "answer_first": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "answer_second": null, "raw_answer": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "raw_answer_first": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "raw_answer_second": null, "mean_entropy_first": 1.397023985609121, "normalized_entropy_first": 1.3274484345532491, "min_margin_first": 0.029848098754882812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1941, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1942, "total_latency_s": 1.942, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0347036123275757, 0.7908605337142944, 1.426742434501648, 0.00012785903527401388, 0.061105914413928986, 0.8048086166381836, 0.6649423241615295, 0.017159463837742805, 2.7054853439331055, 2.734367847442627, 3.494915008544922, 1.4400891065597534, 1.2178014516830444, 1.8742218017578125, 2.678999900817871, 0.8540936708450317, 1.4397342205047607, 1.6139636039733887, 1.4258052110671997, 1.6605517864227295], "entropies_second": null, "final_normalized_entropy": 1.3274484345532491, "sequence_confidence_first": 0.5104129688635219, "sequence_confidence_second": null, "sequence_confidence_final": 0.5104129688635219, "token_confidences_first": [0.4796358346939087, 0.6926998496055603, 0.5564688444137573, 0.9999912977218628, 0.9919623732566833, 0.5279248952865601, 0.849297821521759, 0.9978528022766113, 0.30059993267059326, 0.23134039342403412, 0.13759708404541016, 0.588933527469635, 0.7604846954345703, 0.601432740688324, 0.17795100808143616, 0.6789438128471375, 0.44402584433555603, 0.5353621244430542, 0.5192807912826538, 0.45662590861320496], "token_confidences_second": null, "final_mean_entropy": 1.397023985609121, "final_min_margin": 0.029848098754882812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 40.0, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Science fiction."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "used_ocr": false, "answer_first": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "answer_second": null, "raw_answer": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "raw_answer_first": "This image appears to be an infographic or a graphical representation of international book statistics, specifically foc", "raw_answer_second": null, "mean_entropy_first": 1.397023985609121, "normalized_entropy_first": 1.1611708466629436, "min_margin_first": 0.029848098754882812, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1352, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1353, "total_latency_s": 1.353, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0347036123275757, 0.7908605337142944, 1.426742434501648, 0.00012785903527401388, 0.061105914413928986, 0.8048086166381836, 0.6649423241615295, 0.017159463837742805, 2.7054853439331055, 2.734367847442627, 3.494915008544922, 1.4400891065597534, 1.2178014516830444, 1.8742218017578125, 2.678999900817871, 0.8540936708450317, 1.4397342205047607, 1.6139636039733887, 1.4258052110671997, 1.6605517864227295], "entropies_second": null, "final_normalized_entropy": 1.1611708466629436, "sequence_confidence_first": 0.5104129688635219, "sequence_confidence_second": null, "sequence_confidence_final": 0.5104129688635219, "token_confidences_first": [0.4796358346939087, 0.6926998496055603, 0.5564688444137573, 0.9999912977218628, 0.9919623732566833, 0.5279248952865601, 0.849297821521759, 0.9978528022766113, 0.30059993267059326, 0.23134039342403412, 0.13759708404541016, 0.588933527469635, 0.7604846954345703, 0.601432740688324, 0.17795100808143616, 0.6789438128471375, 0.44402584433555603, 0.5353621244430542, 0.5192807912826538, 0.45662590861320496], "token_confidences_second": null, "final_mean_entropy": 1.397023985609121, "final_min_margin": 0.029848098754882812, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.6875, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Tomorrow's world."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_second": null, "mean_entropy_first": 0.8036385535626323, "normalized_entropy_first": -0.7924299329934734, "min_margin_first": 0.0048656463623046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1933, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1934, "total_latency_s": 1.934, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7867201566696167, 0.018472082912921906, 1.5410538911819458, 0.2337101846933365, 0.002630502451211214, 0.6862288117408752, 0.863387405872345, 3.204800304956734e-05, 0.016095027327537537, 0.8300103545188904, 0.4352874159812927, 0.007553412578999996, 1.9270561933517456, 2.199112892150879, 3.0285468101501465, 0.1951153576374054, 0.11820725351572037, 0.866955578327179, 2.310790538787842, 0.005805153399705887], "entropies_second": null, "final_normalized_entropy": -0.7924299329934734, "sequence_confidence_first": 0.6562159459340845, "sequence_confidence_second": null, "sequence_confidence_final": 0.6562159459340845, "token_confidences_first": [0.5861688852310181, 0.9980383515357971, 0.4785546362400055, 0.9502339959144592, 0.9997346997261047, 0.6833568215370178, 0.48585161566734314, 0.9999982118606567, 0.9980944991111755, 0.4947408437728882, 0.8950231671333313, 0.9991587400436401, 0.3722420334815979, 0.3855527341365814, 0.19100084900856018, 0.9726919531822205, 0.9756215810775757, 0.7885231375694275, 0.27427881956100464, 0.9994065761566162], "token_confidences_second": null, "final_mean_entropy": 0.8036385535626323, "final_min_margin": 0.0048656463623046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.470588235294118, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["33%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_second": null, "mean_entropy_first": 0.8036385535626323, "normalized_entropy_first": -0.7292181567914549, "min_margin_first": 0.0048656463623046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7867201566696167, 0.018472082912921906, 1.5410538911819458, 0.2337101846933365, 0.002630502451211214, 0.6862288117408752, 0.863387405872345, 3.204800304956734e-05, 0.016095027327537537, 0.8300103545188904, 0.4352874159812927, 0.007553412578999996, 1.9270561933517456, 2.199112892150879, 3.0285468101501465, 0.1951153576374054, 0.11820725351572037, 0.866955578327179, 2.310790538787842, 0.005805153399705887], "entropies_second": null, "final_normalized_entropy": -0.7292181567914549, "sequence_confidence_first": 0.6562159459340845, "sequence_confidence_second": null, "sequence_confidence_final": 0.6562159459340845, "token_confidences_first": [0.5861688852310181, 0.9980383515357971, 0.4785546362400055, 0.9502339959144592, 0.9997346997261047, 0.6833568215370178, 0.48585161566734314, 0.9999982118606567, 0.9980944991111755, 0.4947408437728882, 0.8950231671333313, 0.9991587400436401, 0.3722420334815979, 0.3855527341365814, 0.19100084900856018, 0.9726919531822205, 0.9756215810775757, 0.7885231375694275, 0.27427881956100464, 0.9994065761566162], "token_confidences_second": null, "final_mean_entropy": 0.8036385535626323, "final_min_margin": 0.0048656463623046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 21.75, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1.6 million+."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_second": null, "mean_entropy_first": 0.8036385535626323, "normalized_entropy_first": -0.6741061193975717, "min_margin_first": 0.0048656463623046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1347, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1347, "total_latency_s": 1.347, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7867201566696167, 0.018472082912921906, 1.5410538911819458, 0.2337101846933365, 0.002630502451211214, 0.6862288117408752, 0.863387405872345, 3.204800304956734e-05, 0.016095027327537537, 0.8300103545188904, 0.4352874159812927, 0.007553412578999996, 1.9270561933517456, 2.199112892150879, 3.0285468101501465, 0.1951153576374054, 0.11820725351572037, 0.866955578327179, 2.310790538787842, 0.005805153399705887], "entropies_second": null, "final_normalized_entropy": -0.6741061193975717, "sequence_confidence_first": 0.6562159459340845, "sequence_confidence_second": null, "sequence_confidence_final": 0.6562159459340845, "token_confidences_first": [0.5861688852310181, 0.9980383515357971, 0.4785546362400055, 0.9502339959144592, 0.9997346997261047, 0.6833568215370178, 0.48585161566734314, 0.9999982118606567, 0.9980944991111755, 0.4947408437728882, 0.8950231671333313, 0.9991587400436401, 0.3722420334815979, 0.3855527341365814, 0.19100084900856018, 0.9726919531822205, 0.9756215810775757, 0.7885231375694275, 0.27427881956100464, 0.9994065761566162], "token_confidences_second": null, "final_mean_entropy": 0.8036385535626323, "final_min_margin": 0.0048656463623046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.3076923076923075, "wer": 8.5, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Industries in transition."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_second": null, "mean_entropy_first": 0.8036385535626323, "normalized_entropy_first": -0.6254600453731894, "min_margin_first": 0.0048656463623046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1351, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7867201566696167, 0.018472082912921906, 1.5410538911819458, 0.2337101846933365, 0.002630502451211214, 0.6862288117408752, 0.863387405872345, 3.204800304956734e-05, 0.016095027327537537, 0.8300103545188904, 0.4352874159812927, 0.007553412578999996, 1.9270561933517456, 2.199112892150879, 3.0285468101501465, 0.1951153576374054, 0.11820725351572037, 0.866955578327179, 2.310790538787842, 0.005805153399705887], "entropies_second": null, "final_normalized_entropy": -0.6254600453731894, "sequence_confidence_first": 0.6562159459340845, "sequence_confidence_second": null, "sequence_confidence_final": 0.6562159459340845, "token_confidences_first": [0.5861688852310181, 0.9980383515357971, 0.4785546362400055, 0.9502339959144592, 0.9997346997261047, 0.6833568215370178, 0.48585161566734314, 0.9999982118606567, 0.9980944991111755, 0.4947408437728882, 0.8950231671333313, 0.9991587400436401, 0.3722420334815979, 0.3855527341365814, 0.19100084900856018, 0.9726919531822205, 0.9756215810775757, 0.7885231375694275, 0.27427881956100464, 0.9994065761566162], "token_confidences_second": null, "final_mean_entropy": 0.8036385535626323, "final_min_margin": 0.0048656463623046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.92, "wer": 5.666666666666667, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["15.9."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "used_ocr": false, "answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "answer_second": null, "raw_answer": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_first": "The image you've provided appears to be an infographic or a page from a report titled \"", "raw_answer_second": null, "mean_entropy_first": 0.8036385535626323, "normalized_entropy_first": -0.5820870926502071, "min_margin_first": 0.0048656463623046875, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1350, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1351, "total_latency_s": 1.351, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7867201566696167, 0.018472082912921906, 1.5410538911819458, 0.2337101846933365, 0.002630502451211214, 0.6862288117408752, 0.863387405872345, 3.204800304956734e-05, 0.016095027327537537, 0.8300103545188904, 0.4352874159812927, 0.007553412578999996, 1.9270561933517456, 2.199112892150879, 3.0285468101501465, 0.1951153576374054, 0.11820725351572037, 0.866955578327179, 2.310790538787842, 0.005805153399705887], "entropies_second": null, "final_normalized_entropy": -0.5820870926502071, "sequence_confidence_first": 0.6562159459340845, "sequence_confidence_second": null, "sequence_confidence_final": 0.6562159459340845, "token_confidences_first": [0.5861688852310181, 0.9980383515357971, 0.4785546362400055, 0.9502339959144592, 0.9997346997261047, 0.6833568215370178, 0.48585161566734314, 0.9999982118606567, 0.9980944991111755, 0.4947408437728882, 0.8950231671333313, 0.9991587400436401, 0.3722420334815979, 0.3855527341365814, 0.19100084900856018, 0.9726919531822205, 0.9756215810775757, 0.7885231375694275, 0.27427881956100464, 0.9994065761566162], "token_confidences_second": null, "final_mean_entropy": 0.8036385535626323, "final_min_margin": 0.0048656463623046875, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 17.4, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["36%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.5265179208674328, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1226, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1227, "total_latency_s": 1.227, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.5265179208674328, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.25, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["33%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.4927158827314018, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1228, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1229, "total_latency_s": 1.229, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.4927158827314018, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.25, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["43%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.46185869996820056, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1227, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1228, "total_latency_s": 1.228, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.46185869996820056, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.25, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["SAVINGS FOR NEW COMPUTER/TECHNOLOGY."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.43355784897094507, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1229, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1230, "total_latency_s": 1.23, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.43355784897094507, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.361111111111111, "wer": 4.25, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["16%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.40749701106769365, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1228, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1229, "total_latency_s": 1.229, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.40749701106769365, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 26.25, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Saving for Education."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.38341533616749596, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1225, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1226, "total_latency_s": 1.226, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.38341533616749596, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 4.285714285714286, "wer": 5.666666666666667, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["8%."], "experiment": "baseline_no_ocr", "routed": {"answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "used_ocr": false, "answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "answer_second": null, "raw_answer": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_first": "This image appears to be an infographic or poster that provides information on how Australians save money.", "raw_answer_second": null, "mean_entropy_first": 1.1030315162279294, "normalized_entropy_first": 0.3610952271152396, "min_margin_first": 0.04537391662597656, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1227, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1228, "total_latency_s": 1.228, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9815016984939575, 1.025421142578125, 1.42041015625, 0.00019863473426084965, 0.05598803982138634, 0.6341150403022766, 1.212009072303772, 0.010199303738772869, 2.653623104095459, 2.0932416915893555, 2.8200714588165283, 2.5083112716674805, 2.0010485649108887, 0.8861955404281616, 1.6250476837158203, 0.6636664271354675, 0.00018090927915181965, 0.43820440769195557, 0.09457904100418091, 0.9366171360015869], "entropies_second": null, "final_normalized_entropy": 0.3610952271152396, "sequence_confidence_first": 0.5878867398860762, "sequence_confidence_second": null, "sequence_confidence_final": 0.5878867398860762, "token_confidences_first": [0.49054786562919617, 0.49644583463668823, 0.43945616483688354, 0.9999861717224121, 0.9931477904319763, 0.7342629432678223, 0.6020536422729492, 0.9988018274307251, 0.28557106852531433, 0.3887050449848175, 0.1852545589208603, 0.26446884870529175, 0.5050987601280212, 0.561583936214447, 0.5724840760231018, 0.8326438069343567, 0.9999877214431763, 0.9170387387275696, 0.9847931861877441, 0.7802101969718933], "token_confidences_second": null, "final_mean_entropy": 1.1030315162279294, "final_min_margin": 0.04537391662597656, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 35.0, "wer": 17.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["24.415 million."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "used_ocr": false, "answer_first": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "answer_second": null, "raw_answer": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "raw_answer_first": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "raw_answer_second": null, "mean_entropy_first": 0.858285464322762, "normalized_entropy_first": -0.8413783120733976, "min_margin_first": 0.11567115783691406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1192, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1193, "total_latency_s": 1.193, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0530273914337158, 0.1687432825565338, 1.8790699243545532, 0.320939838886261, 0.0013115560868754983, 0.6545358896255493, 1.1308767795562744, 6.773217319278046e-05, 0.06928554177284241, 0.6791585683822632, 2.4109129905700684, 0.5582868456840515, 1.630342960357666, 1.6447923183441162, 0.017236940562725067, 1.575120210647583, 0.5794187784194946, 0.7636156678199768, 1.0924080610275269, 0.9365580081939697], "entropies_second": null, "final_normalized_entropy": -0.8413783120733976, "sequence_confidence_first": 0.6870320292435913, "sequence_confidence_second": null, "sequence_confidence_final": 0.6870320292435913, "token_confidences_first": [0.6179354190826416, 0.9772919416427612, 0.3170761466026306, 0.9236634373664856, 0.9998856782913208, 0.7172946929931641, 0.4719148874282837, 0.9999959468841553, 0.99018794298172, 0.7303580045700073, 0.4898301661014557, 0.8871376514434814, 0.4039023220539093, 0.597224771976471, 0.9978199005126953, 0.6135203838348389, 0.869840681552887, 0.752890944480896, 0.5087132453918457, 0.5931369066238403], "token_confidences_second": null, "final_mean_entropy": 0.858285464322762, "final_min_margin": 0.11567115783691406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 5.866666666666666, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["7.692.024 km2."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "used_ocr": false, "answer_first": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "answer_second": null, "raw_answer": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "raw_answer_first": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "raw_answer_second": null, "mean_entropy_first": 0.858285464322762, "normalized_entropy_first": -0.7713652328933388, "min_margin_first": 0.11567115783691406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1191, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1192, "total_latency_s": 1.192, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0530273914337158, 0.1687432825565338, 1.8790699243545532, 0.320939838886261, 0.0013115560868754983, 0.6545358896255493, 1.1308767795562744, 6.773217319278046e-05, 0.06928554177284241, 0.6791585683822632, 2.4109129905700684, 0.5582868456840515, 1.630342960357666, 1.6447923183441162, 0.017236940562725067, 1.575120210647583, 0.5794187784194946, 0.7636156678199768, 1.0924080610275269, 0.9365580081939697], "entropies_second": null, "final_normalized_entropy": -0.7713652328933388, "sequence_confidence_first": 0.6870320292435913, "sequence_confidence_second": null, "sequence_confidence_final": 0.6870320292435913, "token_confidences_first": [0.6179354190826416, 0.9772919416427612, 0.3170761466026306, 0.9236634373664856, 0.9998856782913208, 0.7172946929931641, 0.4719148874282837, 0.9999959468841553, 0.99018794298172, 0.7303580045700073, 0.4898301661014557, 0.8871376514434814, 0.4039023220539093, 0.597224771976471, 0.9978199005126953, 0.6135203838348389, 0.869840681552887, 0.752890944480896, 0.5087132453918457, 0.5931369066238403], "token_confidences_second": null, "final_mean_entropy": 0.858285464322762, "final_min_margin": 0.11567115783691406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 6.5, "wer": 8.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Federal parliamentary constitutional monarchy."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "used_ocr": false, "answer_first": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "answer_second": null, "raw_answer": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "raw_answer_first": "The image you've provided appears to be a fact sheet or infographic about Australia. Here are", "raw_answer_second": null, "mean_entropy_first": 0.858285464322762, "normalized_entropy_first": -0.7109363922590808, "min_margin_first": 0.11567115783691406, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1193, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1194, "total_latency_s": 1.194, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.0530273914337158, 0.1687432825565338, 1.8790699243545532, 0.320939838886261, 0.0013115560868754983, 0.6545358896255493, 1.1308767795562744, 6.773217319278046e-05, 0.06928554177284241, 0.6791585683822632, 2.4109129905700684, 0.5582868456840515, 1.630342960357666, 1.6447923183441162, 0.017236940562725067, 1.575120210647583, 0.5794187784194946, 0.7636156678199768, 1.0924080610275269, 0.9365580081939697], "entropies_second": null, "final_normalized_entropy": -0.7109363922590808, "sequence_confidence_first": 0.6870320292435913, "sequence_confidence_second": null, "sequence_confidence_final": 0.6870320292435913, "token_confidences_first": [0.6179354190826416, 0.9772919416427612, 0.3170761466026306, 0.9236634373664856, 0.9998856782913208, 0.7172946929931641, 0.4719148874282837, 0.9999959468841553, 0.99018794298172, 0.7303580045700073, 0.4898301661014557, 0.8871376514434814, 0.4039023220539093, 0.597224771976471, 0.9978199005126953, 0.6135203838348389, 0.869840681552887, 0.752890944480896, 0.5087132453918457, 0.5931369066238403], "token_confidences_second": null, "final_mean_entropy": 0.858285464322762, "final_min_margin": 0.11567115783691406, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 1.5869565217391304, "wer": 4.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["68%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an informational poster or brochure about disability", "used_ocr": false, "answer_first": "The image you've provided appears to be an informational poster or brochure about disability", "answer_second": null, "raw_answer": "The image you've provided appears to be an informational poster or brochure about disability", "raw_answer_first": "The image you've provided appears to be an informational poster or brochure about disability", "raw_answer_second": null, "mean_entropy_first": 0.7631016815641487, "normalized_entropy_first": -1.151080620069399, "min_margin_first": 0.07842826843261719, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1363, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1364, "total_latency_s": 1.364, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8199222087860107, 0.06208490580320358, 1.484236240386963, 0.2299908846616745, 0.004707354586571455, 0.6952851414680481, 0.8541736006736755, 4.726093902718276e-05, 0.010645485483109951, 0.7528807520866394, 1.0745525360107422, 0.4946262836456299, 1.1047642230987549, 1.1334784030914307, 2.251291275024414, 0.0004239822446834296, 0.0003405522438697517, 2.486388683319092, 1.7302109003067017, 0.0719829574227333], "entropies_second": null, "final_normalized_entropy": -1.151080620069399, "sequence_confidence_first": 0.6553976238775879, "sequence_confidence_second": null, "sequence_confidence_final": 0.6553976238775879, "token_confidences_first": [0.5336252450942993, 0.9918001294136047, 0.49372512102127075, 0.9521231055259705, 0.9994839429855347, 0.6816310286521912, 0.5053272843360901, 0.9999971389770508, 0.9988699555397034, 0.5891889929771423, 0.4934639036655426, 0.8067415356636047, 0.47525638341903687, 0.7586809396743774, 0.25554323196411133, 0.9999692440032959, 0.9999704360961914, 0.28203603625297546, 0.41562139987945557, 0.9866895079612732], "token_confidences_second": null, "final_mean_entropy": 0.7631016815641487, "final_min_margin": 0.07842826843261719, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.0, "wer": 14.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Illness, injury or disability."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be an informational poster or brochure about disability", "used_ocr": false, "answer_first": "The image you've provided appears to be an informational poster or brochure about disability", "answer_second": null, "raw_answer": "The image you've provided appears to be an informational poster or brochure about disability", "raw_answer_first": "The image you've provided appears to be an informational poster or brochure about disability", "raw_answer_second": null, "mean_entropy_first": 0.7631016815641487, "normalized_entropy_first": -1.0261436181460508, "min_margin_first": 0.07842826843261719, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1363, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1364, "total_latency_s": 1.364, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.8199222087860107, 0.06208490580320358, 1.484236240386963, 0.2299908846616745, 0.004707354586571455, 0.6952851414680481, 0.8541736006736755, 4.726093902718276e-05, 0.010645485483109951, 0.7528807520866394, 1.0745525360107422, 0.4946262836456299, 1.1047642230987549, 1.1334784030914307, 2.251291275024414, 0.0004239822446834296, 0.0003405522438697517, 2.486388683319092, 1.7302109003067017, 0.0719829574227333], "entropies_second": null, "final_normalized_entropy": -1.0261436181460508, "sequence_confidence_first": 0.6553976238775879, "sequence_confidence_second": null, "sequence_confidence_final": 0.6553976238775879, "token_confidences_first": [0.5336252450942993, 0.9918001294136047, 0.49372512102127075, 0.9521231055259705, 0.9994839429855347, 0.6816310286521912, 0.5053272843360901, 0.9999971389770508, 0.9988699555397034, 0.5891889929771423, 0.4934639036655426, 0.8067415356636047, 0.47525638341903687, 0.7586809396743774, 0.25554323196411133, 0.9999692440032959, 0.9999704360961914, 0.28203603625297546, 0.41562139987945557, 0.9866895079612732], "token_confidences_second": null, "final_mean_entropy": 0.7631016815641487, "final_min_margin": 0.07842826843261719, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 2.4, "wer": 3.25, "precision": 0.07142857142857142, "recall": 0.25, "f1": 0.11111111111111112, "rouge_l": 0.11111111111111112, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Australia."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "raw_answer_first": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "raw_answer_second": null, "mean_entropy_first": 1.0846564136008965, "normalized_entropy_first": 0.7282274492695322, "min_margin_first": 0.026292800903320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1362, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1363, "total_latency_s": 1.363, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9454635381698608, 0.06766253709793091, 1.596567988395691, 0.3133314251899719, 0.002512269653379917, 0.7007160186767578, 1.077228307723999, 6.448297062888741e-05, 0.06713855266571045, 0.6814858317375183, 2.728668689727783, 0.7452877759933472, 2.394428253173828, 1.894690990447998, 0.01773640513420105, 1.8463404178619385, 1.5580434799194336, 2.435901403427124, 0.40571328997612, 2.214146614074707], "entropies_second": null, "final_normalized_entropy": 0.7282274492695322, "sequence_confidence_first": 0.5965378932276372, "sequence_confidence_second": null, "sequence_confidence_final": 0.5965378932276372, "token_confidences_first": [0.48476099967956543, 0.9916402101516724, 0.44889065623283386, 0.9266446232795715, 0.9997487664222717, 0.6574813723564148, 0.5694229006767273, 0.9999961853027344, 0.9901139736175537, 0.6918746829032898, 0.2846491038799286, 0.6792336702346802, 0.4025101959705353, 0.5350422263145447, 0.9977529644966125, 0.5471751093864441, 0.5874097347259521, 0.22232191264629364, 0.903868556022644, 0.23671887814998627], "token_confidences_second": null, "final_mean_entropy": 1.0846564136008965, "final_min_margin": 0.026292800903320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 8.6, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["80%."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "used_ocr": false, "answer_first": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "answer_second": null, "raw_answer": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "raw_answer_first": "The image you've provided appears to be a graphic or infographic comparing the prices of books", "raw_answer_second": null, "mean_entropy_first": 1.0846564136008965, "normalized_entropy_first": 0.6732364383455479, "min_margin_first": 0.026292800903320312, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1360, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1360, "total_latency_s": 1.36, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.9454635381698608, 0.06766253709793091, 1.596567988395691, 0.3133314251899719, 0.002512269653379917, 0.7007160186767578, 1.077228307723999, 6.448297062888741e-05, 0.06713855266571045, 0.6814858317375183, 2.728668689727783, 0.7452877759933472, 2.394428253173828, 1.894690990447998, 0.01773640513420105, 1.8463404178619385, 1.5580434799194336, 2.435901403427124, 0.40571328997612, 2.214146614074707], "entropies_second": null, "final_normalized_entropy": 0.6732364383455479, "sequence_confidence_first": 0.5965378932276372, "sequence_confidence_second": null, "sequence_confidence_final": 0.5965378932276372, "token_confidences_first": [0.48476099967956543, 0.9916402101516724, 0.44889065623283386, 0.9266446232795715, 0.9997487664222717, 0.6574813723564148, 0.5694229006767273, 0.9999961853027344, 0.9901139736175537, 0.6918746829032898, 0.2846491038799286, 0.6792336702346802, 0.4025101959705353, 0.5350422263145447, 0.9977529644966125, 0.5471751093864441, 0.5874097347259521, 0.22232191264629364, 0.903868556022644, 0.23671887814998627], "token_confidences_second": null, "final_mean_entropy": 1.0846564136008965, "final_min_margin": 0.026292800903320312, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 23.5, "wer": 16.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["8."], "experiment": "baseline_no_ocr", "routed": {"answer": "Living in Australia", "used_ocr": false, "answer_first": "Living in Australia", "answer_second": null, "raw_answer": "ANSWER: Living in Australia", "raw_answer_first": "ANSWER: Living in Australia", "raw_answer_second": null, "mean_entropy_first": 0.25557901900901925, "normalized_entropy_first": -3.892152592314163, "min_margin_first": 1.5749835968017578, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 791, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 792, "total_latency_s": 0.792, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [1.2860734462738037, 0.0494324192404747, 0.0017708172090351582, 0.0012958067236468196, 0.012666337192058563, 0.5221397280693054, 0.027654727920889854, 0.14359886944293976], "entropies_second": null, "final_normalized_entropy": -3.892152592314163, "sequence_confidence_first": 0.9340324853904296, "sequence_confidence_second": null, "sequence_confidence_final": 0.9340324853904296, "token_confidences_first": [0.6570207476615906, 0.9950045943260193, 0.9998606443405151, 0.9998819828033447, 0.9987087249755859, 0.8995871543884277, 0.9963107705116272, 0.9738367199897766, 0.9497348666191101], "token_confidences_second": null, "final_mean_entropy": 0.25557901900901925, "final_min_margin": 1.5749835968017578, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.5, "wer": 3.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["1 in 8."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"BULLYING\" that aims", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"BULLYING\" that aims", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"BULLYING\" that aims", "raw_answer_first": "The image you've provided is an infographic titled \"BULLYING\" that aims", "raw_answer_second": null, "mean_entropy_first": 0.7341621139261406, "normalized_entropy_first": -0.5952977408451494, "min_margin_first": 0.08201408386230469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1358, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1359, "total_latency_s": 1.359, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.844440221786499, 0.06875386834144592, 1.795681118965149, 0.243931844830513, 0.005829784087836742, 0.7641736268997192, 0.8524971008300781, 0.7599384188652039, 1.2298214435577393, 0.008633308112621307, 1.9287071228027344, 0.006637674756348133, 0.28437745571136475, 0.1034170538187027, 0.0334380678832531, 0.0048340908251702785, 1.1689672470092773, 2.088129997253418, 2.4907584190368652, 0.0002744131488725543], "entropies_second": null, "final_normalized_entropy": -0.5952977408451494, "sequence_confidence_first": 0.6779206145253575, "sequence_confidence_second": null, "sequence_confidence_final": 0.6779206145253575, "token_confidences_first": [0.5093140006065369, 0.9909365773200989, 0.3540768325328827, 0.9479321241378784, 0.9993345141410828, 0.6269002556800842, 0.5880409479141235, 0.7220004200935364, 0.40735653042793274, 0.999021053314209, 0.48585379123687744, 0.9993327260017395, 0.9511512517929077, 0.9809802174568176, 0.9951327443122864, 0.9996168613433838, 0.4894009232521057, 0.29287421703338623, 0.3548869788646698, 0.99997878074646], "token_confidences_second": null, "final_mean_entropy": 0.7341621139261406, "final_min_margin": 0.08201408386230469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 9.571428571428571, "wer": 3.6666666666666665, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
{"question": "", "image_id": null, "answers": ["Bullying."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image you've provided is an infographic titled \"BULLYING\" that aims", "used_ocr": false, "answer_first": "The image you've provided is an infographic titled \"BULLYING\" that aims", "answer_second": null, "raw_answer": "The image you've provided is an infographic titled \"BULLYING\" that aims", "raw_answer_first": "The image you've provided is an infographic titled \"BULLYING\" that aims", "raw_answer_second": null, "mean_entropy_first": 0.7341621139261406, "normalized_entropy_first": -0.5550005992616619, "min_margin_first": 0.08201408386230469, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1355, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1356, "total_latency_s": 1.356, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.844440221786499, 0.06875386834144592, 1.795681118965149, 0.243931844830513, 0.005829784087836742, 0.7641736268997192, 0.8524971008300781, 0.7599384188652039, 1.2298214435577393, 0.008633308112621307, 1.9287071228027344, 0.006637674756348133, 0.28437745571136475, 0.1034170538187027, 0.0334380678832531, 0.0048340908251702785, 1.1689672470092773, 2.088129997253418, 2.4907584190368652, 0.0002744131488725543], "entropies_second": null, "final_normalized_entropy": -0.5550005992616619, "sequence_confidence_first": 0.6779206145253575, "sequence_confidence_second": null, "sequence_confidence_final": 0.6779206145253575, "token_confidences_first": [0.5093140006065369, 0.9909365773200989, 0.3540768325328827, 0.9479321241378784, 0.9993345141410828, 0.6269002556800842, 0.5880409479141235, 0.7220004200935364, 0.40735653042793274, 0.999021053314209, 0.48585379123687744, 0.9993327260017395, 0.9511512517929077, 0.9809802174568176, 0.9951327443122864, 0.9996168613433838, 0.4894009232521057, 0.29287421703338623, 0.3548869788646698, 0.99997878074646], "token_confidences_second": null, "final_mean_entropy": 0.7341621139261406, "final_min_margin": 0.08201408386230469, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 1.0, "cer": 7.0, "wer": 11.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 1.0, "relaxed_accuracy": 1.0}}
{"question": "", "image_id": null, "answers": ["6."], "experiment": "baseline_no_ocr", "routed": {"answer": "The image appears to be an infographic or a graphical representation of data related to higher education in", "used_ocr": false, "answer_first": "The image appears to be an infographic or a graphical representation of data related to higher education in", "answer_second": null, "raw_answer": "The image appears to be an infographic or a graphical representation of data related to higher education in", "raw_answer_first": "The image appears to be an infographic or a graphical representation of data related to higher education in", "raw_answer_second": null, "mean_entropy_first": 1.157080161529302, "normalized_entropy_first": 1.1285192445459655, "min_margin_first": 0.11408805847167969, "mean_entropy_second": null, "normalized_entropy_second": null, "min_margin_second": null, "triggered": false, "ocr_engine": null, "latency_ms_first": 1366, "latency_ms_ocr": 0, "latency_ms_second": 0, "total_latency_ms": 1367, "total_latency_s": 1.367, "model_id": "/root/OCR_route/OCR-route/llava-v1.6-vicuna-7b-hf", "image_source": "in_memory:None", "entropies_first": [0.7107620239257812, 0.20657673478126526, 2.0223894119262695, 0.000290006777504459, 0.13013148307800293, 0.7284606695175171, 0.6516547799110413, 0.012343207374215126, 2.196444034576416, 2.365161180496216, 2.929126501083374, 1.200923204421997, 0.9562814831733704, 2.326089859008789, 2.5588674545288086, 1.48695969581604, 0.0008989225025288761, 1.7414964437484741, 0.013224238529801369, 0.9035218954086304], "entropies_second": null, "final_normalized_entropy": 1.1285192445459655, "sequence_confidence_first": 0.5902626603643554, "sequence_confidence_second": null, "sequence_confidence_final": 0.5902626603643554, "token_confidences_first": [0.7271368503570557, 0.9678636789321899, 0.3399685025215149, 0.9999799728393555, 0.9816028475761414, 0.5430426597595215, 0.8136165738105774, 0.9985107779502869, 0.390256404876709, 0.30527520179748535, 0.328113853931427, 0.4788358211517334, 0.8225065469741821, 0.37886539101600647, 0.18999673426151276, 0.6774466633796692, 0.9999250173568726, 0.4198399484157562, 0.9983869791030884, 0.8084537386894226], "token_confidences_second": null, "final_mean_entropy": 1.157080161529302, "final_min_margin": 0.11408805847167969, "experiment": "baseline_no_ocr"}, "routed_metrics": {"exact_match": 0.0, "accuracy": 0.0, "cer": 53.5, "wer": 18.0, "precision": 0.0, "recall": 0.0, "f1": 0.0, "rouge_l": 0.0, "anls": 0.0, "relaxed_accuracy": 0.0}}
